<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.8.24">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Anon">
<meta name="keywords" content="decision theory, model, idealisation, advice, second-best">

<title>The End of Decision Theory</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="teodt-ajp-2_files/libs/clipboard/clipboard.min.js"></script>
<script src="teodt-ajp-2_files/libs/quarto-html/quarto.js" type="module"></script>
<script src="teodt-ajp-2_files/libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="teodt-ajp-2_files/libs/quarto-html/axe/axe-check.js" type="module"></script>
<script src="teodt-ajp-2_files/libs/quarto-html/popper.min.js"></script>
<script src="teodt-ajp-2_files/libs/quarto-html/tippy.umd.min.js"></script>
<script src="teodt-ajp-2_files/libs/quarto-html/anchor.min.js"></script>
<link href="teodt-ajp-2_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="teodt-ajp-2_files/libs/quarto-html/quarto-syntax-highlighting-dc55a5b9e770e841cd82e46aadbfb9b0.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="teodt-ajp-2_files/libs/bootstrap/bootstrap.min.js"></script>
<link href="teodt-ajp-2_files/libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="teodt-ajp-2_files/libs/bootstrap/bootstrap-d6a003b94517c951b2d65075d42fb01b.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">


</head>

<body class="quarto-light">

<div id="quarto-content" class="page-columns page-rows-contents page-layout-article">
<div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
<div class="quarto-alternate-formats"><h2>Other Formats</h2><ul><li><a href="The End of Decision Theory - AJP.docx"><i class="bi bi-file-word"></i>MS Word</a></li><li><a href="../The End of Decision Theory - AJP.pdf"><i class="bi bi-file-pdf"></i>PDF</a></li></ul></div></div>
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">The End of Decision Theory</h1>
</div>



<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>Anon </p>
          </div>
  </div>
    
  
    
  </div>
  
<div>
  <div class="abstract">
    <div class="block-title">Abstract</div>
    <p>What question should decision theorists be trying to answer, and why is it worth trying to answer it? A lot of philosophers talk as if the aim of decision theory is to describe how we should make decisions, and the reason to do this is to help us make better decisions. I disagree on both fronts. The aim of decision theory should be to describe how a certain kind of idealised decider does in fact decide. And the reason to do this is that this idealisation, like many other idealisations, helps generate explanations of real-world behaviour. We shouldn’t do what these ideal deciders do, or try to be more like them, because a lot of what they do only makes sense because of the differences between us and them. Still, sometimes those differences are small enough that they can be ignored in explanations, and that’s when decision theory is useful.</p>
    <p><strong>Keywords</strong>: decision theory, model, idealisation, advice, second-best.</p>
  </div>
</div>

<div>
  <div class="keywords">
    <div class="block-title">Keywords</div>
    <p>decision theory, model, idealisation, advice, second-best</p>
  </div>
</div>

</header>


<section id="sec-what" class="level1" data-number="1">
<h1 data-number="1"><span class="header-section-number">1</span> What is Decision Theory a Theory Of?</h1>
<p>If you’re reading a paper like this, you’re probably familiar with seeing papers defending this or that decision theory. Familiar decision theories include:</p>
<ul>
<li>Causal Decision Theory <span class="citation" data-cites="GibbardHarper1978 Lewis1981b Skyrms1990 Joyce1999">(<a href="#ref-GibbardHarper1978" role="doc-biblioref">Gibbard and Harper 1978</a>; <a href="#ref-Lewis1981b" role="doc-biblioref">Lewis 1981</a>; <a href="#ref-Skyrms1990" role="doc-biblioref">Skyrms 1990</a>; <a href="#ref-Joyce1999" role="doc-biblioref">Joyce 1999</a>)</span>;</li>
<li>Evidential Decision Theory <span class="citation" data-cites="Ahmed2014">(<a href="#ref-Ahmed2014" role="doc-biblioref">Ahmed 2014</a>)</span>;</li>
<li>Benchmark theory <span class="citation" data-cites="Wedgwood2013a">(<a href="#ref-Wedgwood2013a" role="doc-biblioref">Wedgwood 2013</a>)</span>;</li>
<li>Risk-Weighted theory <span class="citation" data-cites="BuchakRisk">(<a href="#ref-BuchakRisk" role="doc-biblioref">Buchak 2013</a>)</span>;</li>
<li>Tournament Decision Theory <span class="citation" data-cites="Podgorski2022">(<a href="#ref-Podgorski2022" role="doc-biblioref">Podgorski 2022</a>)</span>; and</li>
<li>Functional Decision Theory <span class="citation" data-cites="LevinsteinSoares2020">(<a href="#ref-LevinsteinSoares2020" role="doc-biblioref">Levinstein and Soares 2020</a>)</span></li>
</ul>
<p>Other theories haven’t had snappy ‘isms’ applied to them, such as the non-standard version of Causal Decision Theory that Dmitri <span class="citation" data-cites="Gallow2020">Gallow (<a href="#ref-Gallow2020" role="doc-biblioref">2020</a>)</span> defends, or the pluralist decision theory that Jack <span class="citation" data-cites="Spencer2021">Spencer (<a href="#ref-Spencer2021" role="doc-biblioref">2021</a>)</span> defends, or the broadly ratificationist theory that Melissa <span class="citation" data-cites="Fusco2024">Fusco (<a href="#ref-Fusco2024" role="doc-biblioref">2024</a>)</span> defends.</p>
<p>This paper isn’t going to take sides between these nine or more theories.<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a> Rather it is going to ask a prior pair of questions.</p>
<ol type="1">
<li>If these are the possible answers, what could the question be? That is, what question could decision theorsts be asking such that these are plausible answers to it?</li>
<li>Why is that an interesting question? What do we gain by answering it?</li>
</ol>
<p>On 1, I will argue that decision theories should be understood as answers to a question about what an ideal decider would do. The ‘ideal’ here is like the ‘ideal’ in a scientific idealisation, not the ideal in something like an ideal advisor moral theory. That is, the ideal decider is an idealisation in the sense of being simple, not in the sense of being perfect. The ideal decision maker is ideal in the same way that the point-masses in the ideal gas model are ideal; they are (relatively) simple to work with. The main opponent I have in mind is someone who says that the best decision theory tells us what decisions we should make.</p>
<p>On 2, I will argue that the point of asking this question is that these idealisations play important roles in explanatorily useful models of social interactions, such as the model of the used car market that George <span class="citation" data-cites="Akerlof1970">Akerlof (<a href="#ref-Akerlof1970" role="doc-biblioref">1970</a>)</span> described. Here, the main opponent I have in mind is someone who says that decision theory is useful because it helps us make better decisions.</p>
<p>There is another pair of answers to this question which is interesting, but which I won’t have a lot to say about here. David Lewis held that “central question of decision theory is: which choices are the ones that serve one’s desires according to one’s beliefs?” <span class="citation" data-cites="Lewis-Gorman-19041989">(<a href="#ref-Lewis-Gorman-19041989" role="doc-biblioref">Lewis [1989] 2020, 472</a>)</span>. That’s not far from the view I have, though I’d say it’s according to one’s evidence. But I differ a bit more from Lewis as to the point of this activity. For him, a central role for decision theory is supplying a theory of constitutive rationality to an account of mental content <span class="citation" data-cites="Lewis1994b">(<a href="#ref-Lewis1994b" role="doc-biblioref">Lewis 1994, 321–22</a>)</span>. I think the resulting theory is too idealised to help there, and that’s before we get to questions about whether we should accept the approach to mental content that requires constitutive rationality. That said, the view I’m defending is going to be in many ways like Lewis’s: the big task of decision theory is describing an idealised system, not yet recommending it.</p>
<p>The nine theories I mentioned above disagree about a lot of things. In philosophy we typically spend our time looking at cases where theories disagree. Not here! I will focus almost exclusively on two cases where those nine theories all say the same thing. I’ll assume that whatever question they could be asking, the correct answer to it in those two cases must agree with all nine theories. That will be enough to defend the view I want to defend, which is that the best decision theory will be one that correctly describes an idealised version of actual deciders.</p>
<p>The resulting theory has a lot in common with the view that Joe Roussos has defended about ethics (<span class="citation" data-cites="Roussos2022">Roussos (<a href="#ref-Roussos2022" role="doc-biblioref">2022</a>)</span>) and, especially, formal epistemology (<span class="citation" data-cites="Roussos2025">Roussos (<a href="#ref-Roussos2025" role="doc-biblioref">2025</a>)</span>). He says that we should think of philosophical work in these areas as modeling rather than theorizing. I agree. If decision theory is a theory of anything, it’s a theory of how some very strange creatures behave. Why we care about those creatures is not immediately obvious. The best reason, I’ll argue, to care about these creatures is that they help us understand some aspects of the behaviour of real humans. Of course, real humans are not ideal. But sometimes they are close enough, in relevant respects, to the ideal that learning that idealised humans do something helps us understand why actual humans do it. This is to broadly agree with Roussos’s main claims. If anything, I think the case for a view like his is even stronger in decision theory than in ethics or formal epistemology, and the point of this paper is to make that case.</p>
</section>
<section id="sec-two-cases" class="level1" data-number="2">
<h1 data-number="2"><span class="header-section-number">2</span> Two Cases</h1>
<section id="betting" class="level2" data-number="2.1">
<h2 data-number="2.1" class="anchored" data-anchor-id="betting"><span class="header-section-number">2.1</span> Betting</h2>
<p>Chooser has $110, and is in a sports betting shop. There is a basketball game about to start, between two teams they know to be equally matched. Chooser has three options: bet the $110 on Home, bet it on Away, keep money. If they bet and are right, they win $100 (plus get the money back they bet), if they are wrong, they lose the money. Given standard assumptions about how much Chooser likes money, all the decision theories I’m discussing say Chooser should not bet.</p>
<p>From this it follows that decision theory is not in the business of answering this question: <em>What action will produce the best outcome?</em>. We know, and so does Chooser, that the action that produces the best outcome is to bet on the winning team. Keeping their money in their pocket is the only action they know will be sub-optimal. And it’s what decision theory says to do.</p>
<p>This is to say, decision theory is not axiology. It’s not a theory of evaluating outcomes, and saying which is best. Axiology is a very important part of philosophy, but it’s not what decision theorists are up to.</p>
<p>So far this will probably strike you, dear reader, as obvious. But there’s another step, that I think will strike some people as nearly as obvious, that I’m at pains to resist. Some might say that decision theorists don’t tell Chooser to bet on the winner because this is lousy advice. Chooser can’t bet on the winner, at least not as such. That, I’ll argue, would be a misstep. Decision theorists do not restrict themselves to answers that can be practically carried out.</p>
</section>
<section id="salesman" class="level2" data-number="2.2">
<h2 data-number="2.2" class="anchored" data-anchor-id="salesman"><span class="header-section-number">2.2</span> Salesman</h2>
<p>We’ll focus on a version of what Julia <span class="citation" data-cites="Robinson1949">Robinson (<a href="#ref-Robinson1949" role="doc-biblioref">1949</a>)</span> called the travelling salesman problem.<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a> Given some points on a map, find the shortest path through them. We’ll focus on the 257 cities shown on the map in <a href="#fig-map" class="quarto-xref">Figure&nbsp;1</a>.</p>
<div class="cell">
<div class="cell-output-display">
<div id="fig-map" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-map-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="teodt-ajp-2_files/figure-html/fig-map-1.png" class="img-fluid figure-img" width="672">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-map-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1: 257 American cites; our task is to find the shortest path that goes through all of them.
</figcaption>
</figure>
</div>
</div>
</div>
<p>The task is to find the shortest path through those 257 cities.<a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a></p>
<p>All nine of the decision theories I mentioned, and as far as I know every competitor to them in the philosophical literature, say the thing to do here is to draw whichever of the 256! possible paths is shortest. That is not particularly helpful advice. Unless you know a lot about problems like this, you can’t draw the shortest path through the map. At least, you can’t draw it as such. You can’t draw it in the way that you can’t enter the correct code on a locked phone <span class="citation" data-cites="MandelkernEtAl2017">(<a href="#ref-MandelkernEtAl2017" role="doc-biblioref">Mandelkern, Schultheis, and Boylan 2017</a>)</span>.</p>
<p>One of the striking things about this puzzle is that it turns out there are some helpful things that can be said. One helpful bit of advice to someone trying to solve a problem like this is to use a Farthest Insertion Algorithm.<a href="#fn4" class="footnote-ref" id="fnref4" role="doc-noteref"><sup>4</sup></a> Insertion algorithms say to start with a random city, then add cities to the path one at a time, at each time finding the point to insert the city into the existing path that adds the least distance. The Farthest Insertion Algorithm says that the city added at each stage is the one farthest from the existing path. Insertion algorithms in general produce pretty good paths in a very short amount of time - at least on normal computers. And the Farthest Insertion Algorithm is, most of the time, the best Insertion Algorithm to use. <a href="#fig-farthest" class="quarto-xref">Figure&nbsp;2</a> shows the result of one output of this algorithm.<a href="#fn5" class="footnote-ref" id="fnref5" role="doc-noteref"><sup>5</sup></a></p>
<div class="cell">
<div class="cell-output-display">
<div id="fig-farthest" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-farthest-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="teodt-ajp-2_files/figure-html/fig-farthest-1.png" class="img-fluid figure-img" width="672">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-farthest-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2: An output of the Farthest Insertion Algorithm, with a length of 21075 miles.
</figcaption>
</figure>
</div>
</div>
</div>
<p>The path in <a href="#fig-farthest" class="quarto-xref">Figure&nbsp;2</a> is not bad, but with only a bit of extra computational work, one can do better. A fairly simple optimisation algorithm takes a map as input, and then deletes pairs of edges at a time, and finds the shortest path of all possible paths with all but those two edges. The process continues until no improvements can be made by deleting two edges at a time, at which point you’ve found a somewhat resilient local minimum. <a href="#fig-two-opt" class="quarto-xref">Figure&nbsp;3</a> is the output from applying this strategy to the path in <a href="#fig-farthest" class="quarto-xref">Figure&nbsp;2</a>.</p>
<div class="cell">
<div class="cell-output-display">
<div id="fig-two-opt" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-two-opt-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="teodt-ajp-2_files/figure-html/fig-two-opt-1.png" class="img-fluid figure-img" width="672">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-two-opt-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3: The output of an optimisation process, which reduced the path length to 20891 miles.
</figcaption>
</figure>
</div>
</div>
</div>
<p>This optimisation tends to produce paths that look a lot like the original, but are somewhat shorter. For most practical purposes, the best advice you could give someone faced with a problem like this is to use a Farthest Insertion Algorithm, then optimise it in this way. Or, if they have a bit more time, they could do this a dozen or so times, and see if different starting cities led to slightly shorter paths.</p>
<p>While this is good advice, and indeed it’s what most people should do, it’s not typically what is optimal to do. For that reason, it’s not what our nine decision theories would say to do. If one had unlimited and free computing power available, hacks like these would be pointless. One would simply look at all the possible paths, and see which was shortest. I do not have free, unlimited computing power, so I didn’t do this. Using some black box algorithms I did not particularly understand, I was able to find a shorter path, however. It took some time, both of mine and my computer’s, and for most purposes it would not have been worth the hassle of finding it. Still, just to show it exists, I’ve plotted it as <a href="#fig-best" class="quarto-xref">Figure&nbsp;4</a>.</p>
<div class="cell">
<div class="cell-output-display">
<div id="fig-best" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-best-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="teodt-ajp-2_files/figure-html/fig-best-1.png" class="img-fluid figure-img" width="672">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-best-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;4: The shortest path I could find, with a distance of 20301 miles.
</figcaption>
</figure>
</div>
</div>
</div>
<p>I’m not sure if <a href="#fig-best" class="quarto-xref">Figure&nbsp;4</a> is as short as possible, but I couldn’t find a shorter one. Still, for many purposes it wouldn’t have been worth the trouble it took to find this map.</p>
</section>
<section id="the-two-cases" class="level2" data-number="2.3">
<h2 data-number="2.3" class="anchored" data-anchor-id="the-two-cases"><span class="header-section-number">2.3</span> The Two Cases</h2>
<p><a href="#tbl-examples" class="quarto-xref">Table&nbsp;1</a> summarises the examples from the last two sections.</p>
<div id="tbl-examples" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-examples-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;1: How three approaches to decision theory handle the two cases
</figcaption>
<div aria-describedby="tbl-examples-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="caption-top table">
<thead>
<tr class="header">
<th></th>
<th style="text-align: center;">Betting</th>
<th style="text-align: center;">Salesman</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Best outcome</td>
<td style="text-align: center;">Bet on winner</td>
<td style="text-align: center;">Shortest path</td>
</tr>
<tr class="even">
<td>Decision theory</td>
<td style="text-align: center;">Pass</td>
<td style="text-align: center;">Shortest path</td>
</tr>
<tr class="odd">
<td>Best advice</td>
<td style="text-align: center;">Pass</td>
<td style="text-align: center;">Learn algorithms</td>
</tr>
</tbody>
</table>
</div>
</figure>
</div>
<p>The first row says which action would produce the best outcome in the two cases. The third row says what advice one ought give someone who had to choose in the two cases. And the middle row says what all the decision theories say about the two cases. Notably, it agrees with neither the first nor third row. Decision theory is neither in the business of saying what will produce the best result, nor with giving the most useful advice. So what could it be doing?</p>
<p>It won’t do to simply say that decision theory is a theory of rational choice. The person who uses a farthest insertion algorithm in Salesman, possibly supplemented with an edge-deletion optimisation, is being pretty rational. If we say that this person is being irrational, we must be using a somewhat non-standard notion of rationality. And now we’re back to the problems I started with. What could that notion be, and why should we care about it? The next two section try to answer those questions.</p>
</section>
</section>
<section id="sec-idealisations" class="level1" data-number="3">
<h1 data-number="3"><span class="header-section-number">3</span> Decision Theory as Idealisation</h1>
<p>Imagine a version of Chooser with, as Rousseau might have put it, their knowledge as it is, and their computational powers as they might be. That is, a version of Chooser who has unlimited, and free, computational powers, but no more knowledge of the world than the actually have - save what they learn by performing deductions from their existing knowledge. Call this person IC, for Idealised Chooser.</p>
<p>Here’s one important fact about IC: they decline to bet in Betting, and they choose the optimal path in Salesman. That is, they do exactly what the nine decision theories say. Why will they do that?</p>
<p>Decision theories describe what that version of Chooser would do in the problem that Chooser is facing. In the betting case, adding unlimited computing power doesn’t tell you who is going to win the game. So that version of Chooser will still avoid betting. But in the Salesman case, adding unlimited computing power is enough to solve the problem. They don’t even have to use any fancy techniques. To find the shortest path, all it takes is finding the length of each path, and sorting the results. The first requires nothing more that addition; at least if, as was the case here, we provided the computer with the distances between any pairs of cities as input. The second just requires being able to do a bubble sort, which is technically extremely simple. To be sure, doing all these additions, then doing a bubble sort on the results, will take longer than most human lives on the kinds of computers most people have available to them. But a version of Chooser with unlimited, free, computational power will do these computations no problem at all.</p>
<p>Here’s one important fact about IC: they decline to bet in Betting, and they choose the optimal path in Salesman. That is, they do exactly what the nine decision theories say. Why will they do that? In the betting case, adding unlimited computing power doesn’t tell you who is going to win the game. So IC will still avoid betting. But in the Salesman case, adding unlimited computing power is enough to solve the problem. IC doesn’t even have to use any fancy techniques. To find the shortest path, all it takes is finding the length of each path, and sorting the results. The first requires nothing more that addition; at least if, as was the case here, we provided the computer with the distances between any pairs of cities as input. The second just requires being able to do a bubble sort, which is technically extremely simple. To be sure, doing all these additions, then doing a bubble sort on the results, will take longer than most human lives on the kinds of computers most people have available to them. But by hypothesis IC can do them freely and instantaneously, so they will do them, and get the right answer.</p>
<p>Dropping the idealisation, it’s notable that if we say that Chooser should maximise expected utility, and we expect them to compute that, then we’re asking Chooser to perform a task that is one step harder than calculating the shortest path in a Salesman problem. To calculate an expected utility, for each option one looks up a probability and a utility for each state<a href="#fn6" class="footnote-ref" id="fnref6" role="doc-noteref"><sup>6</sup></a>, multiplies the two together, then adds the results to get a value for the option. One repeats that for each state, and finds an extreme value. Calculating the shortest path is exactly the same, except one only has to look up one number (a distance) rather than two (a probability and a utility), and there is no multiplication. Solving for the shortest path is strictly easier than finding the maximum expected utility. And yet finding the shortest path is practically impossible.</p>
<p>This is one reason I focussed on Salesman problems rather than other mathematical claims that Chooser is, in the standard models, assumed to know. I didn’t ask Chooser to bet on the Twin Primes conjecture. It’s possible one could come up with a model where finding the maximum expected utility is typically possible but resolving the Twin Primes conjecture is not; it’s really hard to see how an agent who could always calculate expected utilities couldn’t solve a Salesman problem.</p>
<p>There are two other things that are distinctively interesting about this problem which I’ll simply note here, and defer longer discussion of them to another day. First, it is possible to give practical useful advice about how to solve Salesman problems. I’ve repeated some of the better advice I’ve heard in the previous section. Second, when someone follows this advice and does badly, as can happen with carefully designed maps, it seems they are unlucky in just the same way that someone who maximises expected utility but gets a low amount of actual utility is unlucky. This raises some interesting questions about the normative significance of expected utility maximisation that will be in the background of the rest of the discussion here; hopefully I’ll return to them in later work.</p>
<p>At this point you might complain that I’ve talked about decision theories asking Chooser to <em>calculate</em> expected utilities. They do no such thing. This is a point that Frank Knight made a century ago.</p>
<blockquote class="blockquote">
<p>Let us take Marshall’s example of a boy gathering and eating berries … We can hardly suppose that the boy goes through such mental operations as drawing curves or making estimates of utility and disutility scales. <span class="citation" data-cites="Knight1921">(<a href="#ref-Knight1921" role="doc-biblioref">Knight 1921, 66–67</a>)</span></p>
</blockquote>
<p>And Knight does not say this is irrational. As long as the boy gets enough berries, he’s doing fine. In other terminology, we might say that decision theory provides a criteria of rightness, not a deliberation procedure.<a href="#fn7" class="footnote-ref" id="fnref7" role="doc-noteref"><sup>7</sup></a> As long as one follows the rules of decision theory, even if one follows them largely instinctually like Marshall’s boy, one is rational.</p>
<p>Once again, this move just brings us back to the original problem. It’s easy to understand the distinction in Sidgwick. The criterion of rightness is that one actually produces the best outcome. Which decision procedure actually produces that outcome is hard to determine in advance, though there are good reasons for suspecting that aiming for the best outcome as such is not the optimal procedure. Why, however, should we think that maximising <em>expected</em> utility is a criteria of rightness? What benefits does it have, over the standard of maximising actual utility, as such a criteria?</p>
<p>In Betting, a typical Chooser can maximise expected utility, while they can’t maximise actual utility. Could that be the reason to say that maximising expected utility is the criteria of rightness? Hardly. After all, in Salesman, Chooser can neither maximise expected nor actual utility. There must be some other reason that don’t bet is the right answer, and not just a useful answer, in Betting.</p>
<p>One reason we might suppose that these theories provide the right answer is that expected utility maximisation, or whatever one’s favourite decision theory endorses, is a goal; it is something we should try to achieve. On this picture, decision theory is relevant because it tells us what our ideal selves are like, and it recommends we try to be like them. In practice we can’t always be like them, as in the Salesman problem, but we should try.</p>
<p>The problem with this answer is that it is not, in general, good to try to be like the ideal. The key point goes back to Lipsey and Lancaster’s <em>General Theory of the Second Best</em> <span class="citation" data-cites="LipseyLancaster">(<a href="#ref-LipseyLancaster" role="doc-biblioref">Lipsey and Lancaster 1956</a>)</span>. Often times, the right thing to do is something whose value consists in mitigating the costs of our other flaws. It’s not true in general, indeed it’s rather rare that it’s true in practice, that approaches which differ from the ideal in one respect are better than all approaches which differ from the ideal in two respects. For example, us non-ideal agents should, especially in high stakes settings, stop and have a little think before acting. The ideal agent of decision theory never stops to have a think. After all, stopping is costly, and the ideal agent gets no gain from incurring that cost.</p>
<p>In general, we differ from the ideal agent in any number of ways. Some of these are respects in which we’d be better off being more like them. For example, they hedge against costly but realistic risks, and typical humans don’t take out as many such hedges as they should. But some of these are respects in which we’d be worse off being more like them. For instance, they never stop to have a think, or put in effort to get better at calculations. Knowing that the ideal agent is <em>F</em> doesn’t tell us whether we should try to be <em>F</em> unless we also know that <em>F</em> is more like hedging rather than more like never trying to get better at calculating. That, unfortunately, is not something which we can really figure out from within the idealised approach to decision theory that is standard these days.</p>
<p>Let’s recap. So far I’ve argued that what decision theories do is describe what characters like IC are like. I haven’t yet said anything about why describing IC is a worthwhile activity. I’ve argued that a few attempts to provide a broadly normative explanation of this activity don’t work. Learning more about IC does not provide useful advice, or tell us which of the options we can do are best, or provide us with a goal worth aiming at. I’ll argue in the next section that the reason to learn about IC is not to get better at decision making, but to get better at understanding decision makers.</p>
</section>
<section id="sec-models" class="level1" data-number="4">
<h1 data-number="4"><span class="header-section-number">4</span> Idealisations as Models</h1>
<p>At the start I said that the word ‘idealised’ gets used differently in ethics and in philosophy of science. The main claim I want to make in this section is that we should understand the idealisations in decision theory in the latter sense. In particular, we should understand them as simplifications. Michael <span class="citation" data-cites="Weisberg2007">Weisberg (<a href="#ref-Weisberg2007" role="doc-biblioref">2007</a>)</span> identifies three kinds of idealisations in science: Galilean, which distort the situation to make computation easier; minimalist, which only include the factors one takes to be causally significant to a situation; and multiple models, where one tries to understand a situation by considering different minimal idealisations with different strengths and weaknesses. The idealisations in decision theory are the second kind. They aren’t particularly computationally tractable, unlike the Galilean idealisations, and there is typically just the one of them.</p>
<p>The idealisations in minimalist models like, say, ideal gas theory, are <em>simplifications</em> rather than <em>perfections</em>. We do not think that having volume is an imperfection. Maybe some religious traditions think this, but it isn’t baked into introductory chemistry. Nor do we think that they are things we should aim for. Introductory chemistry does not imply a <em>Smaller the better!</em> rule for molecules. Rather, it says that volumeless molecules with perfectly elastic collisions are simpler to work with, and that some of the phenomena of real gases can be explained by looking at this simpler model.</p>
<p>We can make sense of what decision theory, as a discipline, is doing if we take it to be engaged in the same style of project. (Whether we can make sense of what individual decision theorists are doing this way is a harder question, one I’ll return to below.) Just like the point masses we use in the ideal gas law, decision theories provide a good description of a certain kind of simplification. The idealisation need not be a perfection for this to be interesting. Indeed, even the idealised creatures studies in decision theory are not completely perfect. They have similar informational limitations to what we do.</p>
<p>This is the point of the basketball example. The idealised self that gets used in decision theory is god-like god-like in one respect - computational ability - but human-like in another - informational awareness. That’s a common feature of idealised models; one doesn’t idealised away from absolutely everything.</p>
<p>That still doesn’t tell us why we build these models. Part of the reason is similar to the reason we ever use minimal models. Minimal models are explanatorily powerful when the difference between the minimal model and reality is not relevant to predicting, explaining, or understanding what happens in the real world. The same thing is true in decision theory. The idealised models of decision theory are, at least sometimes, relevant to predicting, explaining, or understanding what happens in the real world.</p>
<p>When could such an idealised account be relevant? They are relevant when the differences between real people and idealised people are small, relative to the size of the problem. It’s tempting to identify these cases with high stakes situations. After all, in high stakes situations deciders are disposed to throw enough computational resources at the problem that the differences between ordinary people and ideal agents shrinks. But that is isn’t quite right. After all, in many high stakes cases, the decider also throws enough investigative resources at the problem that holding actual knowledge fixed is a bad modelling assumption.</p>
<p>Instead, the cases where decision theory is most helpful for modelling real life situations are ones where there are principled limitations to the decider’s informational capacities. There are two kinds of cases where there are such principled limitations. One is where the information concerns the future, and the decision must be made now. And the other is where the information that someone else has (or at least may have) just as much incentive to suppress the information as the decider has to find it. Most textbook examples of the usefulness of decision theory concern the first kind, though they don’t always make explicit why it matters that the case is future directed. I’m going to work through a case of the second kind that I think is enlightening about the way decision theory is valuable.</p>
<p>Until very recently, used cars sold at a huge discount to new cars, even when the cars were just a few months old with almost no usage. For a long time there was no agreed upon explanation for this phenomenon. The most common theory was that it reflected a preference, or perhaps a prejudice, on the part of buyers. George <span class="citation" data-cites="Akerlof1970">Akerlof (<a href="#ref-Akerlof1970" role="doc-biblioref">1970</a>)</span> showed how this discount could be explained in a model of perfectly rational agents. His model makes the following assumptions.</p>
<ol type="1">
<li>Cars vary a lot in quality, even cars that come from the same production line.</li>
<li>Sellers of used cars know how good the particular car they are selling is.</li>
<li>Buyers of used cars do not know how good any token car is; they only know how good that type of car generally is.</li>
<li>People rarely sell cars they just bought.</li>
<li>Everyone involved is an expected utility maximiser.</li>
</ol>
<p>Based on these five assumptions, Akerlof built a formal model of the market for recently used cars. In the model, the most common reason to sell a car one just bought is the discovery that it was a bad token of that type of car. Knowing that this was the main reason to sell, buyers of used cars demanded a big discount in exchange for the possibility they were buying a dud. But as long as there are enough forced sellers of good recently purchased cars, who prefer whatever money they can get for their car to keeping the car, there can be an equilibrium where lightly used cars sell at a heavy discount to new cars, and it is rational for (some) owners to sell into this market, and for (some) buyers to buy in this market.</p>
<p>If Akerlof was right, and I think he was largely correct, you’d expect the used car discount to fall if either of the following things happened. First, it would fall if production lines got more reliable, and cars off the same line were more similar to one another. And second, it would fall if buyers had access to better tools<a href="#fn8" class="footnote-ref" id="fnref8" role="doc-noteref"><sup>8</sup></a> to judge the quality of used cars. By 2020 both of those things had happened, and the used car discount was almost zero.<a href="#fn9" class="footnote-ref" id="fnref9" role="doc-noteref"><sup>9</sup></a></p>
<p>The philosophical significance of this is that one can’t build models like Akerlof’s without a theory of rational action under uncertainty. The usefulness of philosophical decision theory is that it’s an essential input to useful models, like the Akerlof model. Since those models are useful, getting the inputs to them right is useful.</p>
</section>
<section id="sec-theorists" class="level1" data-number="5">
<h1 data-number="5"><span class="header-section-number">5</span> Theories and Theorists</h1>
<p>So far I’ve argued that decision theory is best understood as the project of developing models of agents that are useful in certain kinds of explanations. I argued that it isn’t obvious what question could have the answers “Don’t bet in Betting! Be perfect in Salesman!”, and in particular what question that we should care about could have those answers. My suggestion is that we should take the question to be about what a certain kind of idealised agent does, and we should care about it because it’s a useful input to explanations.</p>
<p>But, one might object, this isn’t what individual decision theorists take themselves to be doing. Some decision theorists, perhaps most of them, disagree with the arguments in <a href="#sec-idealisations" class="quarto-xref">Section&nbsp;3</a> that decision theory does not have normative implications. They take themselves to be saying how ordinary people should act, or offering advice to ordinary people. Isn’t this in tension with my claims about the aims of decision theory as a practice? I think it isn’t, for four reasons.</p>
<p>First, there isn’t any inconsistency between saying that an institution has an aim, and denying that this is the aim of most people who make up the institution. Still, we’d like to have more to say than just that this is consistent.<a href="#fn10" class="footnote-ref" id="fnref10" role="doc-noteref"><sup>10</sup></a></p>
<p>Second, if I’m right about the aims of decision theory, you’d expect that this would typically <em>not</em> show up in everyday practice. To see this, imagine we have two decision theorists, Daniels and O’Leary, and Daniels agrees with me about the aims of decision theory, while O’Leary says decision theory is about how ordinary people should act. The two of them can still debate ordinary cases, and can still say that this or that decision would be the ideal one to make. They might mean different things by ‘ideal’, but that won’t matter for most purposes. It’s harmless enough for Daniels to describe his idealised agent as ‘rational’, and if he does there will be even less conflict with O’Leary. There will be some differences between them, and I’ll describe some of them soon, but for the most part they won’t show up, even if the view Daniels and I have is correct.</p>
<p>Third, there are cases where what O’Leary says is correct. While it’s not <em>always</em> true that ordinary people should resemble the ideal more than they actually do, it is <em>sometimes</em> true. So Daniels can even agree on a case by case basis that some arguments in decision theory do show us what ordinary people should do. He’ll just think that the argument from decision theory to claims about ordinary rationality has one extra step, namely that in the case in question ordinary people should try to be more like the ideal, in the respect being discussed. This isn’t always true, as Salesman shows, but it is often enough so obviously true that it’s fine to leave it out of the argument. For instance, as mentioned above, when thinking about what kinds of insurance to buy, it’s a fair bet that one should do is what the idealised agent in decision theory actualy does.</p>
<p>Fourth, even if we’re interested in modelling, there is a reason that we’d be interested in the kinds of agents who are computationally perfect, and hence that O’Leary would take to be a rational role model. If we spend a lot of resources coming up with the optimal model for agents with our actual computational limitations, then we risk being superseded when computational capacities change. And given that most computation these days is outsourced to machines, and the machines are getting better literally every year, that’s a very real risk. Looking at the cases when we can treat actual computational resources as close enough to ideal means that we don’t have to throw our work out every time Moore’s Law kicks in. Put another way, in my story of Daniels and O’Leary, Daniels has an incentive from within his own theory to work on problems where he and O’Leary won’t disagree.</p>
<p>So there are a lot of reasons that Daniels and O’Leary wouldn’t have to get into disputes about the point of decision theory, even while doing decision theory. Relatedly, Daniels can take O’Leary’s first-order work within decision theory to be excellent contributions to the modelling project he’s engaged in, even if O’Leary takes himself to be engaged in a different, normative, project, and Daniels thinks that project rests on implausible foundations.</p>
<p>But there will ultimately be some disagreements between them, not just about the point of decision theory, but about its practice. I’ll end this paper with a discussion of what those might be, and why Daniels’s side of the disagreement is plausible.</p>
</section>
<section id="sec-consequences" class="level1" data-number="6">
<h1 data-number="6"><span class="header-section-number">6</span> Consequences</h1>
<p>On my view, standard approaches in decision theory provide a kind of minimal model. It is rare that only one minimal model is suitable for a whole area of study. The ideal gas law is useful, but it is more useful when supplemented with theories about when it breaks down, and about what happens in those cases. The same is true in decision theory.</p>
<p>So on the modelling approach to decision theory, you’d like to see the standard kinds of models, which make those distinctive recommendations in Betting and Salesman, supplemented by other models. We are already seeing that in philosophy in work on limited awareness <span class="citation" data-cites="Steele2021">(<a href="#ref-Steele2021" role="doc-biblioref">Steele and Stefánsson 2021</a>)</span>. And we are seeing it in economics in work on cursed equilibrium <span class="citation" data-cites="EysterRabin2005">(<a href="#ref-EysterRabin2005" role="doc-biblioref">Eyster and Rabin 2005</a>)</span>. On the modelling approach, these are not rivals to the standard model, but useful supplements to it.</p>
<p>The previous paragraph mentioned two kinds of models where we drop some of the idealisations in the standard model. There are also interesting cases where we add in even more idealisations.</p>
<p>If one takes decision theory to be giving a normative theory of rational choice, it’s a very hard problem to say what the options are that Chooser has to choose between <span class="citation" data-cites="Hedden2012">(<a href="#ref-Hedden2012" role="doc-biblioref">Hedden 2012</a>)</span>. The problem is that given the structure of the theory, we want to only have options that Chooser will certainly carry out if they choose them, and that’s hard to guarantee. We can’t say that Chooser chooses to take a holiday in Paris, because the flight might be cancelled, or Chooser might have a medical emergency, and so on. It’s tempting to say that Chooser only ever chooses to try to do things. But this is seriously unintuitive. On the modelling approach, things are easier. We can just say that taking a holiday in Paris is one of Chooser’s options, and have the conditional <em>If Chooser chooses Paris, they will go to Paris</em> be a harmless enough idealisation.</p>
<p>More controversially, I think the same argument can be given for an idealisation that is standard in economics, but not in philosophy: the availability of mixed strategies. In economics, it is usually assumed that if A and B are among Chooser’s options, so is any probabilistic mixture of A and B. Philosophers frequently describe decision problems where this is explicitly ruled out. After all, they say, Chooser might not have any randomizing device available. Setting aside the vexed question of whether such a device is needed to carry out a mixed strategy, a question which turns on just what we take a mixed strategy to be, the fact that such a device <em>might</em> not be available isn’t relevant on the modelling approach. As long as it is a reasonable idealisation that such a device is available, and as long as Chooser has a smartphone it surely is, it will make sense to include the ability to use it in Chooser’s idealised capacities.</p>
<p>But, a philosopher might continue, couldn’t we also ask the question of what Chooser should do if they can’t randomise? In practice, this usually comes up in the context of games that roughly resemble playing rock-paper-scissors against a near perfect predictor <span class="citation" data-cites="SpencerWells2019">(<a href="#ref-SpencerWells2019" role="doc-biblioref">Spencer and Wells 2019</a>)</span>. In those cases if you can’t randomise I think the best advice is to learn to lose graciously. But why couldn’t we say more? Why shouldn’t decision theory apply to them? On the modelling approach I favour, denying someone access to mixed strategies is like denying them access to free and limitless computation. There are things we can say with such a limitation imposed, e.g., it’s good to learn insertion strategies in Salesman. But what we don’t say is that standard decision theory should apply in such a case. The modelling approach opens up the possibility, a possibility I suspect we should take, of treating the ability to randomize perfectly as on a par with the ability to calculate perfectly. It’s a frequently useful idealisation, and it’s helpful to impose it to understand some behaviour.<a href="#fn11" class="footnote-ref" id="fnref11" role="doc-noteref"><sup>11</sup></a> But once it is dropped, we are engaged in a different kind of project to standard decision theory.</p>
<p>That’s not to say that’s a bad project to engage in. Quite the contrary, it’s an excellent project. In <a href="#sec-theorists" class="quarto-xref">Section&nbsp;5</a> I mentioned one reason Daniels might want to be cautious about spending too many resources on particular kinds of optimisations projects for computationally limited agents: the agents will probably get more capacities by the time the project is done. While that’s a good reason for caution, there are two kinds of projects in that vein that are more promising. David <span class="citation" data-cites="Thorstad2024">Thorstad (<a href="#ref-Thorstad2024" role="doc-biblioref">2024</a>)</span> has a good recent book that addresses both of these. One is the question of what the criterion of rightness is for decision making under computational limitations.<a href="#fn12" class="footnote-ref" id="fnref12" role="doc-noteref"><sup>12</sup></a> A second is whether there are things which can be said about how to manage the limits that are imposed by the architecture of our cognitive systems.</p>
<p>The point I’m making is not that the projects described in the last paragraph are valuable. That’s something O’Leary could, and probably would, agree with. Rather, my claim is that they are more continuous with ordinary decision theory than is often appreciated. What I want to reject is the division between “ideal” decision theory, as exemplified by the nine theories mentioned at the top, and “non-ideal” decision theory, as exemplified by work like Thorstad’s. On that picture, the theories output different kinds of oughts, or perhaps one outputs an ought and the other a purely descriptive claim. On my view, both kinds of theory involve different kinds of idealisation. (Really most social science involves some idealisation; it’s hard to say anything without simplifying somewhere.) The difference between the theories is not how they judge us, but in what they explain. Neither theory is particularly promising as a universal theory of human behaviour. But both theories are very useful as explanations of particular phenomenon, and that should be good enough.<a href="#fn13" class="footnote-ref" id="fnref13" role="doc-noteref"><sup>13</sup></a></p>

</section>


<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" role="doc-bibliography" id="quarto-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list">
<div id="ref-Ahmed2014" class="csl-entry" role="listitem">
Ahmed, Arif. 2014. <em>Evidence, Decision and Causality</em>. Cambridge: <span>C</span>ambridge <span>U</span>niversity <span>P</span>ress.
</div>
<div id="ref-Akerlof1970" class="csl-entry" role="listitem">
Akerlof, George. 1970. <span>“The Market for "Lemons": Quality Uncertainty and the Market Mechanism.”</span> <em>Quarterly Journal of Economics</em> 84 (3): 488–500. <a href="https://doi.org/10.2307/1879431">https://doi.org/10.2307/1879431</a>.
</div>
<div id="ref-BuchakRisk" class="csl-entry" role="listitem">
Buchak, Lara. 2013. <em>Risk and Rationality</em>. Oxford: Oxford University Press.
</div>
<div id="ref-Burkhart2011" class="csl-entry" role="listitem">
Burkardt, John. 2011. <span>“Cities.”</span> <a href="https://people.sc.fsu.edu/~jburkardt/datasets/cities/cities.html">https://people.sc.fsu.edu/~jburkardt/datasets/cities/cities.html</a>.
</div>
<div id="ref-EysterRabin2005" class="csl-entry" role="listitem">
Eyster, Erik, and Matthew Rabin. 2005. <span>“Cursed Equilibrium.”</span> <em>Econometrica</em> 73 (5): 1623–72. <a href="https://10.1111/j.1468-0262.2005.00631.x">10.1111/j.1468-0262.2005.00631.x</a>.
</div>
<div id="ref-Fusco2024" class="csl-entry" role="listitem">
Fusco, Melissa. 2024. <span>“Absolution of a Causal Decision Theorist.”</span> <em>No<span>û</span>s</em> 58 (3): 616–43. <a href="https://doi.org/10.1111/nous.12459">https://doi.org/10.1111/nous.12459</a>.
</div>
<div id="ref-Gallow2020" class="csl-entry" role="listitem">
Gallow, J. Dmitri. 2020. <span>“The Causal Decision Theorist’s Guide to Managing the News.”</span> <em>The Journal of Philosophy</em> 117 (3): 117–49. <a href="https://doi.org/10.5840/jphil202011739">https://doi.org/10.5840/jphil202011739</a>.
</div>
<div id="ref-GibbardHarper1978" class="csl-entry" role="listitem">
Gibbard, Allan, and William Harper. 1978. <span>“Counterfactuals and Two Kinds of Expected Utility.”</span> In <em>Foundations and Applications of Decision Theory</em>, edited by C. A. Hooker, J. J. Leach, and E. F. McClennen, 125–62. Dordrecht: Reidel.
</div>
<div id="ref-HashlerHornik2007" class="csl-entry" role="listitem">
Hahsler, Michael, and Kurt Hornik. 2007. <span>“TSP—Infrastructure for the Traveling Salesperson Problem.”</span> <em>Journal of Statistical Software</em> 23 (2): 1–21. <a href="https://doi.org/10.18637/jss.v023.i02">https://doi.org/10.18637/jss.v023.i02</a>.
</div>
<div id="ref-Hedden2012" class="csl-entry" role="listitem">
Hedden, Brian. 2012. <span>“Options and the Subjective Ought.”</span> <em>Philosophical Studies</em> 158 (2): 343–60. <a href="https://doi.org/10.1007/s11098-012-9880-0">https://doi.org/10.1007/s11098-012-9880-0</a>.
</div>
<div id="ref-Joyce1999" class="csl-entry" role="listitem">
Joyce, James M. 1999. <em>The Foundations of Causal Decision Theory</em>. Cambridge: Cambridge University Press.
</div>
<div id="ref-Knight1921" class="csl-entry" role="listitem">
Knight, Frank. 1921. <em>Risk, Uncertainty and Profit</em>. Chicago: University of Chicago Press.
</div>
<div id="ref-LevinsteinSoares2020" class="csl-entry" role="listitem">
Levinstein, Benjamin Anders, and Nate Soares. 2020. <span>“Cheating Death in Damascus.”</span> <em>Journal of Philosophy</em> 117 (5): 237–66. <a href="https://doi.org/10.5840/jphil2020117516">https://doi.org/10.5840/jphil2020117516</a>.
</div>
<div id="ref-Lewis1981b" class="csl-entry" role="listitem">
Lewis, David. 1981. <span>“Causal Decision Theory.”</span> <em>Australasian Journal of Philosophy</em> 59 (1): 5–30. <a href="https://doi.org/10.1080/00048408112340011">https://doi.org/10.1080/00048408112340011</a>.
</div>
<div id="ref-Lewis1994b" class="csl-entry" role="listitem">
———. 1994. <span>“Reduction of Mind.”</span> In <em>A Companion to the Philosophy of Mind</em>, edited by Samuel Guttenplan, 412–31. Oxford: Blackwell. <a href="https://doi.org/10.1017/CBO9780511625343.019">https://doi.org/10.1017/CBO9780511625343.019</a>.
</div>
<div id="ref-Lewis-Gorman-19041989" class="csl-entry" role="listitem">
———. (1989) 2020. <span>“Letter to Jonathan Gorman, 19 April 1989.”</span> In <em>Philosophical Letters of David <span>K</span>. Lewis</em>, edited by Helen Beebee and A. R. J. Fisher, 2:472–73. Oxford: Oxford University Press.
</div>
<div id="ref-LipseyLancaster" class="csl-entry" role="listitem">
Lipsey, R. G., and Kelvin Lancaster. 1956. <span>“The General Theory of Second Best.”</span> <em>Review of Economic Studies</em> 24 (1): 11–32. <a href="https://doi.org/10.2307/2296233">https://doi.org/10.2307/2296233</a>.
</div>
<div id="ref-MandelkernEtAl2017" class="csl-entry" role="listitem">
Mandelkern, Matthew, Ginger Schultheis, and David Boylan. 2017. <span>“Agentive Modals.”</span> <em>Philosophical Review</em> 126 (3): 301–43. <a href="https://doi.org/10.1215/00318108-3878483">https://doi.org/10.1215/00318108-3878483</a>.
</div>
<div id="ref-Norcross1997" class="csl-entry" role="listitem">
Norcross, Alastair. 1997. <span>“Consequentialism and Commitment.”</span> <em>Pacific Philosophical Quarterly</em> 78 (4): 380–403. <a href="https://doi.org/10.1111/1468-0114.00045">https://doi.org/10.1111/1468-0114.00045</a>.
</div>
<div id="ref-Podgorski2022" class="csl-entry" role="listitem">
Podgorski, Aberlard. 2022. <span>“Tournament Decision Theory.”</span> <em>No<span>û</span>s</em> 56 (1): 176–203. <a href="https://doi.org/10.1111/nous.12353">https://doi.org/10.1111/nous.12353</a>.
</div>
<div id="ref-Railton1984" class="csl-entry" role="listitem">
Railton, Peter. 1984. <span>“Alienation, Consequentialism, and the Demands of Morality.”</span> <em>Philosophy and Public Affairs</em> 13 (2): 134–71.
</div>
<div id="ref-Robinson1949" class="csl-entry" role="listitem">
Robinson, Julia. 1949. <span>“On the Hamiltonian Game (a Traveling Salesman Problem).”</span> Santa Monica, CA: The RAND Corporation.
</div>
<div id="ref-Roussos2022" class="csl-entry" role="listitem">
Roussos, Joe. 2022. <span>“Modelling in Normative Ethics.”</span> <em>Ethical Theory and Moral Practice</em> 25: 865–89. <a href="https://doi.org/10.1007/s10677-022-10326-4">https://doi.org/10.1007/s10677-022-10326-4</a>.
</div>
<div id="ref-Roussos2025" class="csl-entry" role="listitem">
———. 2025. <span>“Normative Formal Epistemology as Modelling.”</span> <em>British Journal for the Philosophy of Science</em> 76 (2): 421–48. <a href="https://doi.org/10.1086/718493">https://doi.org/10.1086/718493</a>.
</div>
<div id="ref-Schrijver2005" class="csl-entry" role="listitem">
Schrijver, Alexander. 2005. <span>“On the History of Combinatorial Optimization (till 1960).”</span> <em>Handbooks in Operations Research and Management Science</em> 12: 1–68. <a href="https://doi.org/10.1016/S0927-0507(05)12001-5">https://doi.org/10.1016/S0927-0507(05)12001-5</a>.
</div>
<div id="ref-Sidgwick1907" class="csl-entry" role="listitem">
Sidgwick, Henry. 1907. <em>The Methods of Ethics</em>. Seventh. London: Macmillan.
</div>
<div id="ref-Skyrms1990" class="csl-entry" role="listitem">
Skyrms, Brian. 1990. <em>The Dynamics of Rational Deliberation</em>. Cambridge, MA: Harvard University Press.
</div>
<div id="ref-Spencer2021" class="csl-entry" role="listitem">
Spencer, Jack. 2021. <span>“An Argument Against Causal Decision Theory.”</span> <em>Analysis</em> 81 (1): 52–61. <a href="https://doi.org/10.1093/analys/anaa037">https://doi.org/10.1093/analys/anaa037</a>.
</div>
<div id="ref-SpencerWells2019" class="csl-entry" role="listitem">
Spencer, Jack, and Ian Wells. 2019. <span>“Why Take Both Boxes?”</span> <em><span>P</span>hilosophy and <span>P</span>henomenological <span>R</span>esearch</em> 99 (1): 27–48. <a href="https://doi.org/10.1111/phpr.12466">https://doi.org/10.1111/phpr.12466</a>.
</div>
<div id="ref-Steele2021" class="csl-entry" role="listitem">
Steele, Katie, and H. Orri Stefánsson. 2021. <em>Beyond Uncertainty: Reasoning with Unknown Possibilities</em>. Elements in Decision Theory and Philosophy. Cambridge University Press.
</div>
<div id="ref-Sutton2000" class="csl-entry" role="listitem">
Sutton, John. 2000. <em>Marshall’s Tendencies: What Can Economists Know?</em> Cambridge, MA: <span>MIT</span> Press.
</div>
<div id="ref-Thorstad2024" class="csl-entry" role="listitem">
Thorstad, David. 2024. <em>Inquiry Under Bounds</em>. Oxford University Press.
</div>
<div id="ref-wiki-salesman" class="csl-entry" role="listitem">
Travelling salesman problem. 2024. <span>“Travelling Salesman Problem— <span>W</span>ikipedia<span>,</span> the Free Encyclopedia.”</span> <a href="https://en.wikipedia.org/w/index.php?title=Travelling_salesman_problem&amp;oldid=1209291065">https://en.wikipedia.org/w/index.php?title=Travelling_salesman_problem&amp;oldid=1209291065</a>.
</div>
<div id="ref-Wedgwood2013a" class="csl-entry" role="listitem">
Wedgwood, Ralph. 2013. <span>“Gandalf’s Solution to the Newcomb Problem.”</span> <em>Synthese</em> 190 (14): 2643–75. <a href="https://doi.org/10.1007/s11229-011-9900-1">https://doi.org/10.1007/s11229-011-9900-1</a>.
</div>
<div id="ref-Weisberg2007" class="csl-entry" role="listitem">
Weisberg, Michael. 2007. <span>“Three Kinds of Idealization.”</span> <em>The Journal of Philosophy</em> 104 (12): 639–59. <a href="https://doi.org/10.5840/jphil20071041240">https://doi.org/10.5840/jphil20071041240</a>.
</div>
</div></section><section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes"><h2 class="anchored quarto-appendix-heading">Footnotes</h2>

<ol>
<li id="fn1"><p>The arguments here are intended to support a theory like Fusco’s, but in a fairly roundabout way, but the connection between what I say here and Fusco’s theory would take a paper as long as this one to set out.<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2"><p>For a thorough history of the problem, see <span class="citation" data-cites="Schrijver2005">Schrijver (<a href="#ref-Schrijver2005" role="doc-biblioref">2005</a>)</span>. For an accessible history of the problem, which includes these references, see the Wikipedia article on the <span class="citation" data-cites="wiki-salesman">Travelling salesman problem (<a href="#ref-wiki-salesman" role="doc-biblioref">2024</a>)</span>.<a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn3"><p>The 257 cities are the cities in the lower 48 states from the 312 cities in North America that John <span class="citation" data-cites="Burkhart2011">Burkardt (<a href="#ref-Burkhart2011" role="doc-biblioref">2011</a>)</span> mapped in his dataset USCA312.<a href="#fnref3" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn4"><p>To implement both this algorithm and the optimisation I’ll mention below, I’ve used the TSP package by Michael Hashler and Kurt Hornik <span class="citation" data-cites="HashlerHornik2007">(<a href="#ref-HashlerHornik2007" role="doc-biblioref">2007</a>)</span>. The description of the two steps owes a lot to their summaries in the package documentation.<a href="#fnref4" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn5"><p>The algorithm is silent on which city you start with, and typical implementations of it choose the starting city randomly.<a href="#fnref5" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn6"><p>Exactly which probability it is, or indeed whether it even strictly is a probability, varies by which theory one chooses. But the basic idea that Chooser multiples something probability like by a utility is common across theories<a href="#fnref6" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn7"><p>I’m taking this distinction from Peter <span class="citation" data-cites="Railton1984">Railton (<a href="#ref-Railton1984" role="doc-biblioref">1984</a>)</span>, though his isn’t the earliest use of the distinction. Alastair <span class="citation" data-cites="Norcross1997">Norcross (<a href="#ref-Norcross1997" role="doc-biblioref">1997</a>)</span> notes that the phrase “criterion of rightness” is used in the context of drawing this distinction by Sidgwick <span class="citation" data-cites="Sidgwick1907">(<a href="#ref-Sidgwick1907" role="doc-biblioref">1907, bk. 4</a>, Chapter 1, §1)</span>.<a href="#fnref7" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn8"><p>Better that is than a drive around the block test drive.<a href="#fnref8" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn9"><p>Then during the pandemic very strange things happened in the used car market and the ‘discount’ arguably went negative. Whatever was happening there was not explained by the Akerlof model.<a href="#fnref9" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn10"><p>Though if no one disagreed with my claims about the aim of the institution, this paper would be useless for a different reason.<a href="#fnref10" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn11"><p>For a really nice example of this, see the explanation of bidding behaviour on oil rights in <span class="citation" data-cites="Sutton2000">Sutton (<a href="#ref-Sutton2000" role="doc-biblioref">2000</a>)</span>.<a href="#fnref11" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn12"><p>I favour a version of reliabilism here, but I won’t argue for that in this paper.<a href="#fnref12" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn13"><p><strong>Declarations</strong>: I have no conflicts of interest to report. I used Anthropic’s LLM Sonnet 4.5 to check the document for spelling errors and to help with the code used to produce the figures.<a href="#fnref13" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section></div></main>
<!-- /main column -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
      const outerScaffold = trigger.parentElement.cloneNode(true);
      const codeEl = outerScaffold.querySelector('code');
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
</div> <!-- /content -->




</body></html>