---
title: "Gamified Decision Theory"
author: "Brian Weatherson"
format: html
editor: visual
bibliography: ../rBib.bib
number-sections: true
---

# Introduction {#sec-intro}

Textbook versions of game theory embed a distinctive approach to decision theory. That theory isn't always made explicit, and it isn't always clear how it handles some cases. But we can extract one interesting and plausible theory, which I'll call Gamified Decision Theory (GDT), from these textbooks. There are nine characteristics of GDT (as I'll understand it) that I will focus on, with the bulk of this paper one in each of the following nine sections.

1.  **Idealised**; GDT is a theory of what ideal deciders do.
2.  **Expectationist**; the ideal decider prefers getting more expected value to getting less.
3.  **Causal**; GDT is a variety of Causal Decision Theory (CDT).
4.  **Allows Mixtures**; the ideal decider can perform a probabilistic mixture of any acts they can perform.
5.  **Ratificationist**; the ideal decider endorses the decisions they make.
6.  **Indecisive**; GDT sometimes says that multiple options are permissible, and they are not equally good.
7.  **Dual Mandate**; in a dynamic choice, the ideal decider will follow a plan that's permissible, and take choices at every stage that are permissible.
8.  **Substantive Probability**; the ideal decider has rational credences.
9.  **Weak Dominance, Once**; the ideal decider will not choose weakly dominated options, but they may choose options that would not survive iterated deletion of weakly dominated strategies.

This is not going to be a work of exegesis, poring over game theory texts to show that they really do endorse all of 1-9. In fact it wouldn't take much work to show that they endorse 1-5, so the work wouldn't be worth doing. And while some books endorse 8 and 9, it would take a lot more investigative work than I'm going to do here to show that anything like a majority of them do. It would be interesting, but not obviously a philosophical question, to see what proportion endorse 6 and 7. But I'm going to set that aside.

What I do want to argue is that you can find some support for all of these in some game theory textbooks, and that combined they produce a plausible decision theory. While the textbooks don't all agree, for simplicity I'm going to focus on one book: Giacomo Bonanno's *Game Theory* @Bonanno2018. This book has two important virtues: it is philosophically deep, and it is available for free. It isn't hard to find a game theory text with one or other of these virtues, but few have both. So it will be our primary guide in what follows, along with some primary sources (most of which are referenced in that book).

Methodologically, this paper differs from most works in decision theory in two ways. It has been a commonplace since @Nozick1969 to include demons, who are arbitrarily good at predicting a decision, in problems. Some of the cases here will involve two such demons, each of which is arbitrarily good at predicting a decision, and whose errors are probabilistically independent. Second, I'm going to rely less on intuitions about particular cases, and more on intuitions that certain cases should be treated the same way. This makes sense given the history of the field. There is much less consensus about what to do in Newcomb problems than about which problems are Newcomb problems. Judgments, or intuitions if you prefer, about how to classify problems seem more stable and more reliable, and they will be central to this paper.

\[TBC\]

# Idealised {#sec-ideal}

Consider the following decision problem. Chooser (our main protagonist) is going to be given a series of multiplication problems, where each of the multiplicands is a four digit number. Chooser doesn't have access to any kind of calculating device, and has no special arithmetic ability. For each question, if Chooser says the right answer, they get \$2; if they pass, they get \$1; if they say the wrong answer, they get nothing. @tbl-multi has the payout in table form.

|           |                        |                          |
|----------:|:----------------------:|:------------------------:|
|           | **Best Guess Correct** | **Best Guess Incorrect** |
| **Guess** |          \$2           |           \$0            |
|  **Pass** |          \$1           |           \$1            |

: The multiplication game {#tbl-multi}

Every variety of decision theory defended in philosophy journals in recent years, and every game theory textbook, says that Chooser should simply say the correct answer. After all, Chooser should have probabilistically coherent credences, and every mathematical truth has probability 1, so whatever the correct answer is, saying it is a sure \$2.

This is completely terrible advice. Chooser should pass every time, unless the question is something boring like 1000 times 2000. The chance of them getting a question like 5278 times 9713 correct are way less than one in two, so they are better off passing.

This doesn't mean that every philosophical decision theory, and every game theory textbook, is wrong. Those theories are not in the business of giving advice to people like Chooser. They are in the business of saying what it would be ideal, in some sense of ideal, for Chooser to do. And it would be ideal for Chooser to be a reliable computer, so if Chooser were reliable, they would always give the correct answer.

There is a big question about why we should care about what would have if Chooser were ideal. Chooser is not in fact ideal, so who cares what they would do if they were different. One might think that knowing what the ideal is gives Chooser something to aim for. Even if Chooser is not ideal, they can try to be closer to the ideal. The problem is that trying to be more like the ideal will make things worse. The ideal agent will never pass, and even if Chooser doesn't know the answers to the particular questions, they can know this fact. So if they try to be more like the ideal, they will never pass, and things will go badly.[^1]

[^1]: This is a special case of Lipsey and Lancaster's Theory of the Second Best @LipseyLancaster1956. If you don't have control over every parameter, setting the parameters you do control to the ideal values is generally inadvisable.

In philosophy we have two very different uses of the term 'idealisation'. One is the kind of idealisation we see in, for example, Ideal Observer theories in ethics. The other is the kind of idealisation we see in, for example, Ideal Gas models in chemistry. It's important to not confuse the two. Think about the volumeless, infinitely dense, molecules in an Ideal Gas model. To say that this is an idealised model is not to say that having volume, taking up space, is an imperfection. The point is not to tell molecules what the perfect size is. ("The only good molecule is a volumeless molecule.") Nor is it to tell them that they should approximate the ideal. ("Smaller the better, fellas.") It's to say that for some predictive and explanatory purposes, molecules behave no differently to how they would behave if they took up no space.[^2]

[^2]: I'm drawing here on work on the nature of idealisations by Michael @Strevens2008 and by Kevin @Davey2011.

The best way to understand decision theorists, and game theorists, is that they are using idealisations in this latter sense. The ideal choosers of decision theory are not like the Ideal Observers in ethics, but like the Ideal Gases. The point of the theory is to say how things go in a simplified version of the case, and then argue that this is useful for predictive and explanatory purposes because, at least some of the time, the simplifications don't make a difference.

One nice example of this working is George Akerlof's discussion of the used car market @Akerlof1970. In the twentieth century, it was common for lightly used cars to sell at a massive discount to new cars. There was no good explanation for this, and it was often put down to a brute preference for new cars. What Akerlof showed was that a model where (a) new cars varied substantially in quality, and (b) in the used car market, buyers had less information about the car than sellers, you could get a discount similar to what you saw in real life even if the buyers had no special preference for new cars. Rather, buyers had a preference for good cars, and took the fact that this car was for sale to be evidence that it was badly made. It was important for Akerlof's explanatory purposes that he could show that people were being rational, and this required that he have a decision theory that they followed. In fact what he used was something like GDT. We now have excellent evidence that something like his model was correct. As the variation in quality of new cars has declined, and the information available to buyers of used cars has risen, the used car discount has just about vanished. (In fact it went negative during the pandemic, for reasons I don't at all understand.)

I'll end this section with a response to one objection, one caveat, and one surprising bonus to doing idealised decision theory this way.

The objection is that decision theory isn't actually that helpful for prediction and explanation. If all that it says are things like when rain is more probable, more people take umbrellas, that doesn't need a whole academic discipline. The response to that is that in non-cooperative games, the predictions, and explanations, can be somewhat surprising. One nice case of this is the discussion of Gulf of Mexico oil leases in @Wilson1967.[^3] But here's a simpler surprising prediction that you need something like GDT to get.[^4]

[^3]: I learned about this paper from the excellent discussion of the case in @Sutton2000.

[^4]: A somewhat similar point is made in the example of the drowning dog on page 216 of @Bonanno2018.

Imagine Row and Column are playing rock-paper-scissors. A bystander, C, says that he really likes seeing rock beat scissors, so he will pay whoever wins by playing rock \$1. Assuming that Row and Column have no ability to collude, the effect of this will be to shift the payouts in the game they are playing from left table to right table, where *x* is the value of the dollar compared to the value of winning the game. This changes the game they are playing from @tbl-rps-basic to @tbl-rps-modified.

::: {#tbl-rps layout-ncol="2"}
|              |          |           |              |
|-------------:|:--------:|:---------:|:------------:|
|              | **Rock** | **Paper** | **Scissors** |
|     **Rock** |   0,0    |   -1,1    |     1,-1     |
|    **Paper** |   1,-1   |    0,0    |     -1,1     |
| **Scissors** |   -1,1   |   1,-1    |     0,0      |

: Original game {#tbl-rps-basic}

|              |          |           |              |
|-------------:|:--------:|:---------:|:------------:|
|              | **Rock** | **Paper** | **Scissors** |
|     **Rock** |   0,0    |   -1,1    |   1+*x*,-1   |
|    **Paper** |   1,-1   |    0,0    |     -1,1     |
| **Scissors** | -1,1+*x* |   1,-1    |     0,0      |

: Modified game {#tbl-rps-modified}

Two versions of Rock-Paper-Scissors
:::

The surprising prediction is that this will *decrease* the frequency with which rock is played, and the larger *x* is, the larger the decrease will be. Simple rules like "When behavior is rewarded, it happens more often" don't always work in strategic settings, and it takes some care to tell when they do work.

The caveat is that

-   Objection: Only get boring predictions. Response: Not in Nash equilibrium games.
-   Caveat: Doesn't it matter that these are rational, not arbitrary simplifcations? Answer: Yes, but only because it means that they simplification stays working even when the stakes go up.
-   Bonus: Cursed equilibrium.

# Expectationist {#sec-expect}

There is a strange split in contemporary decision theory. On the one hand, there are questions about the way to model attitudes to risk, largely organised around the challenge to orthodoxy from @Quiggin1982 and @BuchakRisk. On the other hand, there are questions about what to do in cases where the states are causally but not probabilistically independent of one's actions, with the central case being Newcomb's Problem [@Nozick1969]. The strange split is that these two literatures have almost nothing in common.[^5]

[^5]: There is a survey article from a few years ago - @Elliot2019 - that has summaries of the then state-of-the-art on these two questions. And it makes it very striking how little the literatures on each of them overlap.

This split might seem to make sense when one reflects that there is no logical difficulty in endorsing any prominent answer to one set of questions with any prominent answer to the other set. But things get more difficult quickly. For one thing, one answer to questions about risk, what I'll call the expectationist answer, is universally assumed by people working on issues around Newcomb's Problem. For another, the argument forms used in the two debates are similar, and that should affect how the two arguments go. 

Say that a normal decision problem is one where the states are probabilistically independent of the choices. A simple example is betting on a coin flip. In talking about normal decision problems I'll normally label the states H, for Heads, or T for Tails. Unless otherwise stated coins are fair, so H and T are equiprobable. 

Say that an abnormal decision problem is simply one that isn't normal. A simple example is where the states are predictions of an arbitrarily accurate predictor. I'll normally label such states as PX, where X is a choice the agent may make. In these cases the Predictor is arbitrarily accurate unless otherwise stated, but we will spend some time with more error prone predictors. 

The view I call expectationism has two parts. First, it says that in normal decision problems, the rational agent maximises the expected value of something like the value of their action. Second, it says that something like this expected value plays an important role in the theory of abnormal decision problems. These definitions are vague, so there are possible borderline cases. But in practice this doesn't arise, at least in the philosophy literature. Everyone working on abnormal problems is an expectationist. Indeed, most work assumes without even saying it that the first clause of expectationism is correct. Everyone working on normal problems makes it clear which side they fall on, so there is no vagueness there. And every game theory text is expectationist. 

I'm going to mostly follow suit. So why am I belabouring this point? One small reason and one large reason. The small reason is that one of the arguments I'll give concerning abnormal cases generalises to an argument for expectationism about normal cases. The other reason is dialectical. In the debate about normal cases, the method of gathering intuitions about cases, and seeing which theory fits the intuitions best, does not favor expectationism. On the contrary, the Quiggin-Buchak theory does a much better job on that score. 

# Causal {#sec-causal}

It shouldn't be controversial to claim that game theory textbooks are committed a broadly causal version of decision theory. For one thing, they always recommend defecting in Prisoners' Dilemma, even when playing with a twin. As David Lewis showed, this is equivalent to recommending two-boxing in Newcomb's Problem [@Lewis1979e]. They endorse the causal decision theorist's signature argument form: the deletion of strongly dominated strategies. Indeed, the typical book introduces this before it introduces anything about probability. When they do get around to probabilities, they tend to define the expected value of a choice in a way only a causal decision theorist could endorse. In particular, they define expected values using unconditional, rather than conditional, probabilities.[^6] And the probabilities are simply probabilities of states, not probabilities of any kind of counterfactual. Indeed, you can go entire textbooks without even getting a symbol for a counterfactual conditional.

[^6]: See, for instance, the introduction of them on page 136 of @Bonanno2018. And note that we get 135 pages before the notion of an expectation is introduced; that's how much is done simply with dominance reasoning

What's more controversial is that they are right to adopt a kind of causal decision theory (CDT).[^7] In the recent literature, I think there are three main kinds of objections to CDT. First, it leaves one with too little money in Newcomb's Problem. Second, it gives the wrong result in problems like Frustrator [@SpencerWells2019]. Third, it gives the wrong result in asymmetric Death in Damascus cases, as in @Egan2007-EGASCT. Fourth, it gives strange results in Ahmed's *Betting on the Past* and *Betting on the Laws* cases. I'm going to set those problems aside because (a) they require that an agent not always be aware of what actions are possible, and that's inconsistent with the idealisations introduced in @sec-ideal, and (b) they raise questions about just what it means for two things to be causally independent that go beyond the scope of this paper.

[^7]: Note that I'm using *CDT* here as the name of a family of theories, not a particular theory. So it's not a great name; Causal Decision Theory is not a theory. Different versions of CDT can, and do, differ in what they say about the Stag Hunt cases I'll discuss in @sec-indecisive. But the label seems entrenched, so I'll use it. In contrast, evidential decision theory, EDT, is a theory; it is a full account of what to do in all cases.

The intuitions behind the asymmetric Death in Damascus cases are inconsistent with the Exit Principle that I'll discuss in @sec-indecisive. The Frustrator cases are no problem for a version of CDT that says that idealised agents can always play mixed strategies. Like the game theorists, I will also assume mixed strategies are available, and I'll come back in @sec-mixed to why that assumption should be allowed.

That leaves the point that CDT leaves one poorly off in Newcomb's Problem, while other theories, like evidential decision theory (EDT) leave one well off. This isn't a particular mark against CDT, since other theories, like EDT, leave one poorly off in some situations. Here is one such case.

There are two demons, who will predict what Chooser will do. Both of them are arbitrarily good, though not quite perfect, and their errors are independent. Chooser will play either the left or right game in @tbl-edt-war.

::: {#tbl-edt-war layout-ncol="2"}
|          |         |           |
|---------:|:-------:|:---------:|
|          | **PUp** | **PDown** |
|   **Up** |    1    |     3     |
| **Down** |    0    |     2     |

: Demon-1 predicts Down {#tbl-war-left}

|          |         |           |
|---------:|:-------:|:---------:|
|          | **PUp** | **PDown** |
|   **Up** |  1001   |   1003    |
| **Down** |  1000   |   1002    |

: Demon-1 predicts Up {#tbl-war-right}

A Newcomb problem with two demons
:::

If Demon-1 predicts that Chooser will play Down, Demon-1 will offer Chooser @tbl-war-left; if Demon-1 predicts that Chooser will play Up, Demon-1 will offer Chooser @tbl-war-lighteft. Then Demon-2's prediction will be used for determining whether the payout is from column PU or PD. In almost all cases, if Chooser uses CDT, they will get 1001, while if they use EDT, they will get 2. So in this case, CDT will get more than EDT.

This case is not meant as an objection to EDT. It is perfectly fair for the evidential decision theorist to complain that they have simply been the victim of a Demon who intends to punish users of EDT, and reward users of CDT. That seems a perfectly fair complaint. But if the evidential decision theorist makes it, they cannot object when causal decision theorists, such as @Lewis1981e, use the same language to describe Newcomb's Problem. The 'objection' that CDT leaves one poorly off in one particular case is equally an objection to everyone, and so it is an objection to no one.

One might worry at this stage that I haven't shown that everyone is vulnerable to this kind of 'objection', just that CDT and EDT are equally vulnerable to it. In particular, so-called 'resolute' decision theories will choose one-box in Newcomb's Problem, and Up in @tbl-edt-war, and so be enriched both times. The so-called 'foundational decision theory' that @LevinsteinSoares2020 endorse also makes that pair of choices. But those theories are vulnerable to much more serious objections, that I'll come to in @sec-dualmandate.

So I conclude that there is no good objection to adopting a broadly causal decision theory, much as the game theorists do. But which version of CDT do they adopt, and are they right to do so? That will take us much more time.

# Mixtures {#sec-mixed}

Perhaps the biggest difference between the decision theory found in game theory textbooks, and the one found in philosophy journals, concerns the status of mixed strategies. In the textbooks, mixed strategies are brought in almost without comment, or perhaps with a remark about their role in a celebrated theorem by Nash1944. In philosophy journals, the possibility of mixed strategies is often dismissed almost as quickly.

The philosophers' dismissal is usually accompanied by one or both of these reasons.\^These reasons are both offered, briefly, by Nozick - cite here - in the foundational document of modern decision theory. Other philosophers have followed him in using them, but

# Ratificationist

Solution concepts in game theory tend to be equilibria. And by an equilibria, everyone is happy with their moves knowing what all the moves of all the players are. (Or, at least, they are as happy as they can be.) Put in decision theoretic terms, that means that all solutions are ratifiable; Chooser is happy with their choice once it is made.

Ratificationism used to be a more popular view among decision theorists. Richard @Jeffrey1983 added a ratifiability constraint to a broadly evidential decision theory. And

insert references endorsed versions of causal decision theory that included a ratifiability constraint. But it lost popularity for two reasons.

One was the existence of cases where there is (allegedly) no ratifiable option. Table below is one such case.

PU PD U 3 5 D 4 3

If Chooser plays U, they would prefer to play D. If Chooser plays D, they would prefer to play U.

Things get worse if we add an option that is ratifiable, but unfortunate, as in Table below

PU PD PX U 3 5 0 D 4 3 0 X 0 0 0

The only ratifiable option is X, but surely it is worse than U or D. One might avoid this example by saying that there is a weak dominance constraint on rational choices, as well as a ratifiability constraint. That won't solve the problem, but it will turn it into a problem like First table, where there is no good solution. But that won't help us much, since in Table below there is no weakly dominant option, but X is surely still a bad play.

PU PD PX U 3 5 0 D 4 3 0 X 0 0 0.01

A better option is to insist, as I argued in previous section, that if Chooser is rational, they can play a mixed strategy. In all three of these games, the mixed strategy of (0.5 U, 0.5 D) will be ratifiable, as long as Chooser forms the belief (upon choosing to play this), that Demon will play the mixed strategy (1/3 U, 2/3 D). And that's a sensible thing for Demon to play, since it is the only strategy that is ratifiable for Demon if Demon thinks Chooser can tell what they are going to do. And given Chooser's knowledge of Demon's goals, Chooser can tell what Demon is going to do once they choose.

So if mixed strategies are allowed, none of the problems for ratifiability persist. And since mixed strategies should be allowed, since Chooser is an ideal practical actor, and not being able to play mixed strategies is an imperfection.

Moreover, ratifiability is an intuitive constraint. There is something very odd about saying that such-and-such is a rational thing to do, but whoever does it will regret it the moment they act. So I'll follow the game theory textbooks in saying ratifiability should be part of the correct decision theory.

This does not mean that we need to have an explicit ratifiability clause in our theory. It could be, and arguably should be, that ratifiability is a consequence of the theory, not an explicit stipulation.

Could we defend ratifiability without appeal to mixed strategies? It's not a completely impossible task, but nor is it an appealing one.

First table poses no serious problem. Without mixed strategies, the case is simply a dilemma. And we know that there are dilemmas in decision theory. Here's one familiar example. A sinner faces Judgment Day. Because of his sins, it is clear things will end badly for him. But he has done some good in his life, and that counts for something. The judge thinks he should get some days in the Good Place before being off to the Bad Place. But the judge can't decide how many. So the judge says to the sinner to pick a natural number *n*, and the sinner will spend *n* days in the Good Place, and then goodbye. This clearly is a dilemma; for any large *n*, saying *n*! would be considerably better.\^Note that this is true even if days in heaven have diminishing marginal utility, so the dilemma can arise even if we work within bounded utility theory. This is not just the kind of problem, as discussed by XXXX, that arises in decision theory with unbounded utilities. @Ahmed2012 says that it is an objection to a theory that it allows dilemmas in cases with finitely many options; dilemmas should only arise in infinite cases. But he doesn't really argue for this, and I can't see what an argument would be. Once you've allowed dilemmas of any kind, the door is open to all of them.

Nor does Second table pose a problem, since as I said, the ratifiability theorist could add a weak dominance constraint and turn Second table into another dilemma.

The problem is Third table. There the ratifiability theorist who does not allow mixed strategies has to say that the case is an odd kind of Newcomb Problem, where the rational agent will predictably do badly. But it's a very odd Newcomb Problem; by choosing X the chooser didn't even make themselves better off. Indeed, they guaranteed the lowest payout in the game. I don't have a knock-down argument here, and maybe there is more to be said. But this is where I think things go wrong for a ratificationist who does not allow mixed strategies.

# Indecisive {#sec-indecisive}

Game theory is full of *solution concepts*; ideas for how to solve a game. That is, they are methods for determining the possible outcomes of a game played by rational players. Compared to philosophical decision theory, there are two big things to know about these solution concepts. One is that there are many of them. It isn't like having a single theory to rule all cases. More complex theories tend to give more intuitive results on more cases. But the complexity is a cost, and in any case no theory gets all the intuitions about all the cases. The other thing is that these will often say that there are multiple possible outcomes for a game, and that knowing the players are rational doesn't suffice to know what they will do. It's this latter feature of game theory that I'll argue here decision theory should imitate.

Say that a theory is *indecisive* if for at least one problem it says there are at least two options such that both are rationally permissible, and the options are not equally good. And say, following Ruth cite Chang that two options are equally good if improving either of them by a small amount epsilon would make that one better, i.e., would make it the only permissible choice. So an indecisive theory says that sometimes, multiple choices are permissible, and stay permissible after one or other is sweetened by a small improvement. The vast majority of decision theories on the market are decisive. That's because they first assign a numerical value to each option, and say to choose with the highest value. This allows multiple options iff multiple choices have the same numerical value. But sweetenings increase the value, so they destroy equality and hence the permissibility of each choice.

Perhaps the most intuitive case for indecisiveness involves what I'll call Stag Hunt decisions.\^For much more on Stag Hunts and their philosophical significance, see @Skyrms1994. Here is an example of a Stag Hunt.

PU PD U 6 0 D 5 2

Note three things about this game. First, both Up and Down are ratifiable. Second, Up has a higher expected return than Down. Third, Up has a higher possible regret than Down. If Chooser plays Up and Demon is wrong, Chooser gets 2 less than they might have otherwise. (They get 0 but could have got 2.) If Chooser plays Down and Demon is wrong, Chooser only gets 1 less than they might have otherwise. (They get 5 but could have got 6.)

There is considerable disagreement about what this means for Chooser. EDT says that Chooser should play Up, as does the ratifiable variant of EDT in @Jeffrey1983, and as do some recent causal decision theorists like cite Arntzenius and Schwarz. On the other hand, several other theorists who endorse two-boxing in Newcomb's Problem, like cite four, endorse playing Down on the ground of regret miminisation. I think both Up and Down are permissible. I also think this is the intuitively right verdict, though I place no weight on that intuition. In general, I think in any problem that has the three features described in the last paragraph (two equilibria, one better according to EDT, the other with lower possible regret), either option is permissible. Since lightly sweetening either Up or Down in this problem doesn't change either feature, that is why my theory is indecisive.

My argument for indecisiveness will turn on a case that all eight of the views mentioned in the last paragraph agree on, namely cite below table

PU PD U 4 0 D 0 3

All of them agree that Up is the uniquely rational play in this example, and I think intuition agrees with them. I'll argue, however, that Down is permissible. The argument turns on a variation that embeds cite table in a more complicated problem. This problem involves two demons, each of whom are arbitrarily good at predicting Chooser. The (first version of) the problem involves the following sequence.

1.  Both Demon-1 and Demon-2 predict Chooser, but do not reveal their prediction.
2.  If Demon-1 predicts Chooser plays Up, they Exit with probability 0.5, and Chooser gets 0. If Demon-1 predicts Chooser plays Down, they do not Exit. (That is, they Exit with probability 0.) If they Exit, the problem ends, and Chooser is told this. Otherwise, we go to the next step.
3.  Chooser chooses Up or Down.
4.  Demon-2's prediction is chosen, and that determines whether we are in state PU or state PD.
5.  Chooser's payouts are given by cite above table.

I'll call these Exit Problems, and it's worth having the abstract structure of them in front of us.

Two tables. On left have Exit Payout, e, Pr(Exit \| PU) = x, Pr(Exit \| PD) = y. On right have U/D, PU/PD with payouts a, b ,c, d.

Our problem has this abstract structure with *b* = *c* = *e* = *y* = 0, *x* = 0.5, *a* = 4, *d* = 3.

Now consider a simple variant of the above 5 step problem. The same things happen, but steps 2 and 3 are reversed. That is, Chooser decides on Up or Down after Demons make their predictions, but before they are told whether Demon-1 decided to Exit. Still, their choice will only matter if Demon-1 decided not to Exit, since their choices do not make a difference if Demon-1 Exits. Call this variant the Early Choice version, and the original the Late Choice variant. I don't have any clear intuitions about what to do in most Exit Problems, but I do find the following principle very plausible.

-   **Exit Principle**: In any Exit Problem, the same choices are permissible in the Early Choice and Late Choice variants.

The reason comes from thinking about what Chooser is doing in the Early Choice variant. They are making a decision about what to do if Demon-1 doesn't Exit. The way to make that decision is just to assume that Demon-1 doesn't Exit, and then decide what to do. It just is the same choice as they face in the Late Choice variant, except now they make it in the context of a conditional. So they should decide it the same way.

One could also argue, I think correctly, that anyone who violates Exit Principle will violate a plausible version of the Sure Thing Principle. Such an argument seems sound to me, but the Sure Thing Principle is controversial, and I prefer to put more weight on the argument from how conditional reasoning works in the previous paragraph.

To put the point in game-theoretic terms, there is no difference between extensive form and normal form reasoning when a decider has only one possible choice to make.\^Cite Bonano on extensive and normal forms.

Any plausible theory that says that only Up is rationally playable in problems like cite above table will violate Exit Principle. Think about what they will say about Table Below

PU PM PD U 2 3 0 D 0 3 3

In this problem, PU means that both demons predict Up, PD means that they both predict Down, and PM means that one predicts one, and one the other. This possibility is arbitrarily improbable, and the two strategies have the same expected return given M in any case, so we can ignore it. So really this game comes to Table below

PU PD U 2 0 D 0 3

Now presumably if one prefers Up in above table, it is because one prefers Up in any game like Table below where *x* \> *y* \> 0.

PU PD U *x* 0 D 0 *y*

How could it be otherwise? Given expectationism, it's not like there is anything special about the numbers 4 and 3. But anyone who endorses this policy will play *D* in Third table and so, presumably, in Second table. And that means they will violate Exit Principle.

The only view that is consistent with Exit Principle in cases like Fourth table is that both Up and Down are permissible moves. And since in any such case, improving Up or Down be a tiny amount wouldn't materially change the case, they must both be permissible after small sweetenings. So, given Exit Principle, the only viable theories are indecisive.

# Dual Mandate  {#sec-dualmandate}

Say a decision tree is a series of steps with the following characteristics.

-   At every step, Chooser either receives some information, or makes a choice.
-   Chooser knows before the first step what possible choices will be available at each step, given the prior steps, or what possible pieces of information could be received.
-   No matter what happens, the tree ends after finitely many steps. (Though it may end after more or fewer steps depending on what happens).
-   Chooser knows before the first step what payout they will receive given each possible sequence of choices and information.
-   Before the first step, chooser has a probability for each possible piece of information they could receive, given the prior steps in the tree.

That's incredibly abstract, but it excludes some possibilities. It excludes cases where Chooser learns along the way that they have hitherto unknown abilities. It excludes cases where Chooser gains the capacity to think new relevant thoughts along the way, say by meeting a new person and gaining the capacity to have singular thoughts about them.\^Following @Stalnaker200x, I think it excludes the Sleeping Beauty case, since there Beauty gains the capacity to have singular thoughts about a time, the 'now' when she awakes, that she did not previously have. But it does cover a lot of cases.

Say a strategy for a decision tree is a plan for what to do in every possible choice situation. Following the game theory textbooks, I really do mean *every* here. A strategy should say what to do in cases that are ruled out by Chooser's prior choices. A strategy for playing chess as White might say to start with e4, but also include plans for what to do if you inexplicably start Na3. There are both mathematical and philosophical reasons for having such an expansive conception of strategies, but that's beyond the scope of this paper.

In philosophy, there are two common approaches to decision trees. The so-called resolute approach says that one should simply treat the problem as like the kind of one-shot decisions we have discussed so far, except now one is choosing a strategy. Whatever one's theory of choice is, one should simply apply it to the question of which strategy is best. The so-called sophisticated approach says that one should make the current choices that make best sense given one's views about what one's future self will do.

The orthodoxy in game theory, going back to at least Selten, is that both views are correct. When faced with a decision tree, Chooser should follow the advice of the sophisticated theorists, and (given they are ideally rational) do what would be best on the assumption that future choices will be rational. But in doing so, they should instantiate (part of) a strategy that could be rationally chosen by the resolute chooser. I call this the Dual Mandate approach, and I think it's the right one.

Start with one it is bad to just have a resolute approach.\^This is the approach recently recommended by XXX. The so-called Foundational Decision Theory of @SoaresLevinstein200x agrees with the resolute approach in the special case where the only information Chooser will receive are the results of predictions. Game theorists usually reject this approach because it means sometimes making a decision that one knows will have worse consequences than an available alternative. I'll go over an example of this, though I should note it is rather violent. This is unavoidable; it is only in these violent cases that we can be sure the Chooser is really making things worse, and not acting for a strategic or reputational goal.

Chooser is the Prime Minister of a small country, and they are threatened by a large neighbour. Unfortunately, neighbour is thinking of carpet bombing Chooser's capital, in retaliation for some perceived slight. Chooser has no air defences that would prevent a great destruction, and no allies who will rally to help. Fortunately, Chooser has a mighty weapon, a Doomsday device, that could destroy neighbour. Chooser has obviously threatened to use this, but neighbour suspects it is a bluff. This is for a good reason; the doomsday device would also destroy Chooser's own country. Neighbour is known to employ a Demon who is at least 99% accurate in predicting what military plans Chooser will take. So Chooser can do Nothing (N), or use the Doomsday device (D), should neighbour attack. Chooser would obviously prefer no attack, and would certainly not use the device pre-emptively. So here is the table.

PN PD N -1 0 D -50 0

In the top left, neighbour bombs Chooser's capital, thinking correctly that Chooser will not retaliate. In the top right and lower right, neighbour is sufficiently scared of the doomsday device that they do nothing. But in the bottom left, neighbour attacks, and Chooser retaliates, creating a disaster for everyone, something 50 times worse than even the horrors of the carpet bombing.

Still, if Chooser is picking a strategy before anything starts, the strategy with the highest expected return is to plan to retaliate. This has an expected return of -0.5; since one time in a hundred it returns -50, and otherwise it returns 0. The resolute theorist says that's what Chooser should do, even if they see the bombers coming, and they realise their bluff has failed. This seems absurd to me, and it is the kind of result that drives game theorists to the dual mandate, but resolute theorists are familiar with the point that their theory says that sometimes one should carry out a plan now known to be pointless. So instead of resting on this case, as decisive as it seems to many, I'll run through two more arguments against a purely resolute theory.

Change the example so that Chooser has two advisors who are talking to him as the bombers come in. One of them says that the Demon is 99% reliable. The other says that the Demon is 97% reliable. Whether Chooser launches the doomsday device should, according to the resolute theorist, depend on which advisor Chooser believes. This is just absurd. A debate about the general accuracy of a demon can't possibly be what these grave military decisions are based on.

Change the example again, and make it a bit more realistic. Chooser has the same two advisors, with the same views. Chooser thinks the one who says the Demon is 99% reliable is 60% likely to be right, and the other 40% likely. So Chooser forms the plan to retaliate, because right now that's the strategy with highest expected return. But now, to everyone's surprise, neighbour attacks. The resolute theorist will say that Chooser should stick to their (overpowered) guns. But think about how the choice of plans looks to Chooser now. The actions of neighbour are evidence about the reliability of the demon. And a simple application of Bayes' Rule says that Chooser should now think the advisor who thought the demon was 97% reliable is 2/3 likely to be right. That is, given Chooser's current evidence, retaliating wasn't even the utility maximising strategy to start with. Yet it is what the resolute theorist, or at least the resolute theorist who is not also sophisticated, would have Chooser do. This is, again, absurd, and enough reason to give up on such a theory.

What about the other direction? Is it sensible to have a sophisticated theory that is not resolute? There does seem to be something puzzling about such theories. Jack @Spencer20xx points out that James @Joyce2018 endorses a theory that, in a case Spencer develops, recommends taking a sure \$35 when there was available a series of choices that would have resulted in a sure \$40. Spencer's own example is a variant of Frustrator, and relies on mixed strategies not being available. So we shouldn't be worried by the details of it. But should we be worried by the principle, which is, in essence, that one's choices should form a strategy one could sensibly choose as a strategy.

There are some circumstances where one ought adopt a sub-optimal strategy. Odysseus binds himself to the mast because he does not approve of future-Odysseus's preferences. Professor Procrastinate cite turns down a referee request because he does not trust his future self to be practically rational. Both of them deliberately turn down strategies that would be better than where they end up, because they do not trust their future selves to carry them out. They are alienated in this way from their future selves.

When one does not endorse one's future preferences, or does not trust one's rationality in the future, it makes sense to be alienated from one's future self in this way. In such cases, one's future self is just another part of the world that must be predicted and worked around. And so it might make sense to forego, as Odysseus and Procrastinate forego, strategies that one's future self will not be so kind as to carry out.

My main claim here is when neither of those two conditions obtain, i.e., when one knows that one's future self will be rational and have the same preferences, one's choices should make strategic sense. That is, they should satisfy the fairly weak condition that they are part of some strategy that one could choose if one was simply choosing a strategy for the whole tree. Unless one fears future irrationality, or future change of preference, one should not be alienated from one's future self.

While I've introduced this constraint in the context of dynamic choice, it is really meant to be a constraint on static choice. One's choices at a time should be such that collectively one endorses the series of choices one makes. When that doesn't obtain, it's usually a sign that the choices at a time were wrong, not that one's judgments about the strategies were wrong. That's why I won't put anything about dynamic choice into the positive theory at the end of this paper. The only theory of dynamic choice one needs is the sophisticated theory; the other part of the dual mandate should be derivable as a theorem.

# Substantive

Here are two interesting characters. Piz wants to put mud on his pizza. This won't bring him joy, or any other positive emotions; he has a non-instrumental desire for mud pizza. Za wants to eat a tasty pizza, and believes that putting mud on his pizza will make it tasty. There is a long tradition of saying that the point of philosophical decision theory is not to evaluate beliefs and desires, but merely to say what actions those beliefs and desires should issue in. On such a view, both Piz and Za should put mud on their pizzas. Here is David Lewis expressing such a view.

Insert Lewis quote

We need one caveat on this. Philosophical decision theories typically do not issue verdicts unless the chooser satisfies some coherence constraints. So it's not quite that the theory says nothing about what the beliefs and desires should be. It's that it says nothing *substantive* about what the beliefs and desires should be. Purely structural constraints, like transitivity of preferences, or belief in the law of excluded middle, may be imposed.

At least sometimes, game theorists impose non-structural, substantive conditions on the beliefs of players. Most notably, the "intuitive criterion" of cite Cho-Kreps is meant to be continuous with other equilibrium conditions, and is a substantive constraint. Someone who violates it has coherent beliefs that don't conform to their evidence. The intuitive criterion takes some time to set up, but I'll get to a simplified version of it later in this section.

First, I'll note some general reasons for scepticism about this use of the substantive-structural distinction. One obvious point is that Piz and Za do not look like rational choosers. Another is that this draws distinctions between overly similar characters, such as these two, Cla and Sic. Both of them have taken classes in classical statistics, but only skimmed the textbooks without attending to the details. Cla came away with the belief that any experiment with a *P* value less than 0.05 proved that its hypothesis is true. Sic came away with a standing disposition to belief the hypothesis whenever there was an experiment with a *P* value less than 0.05. Cla is incoherent; there is no possible world where that belief is true. Sic is coherent; any one of their beliefs could be true. It's just they just have a disposition to often form substantially irrational beliefs. Personally, I don't think the difference between Cla and Sic is important enough to be philosophically load bearing. Lastly, it has proven incredibly hard to even define what makes a norm structural. The most important recent attempt is in Alex Worsnip's book Name. (He's interested in coherence norms, but that's just what I'm meaning by structural norms.) Here's his definition:

> *Incoherence Test*. A set of attitudinal mental states is jointly incoherent iff it is (partially) constitutive of (at least some of) the states in the set that any agent who holds this set of states has a disposition, when conditions of full transparency are met, to revise at least one of the states. [@Worsnip2021, 132]

This won't capture nearly enough. If probabilism is correct, then non-probabilists about uncertainty like Glenn @Shafer1977 endorse incoherent views. If expectationalism is correct, then non-expectationalist decision theorists, like Lara @Buchak2014, endorse incoherent views. If classical logic is correct, then intuitionist logicians like Crispin Insert Wright citation are incoherent. Those three all seem to meet Worsnip's conditions of full transparency, and don't seem disposed to revise their beliefs. Maybe this is just a problem with Worsnip's definition, but we should take

Even if the substantive/structural distinction can be made precise, and shown to do philosophical work, it won't track the notion game theorists most care about. We can see this with a version of the beer-quiche game @ChoKreps1987, here translated into decision-theoretic language.

There are five steps in the game.

1.  A coin will be flipped, landing Heads or Tails. It is biased, 60% likely to land Heads. It will be shown to Chooser, but not to Demon.
2.  Chooser will say either Heads or Tails.
3.  Demon, knowing what Chooser has said, and being arbitrarily good at predicting Chooser's strategy\^That is, what Chooser will do if Heads, and what they will do if Tails. will say Heads if it is more probable the coin landed Heads, and Tails if it is more probable the coin landed Tails.\^If both are equally likely, Demon will flip a fair coin and say how it lands.
4.  Chooser is paid \$30 if Demon says Heads, and nothing if Demon says Tails.
5.  Chooser is paid \$10 if what they say matches how the coin landed, and nothing otherwise. This is on top of the payment at step 4, so Chooser could make up to \$40.

If you prefer things in table form, here are the payouts chooser gets, given what happens at steps 1-3.

Coin Chooser Demon Dollars H H H 40 H H T 10 H T H 30 H T T 0 T H H 30 T H T 0 T T H 40 T T T 10

What will Chooser do? There are two coherent things for Chooser to do, though each of them is only coherent given a background belief that isn't entailed by the evidence.

1.  Chooser could say Heads however the coin lands. Demon gets no information from Chooser, so their probability that the coin landed Heads is 0.6, so they will say Heads. Further, Chooser believes that if they were to say Tails, Demon would say Tails, so saying Heads produces the best expected return even after seeing the coin.
2.  Chooser could say Tails however the coin lands. Demon gets no information from Chooser, so their probability that the coin landed Heads is 0.6, so they will say Heads. Further, Chooser believes that if they were to say Heads, Demon would say Tails, so saying Tails produces the best expected return even after seeing the coin.

While both of these are coherent, there is something very odd, very unintuitive about option 2. I guess we've been trained to be sceptical when philosophers report intuitions, but here we have a very large data pool to draw on. Cho and Kreps reported essentially the same intuition. Their paper has been cited tens of thousands of times, and I don't think this intuition has been often questioned. Option 2, while coherent, is unintuitive. It is the kind of option that the theory of rationality behind game theory, and behind decision theory, should rule out.

But what about it is incoherent? One might think it is because it has an expected return of \$34, while option 1 has an expected return of \$36. But we showed in section Indecisive ref that using expected returns to choose between coherent options leads to implausible results. Moreover, if you change the payout in the bottom row to \$50, the intuition doesn't really go away, but the expected return of option 2 is now \$38; higher than option 1.\^I believe if you change that payout to \$65, the various regret based theories I discussed in section Indecisive also start preferring option 2. But applying these theories to complex cases is hard, so I'm not quite sure about this. Alternatively, one might think it is because option 2 requires Chooser to believe a counterfactual that is not entailed by the evidence. But option 1 also requires Chooser to believe a counterfactual that is not entailed by the evidence. That can't be the difference between them, but it is closer to the truth.

What Cho and Kreps argue, and what seems right to me, is that the difference between the options is that in one case the counterfactual belief is reasonable, and in the other it is unreasonable. Assume Chooser plans to adopt option 1. But when it becomes time to play, they change their mind, and say Tails. What would explain that? Not the coin landing Heads - given their plan, they will get the maximum possible payout by sticking to the plan (assuming Demon has done their job). No, the only plausible explanation is the coin landed Tails, and Chooser was (foolishly) chasing the extra \$10. In option 1, Chooser believes the counterfactual that's grounded in Demon picking an explanation that makes sense. What about in option 2? Here, everything is back to front. If Chooser is ever going to depart from their plan, it's when the coin lands Heads. Then Chooser might chase the extra \$10 by saying Heads. But Chooser has to believe that were they to depart from the plan, Demon would draw the explanation that makes no sense whatsoever, that they gave up on their plan even though it was about to lead to the best possible outcome. This makes no sense at all. And in fact it makes less sense the more you increase the payout in line 8.

So that's why I think decision theory requires substantive rationality. The right decision theory should say to take option 1. And the argument against option 2 is not that it is incoherent, but that carrying it out requires believing Demon will do things that make no sense given Demon's evidence. It is substantive, not structural, rationality that rules out option 2. And yet, as the game theorists have insisted, option 2 must be ruled out. So decision theory should be sensitive to substantial rationality.

# Weak Dominance, Once

Why weak dominance?

PU PD U 1 1 D 0 1

1.  D seems like a needless risk
2.  If Pr(Demon Correct) \< 1, then D isn't ratifiable

Why only once?

1.  Order variance (Bonano)

2.  Money burning (cite Stalnaker)

3.  This game

    PU PD PX U 1 1 0 D 0 1 2 X 0 1 1

Iterated weak dominance would first rule out X, then PX, then D, leaving U as the only play. But D makes sense, even if the demon might make a mistake. Indeed, it makes more sense than U by some measures.

# Conclusion

-   State positive theory
-   Include clause for weak dominance
-   Note that it can be dropped if you insist that Pr(Demon correct) \< 1.
-   Note that it meets these 9 conditions
-   Final flourish
