<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.433">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Brian Weatherson">

<title>Gamified Decision Theory</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="gdt_files/libs/clipboard/clipboard.min.js"></script>
<script src="gdt_files/libs/quarto-html/quarto.js"></script>
<script src="gdt_files/libs/quarto-html/popper.min.js"></script>
<script src="gdt_files/libs/quarto-html/tippy.umd.min.js"></script>
<script src="gdt_files/libs/quarto-html/anchor.min.js"></script>
<link href="gdt_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="gdt_files/libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="gdt_files/libs/bootstrap/bootstrap.min.js"></script>
<link href="gdt_files/libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="gdt_files/libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">


</head>

<body class="fullcontent">

<div id="quarto-content" class="page-columns page-rows-contents page-layout-article">

<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Gamified Decision Theory</h1>
</div>



<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>Brian Weatherson </p>
          </div>
  </div>
    
  
    
  </div>
  

</header>

<section id="sec-intro" class="level1" data-number="1">
<h1 data-number="1"><span class="header-section-number">1</span> Introduction</h1>
<p>Textbook versions of game theory embed a distinctive approach to decision theory. That theory isn’t always made explicit, and it isn’t always clear how it handles some cases. But we can extract one interesting and plausible theory, which I’ll call Gamified Decision Theory (GDT), from these textbooks. There are nine characteristics of GDT (as I’ll understand it) that I will focus on, with the bulk of this paper one in each of the following nine sections.</p>
<ol type="1">
<li><strong>Idealised</strong>; GDT is a theory of what ideal deciders do.</li>
<li><strong>Expectationist</strong>; the ideal decider prefers getting more expected value to getting less.</li>
<li><strong>Causal</strong>; GDT is a variety of Causal Decision Theory (CDT).</li>
<li><strong>Allows Mixtures</strong>; the ideal decider can perform a probabilistic mixture of any acts they can perform.</li>
<li><strong>Ratificationist</strong>; the ideal decider endorses the decisions they make.</li>
<li><strong>Indecisive</strong>; GDT sometimes says that multiple options are permissible, and they are not equally good.</li>
<li><strong>Dual Mandate</strong>; in a dynamic choice, the ideal decider will follow a plan that’s permissible, and take choices at every stage that are permissible.</li>
<li><strong>Substantive Probability</strong>; the ideal decider has rational credences.</li>
<li><strong>Weak Dominance, Once</strong>; the ideal decider will not choose weakly dominated options, but they may choose options that would not survive iterated deletion of weakly dominated strategies.</li>
</ol>
<p>This is not going to be a work of exegesis, poring over game theory texts to show that they really do endorse all of 1-9. In fact it wouldn’t take much work to show that they endorse 1-5, so the work wouldn’t be worth doing. And while some books endorse 8 and 9, it would take a lot more investigative work than I’m going to do here to show that anything like a majority of them do. It would be interesting, but not obviously a philosophical question, to see what proportion endorse 6 and 7. But I’m going to set that aside.</p>
<p>What I do want to argue is that you can find some support for all of these in some game theory textbooks, and that combined they produce a plausible decision theory. While the textbooks don’t all agree, for simplicity I’m going to focus on one book: Giacomo Bonanno’s <em>Game Theory</em> <span class="citation" data-cites="Bonanno2018">Bonanno (<a href="#ref-Bonanno2018" role="doc-biblioref">2018</a>)</span>. This book has two important virtues: it is philosophically deep, and it is available for free. It isn’t hard to find a game theory text with one or other of these virtues, but few have both. So it will be our primary guide in what follows, along with some primary sources (most of which are referenced in that book).</p>
<p>Methodologically, this paper differs from most works in decision theory in two ways. It has been a commonplace since <span class="citation" data-cites="Nozick1969">Nozick (<a href="#ref-Nozick1969" role="doc-biblioref">1969</a>)</span> to include demons, who are arbitrarily good at predicting a decision, in problems. Some of the cases here will involve two such demons, each of which is arbitrarily good at predicting a decision, and whose errors are probabilistically independent. Second, I’m going to rely less on intuitions about particular cases, and more on intuitions that certain cases should be treated the same way. This makes sense given the history of the field. There is much less consensus about what to do in Newcomb problems than about which problems are Newcomb problems. Judgments, or intuitions if you prefer, about how to classify problems seem more stable and more reliable, and they will be central to this paper.</p>
</section>
<section id="sec-ideal" class="level1" data-number="2">
<h1 data-number="2"><span class="header-section-number">2</span> Idealised</h1>
<p>Consider the following decision problem. Chooser (our main protagonist) is going to be given a series of multiplication problems, where each of the multiplicands is a four digit number. Chooser doesn’t have access to any kind of calculating device, and has no special arithmetic ability. For each question, if Chooser says the right answer, they get $2; if they pass, they get $1; if they say the wrong answer, they get nothing. <a href="#tbl-multi">Table&nbsp;1</a> has the payout in table form.</p>
<div id="tbl-multi" class="anchored">
<table class="table">
<caption>Table&nbsp;1: The multiplication game</caption>
<tbody>
<tr class="odd">
<td style="text-align: right;"></td>
<td style="text-align: center;"><strong>Best Guess Correct</strong></td>
<td style="text-align: center;"><strong>Best Guess Incorrect</strong></td>
</tr>
<tr class="even">
<td style="text-align: right;"><strong>Guess</strong></td>
<td style="text-align: center;">$2</td>
<td style="text-align: center;">$0</td>
</tr>
<tr class="odd">
<td style="text-align: right;"><strong>Pass</strong></td>
<td style="text-align: center;">$1</td>
<td style="text-align: center;">$1</td>
</tr>
</tbody>
</table>
</div>
<p>Every variety of decision theory defended in philosophy journals in recent years, and every game theory textbook, says that Chooser should simply say the correct answer. After all, Chooser should have probabilistically coherent credences, and every mathematical truth has probability 1, so whatever the correct answer is, saying it is a sure $2.</p>
<p>This is completely terrible advice. Chooser should pass every time, unless the question is something boring like 1000 times 2000. The chance of them getting a question like 5278 times 9713 correct are way less than one in two, so they are better off passing.</p>
<p>This doesn’t mean that every philosophical decision theory, and every game theory textbook, is wrong. Those theories are not in the business of giving advice to people like Chooser. They are in the business of saying what it would be ideal, in some sense of ideal, for Chooser to do. And it would be ideal for Chooser to be a reliable computer, so if Chooser were reliable, they would always give the correct answer.</p>
<p>There is a big question about why we should care about what would have if Chooser were ideal. Chooser is not in fact ideal, so who cares what they would do if they were different. One might think that knowing what the ideal is gives Chooser something to aim for. Even if Chooser is not ideal, they can try to be closer to the ideal. The problem is that trying to be more like the ideal will make things worse. The ideal agent will never pass, and even if Chooser doesn’t know the answers to the particular questions, they can know this fact. So if they try to be more like the ideal, they will never pass, and things will go badly.<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a></p>
<p>In philosophy we have two very different uses of the term ‘idealisation’. One is the kind of idealisation we see in, for example, Ideal Observer theories in ethics. The other is the kind of idealisation we see in, for example, Ideal Gas models in chemistry. It’s important to not confuse the two. Think about the volumeless, infinitely dense, molecules in an Ideal Gas model. To say that this is an idealised model is not to say that having volume, taking up space, is an imperfection. The point is not to tell molecules what the perfect size is. (“The only good molecule is a volumeless molecule.”) Nor is it to tell them that they should approximate the ideal. (“Smaller the better, fellas.”) It’s to say that for some predictive and explanatory purposes, molecules behave no differently to how they would behave if they took up no space.<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a></p>
<p>The best way to understand decision theorists, and game theorists, is that they are using idealisations in this latter sense. The ideal choosers of decision theory are not like the Ideal Observers in ethics, but like the Ideal Gases. The point of the theory is to say how things go in a simplified version of the case, and then argue that this is useful for predictive and explanatory purposes because, at least some of the time, the simplifications don’t make a difference.</p>
<p>One nice example of this working is George Akerlof’s discussion of the used car market <span class="citation" data-cites="Akerlof1970">Akerlof (<a href="#ref-Akerlof1970" role="doc-biblioref">1970</a>)</span>. In the twentieth century, it was common for lightly used cars to sell at a massive discount to new cars. There was no good explanation for this, and it was often put down to a brute preference for new cars. What Akerlof showed was that a model where (a) new cars varied substantially in quality, and (b) in the used car market, buyers had less information about the car than sellers, you could get a discount similar to what you saw in real life even if the buyers had no special preference for new cars. Rather, buyers had a preference for good cars, and took the fact that this car was for sale to be evidence that it was badly made. It was important for Akerlof’s explanatory purposes that he could show that people were being rational, and this required that he have a decision theory that they followed. In fact what he used was something like GDT. We now have excellent evidence that something like his model was correct. As the variation in quality of new cars has declined, and the information available to buyers of used cars has risen, the used car discount has just about vanished. (In fact it went negative during the pandemic, for reasons I don’t at all understand.)</p>
<p>I’ll end this section with a response to one objection, one caveat, and one surprising bonus to doing idealised decision theory this way.</p>
<p>The objection is that decision theory isn’t actually that helpful for prediction and explanation. If all that it says are things like when rain is more probable, more people take umbrellas, that doesn’t need a whole academic discipline. The response to that is that in non-cooperative games, the predictions, and explanations, can be somewhat surprising. One nice case of this is the discussion of Gulf of Mexico oil leases in <span class="citation" data-cites="Wilson1967">Wilson (<a href="#ref-Wilson1967" role="doc-biblioref">1967</a>)</span>.<a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a> But here’s a simpler surprising prediction that you need something like GDT to get.<a href="#fn4" class="footnote-ref" id="fnref4" role="doc-noteref"><sup>4</sup></a></p>
<p>Imagine Row and Column are playing rock-paper-scissors. A bystander, C, says that he really likes seeing rock beat scissors, so he will pay whoever wins by playing rock $1. Assuming that Row and Column have no ability to collude, the effect of this will be to shift the payouts in the game they are playing from left table to right table, where <em>x</em> is the value of the dollar compared to the value of winning the game. This changes the game they are playing from <a href="#tbl-rps-basic">Table&nbsp;2 (a)</a> to <a href="#tbl-rps-modified">Table&nbsp;2 (b)</a>.</p>
<div id="tbl-rps" class="tbl-parent quarto-layout-panel anchored">
<div class="panel-caption table-caption">
<p>Table&nbsp;2: Two versions of Rock-Paper-Scissors</p>
</div>
<div class="quarto-layout-row quarto-layout-valign-top">
<div id="tbl-rps-basic" class="quarto-layout-cell quarto-layout-cell-subref anchored" data-ref-parent="tbl-rps" style="flex-basis: 50.0%;justify-content: center;">
<table class="table">
<caption>(a) Original game</caption>
<tbody>
<tr class="odd">
<td style="text-align: right;"></td>
<td style="text-align: center;"><strong>Rock</strong></td>
<td style="text-align: center;"><strong>Paper</strong></td>
<td style="text-align: center;"><strong>Scissors</strong></td>
</tr>
<tr class="even">
<td style="text-align: right;"><strong>Rock</strong></td>
<td style="text-align: center;">0,0</td>
<td style="text-align: center;">-1,1</td>
<td style="text-align: center;">1,-1</td>
</tr>
<tr class="odd">
<td style="text-align: right;"><strong>Paper</strong></td>
<td style="text-align: center;">1,-1</td>
<td style="text-align: center;">0,0</td>
<td style="text-align: center;">-1,1</td>
</tr>
<tr class="even">
<td style="text-align: right;"><strong>Scissors</strong></td>
<td style="text-align: center;">-1,1</td>
<td style="text-align: center;">1,-1</td>
<td style="text-align: center;">0,0</td>
</tr>
</tbody>
</table>
</div>
<div id="tbl-rps-modified" class="quarto-layout-cell quarto-layout-cell-subref anchored" data-ref-parent="tbl-rps" style="flex-basis: 50.0%;justify-content: center;">
<table class="table">
<caption>(b) Modified game</caption>
<tbody>
<tr class="odd">
<td style="text-align: right;"></td>
<td style="text-align: center;"><strong>Rock</strong></td>
<td style="text-align: center;"><strong>Paper</strong></td>
<td style="text-align: center;"><strong>Scissors</strong></td>
</tr>
<tr class="even">
<td style="text-align: right;"><strong>Rock</strong></td>
<td style="text-align: center;">0,0</td>
<td style="text-align: center;">-1,1</td>
<td style="text-align: center;">1+<em>x</em>,-1</td>
</tr>
<tr class="odd">
<td style="text-align: right;"><strong>Paper</strong></td>
<td style="text-align: center;">1,-1</td>
<td style="text-align: center;">0,0</td>
<td style="text-align: center;">-1,1</td>
</tr>
<tr class="even">
<td style="text-align: right;"><strong>Scissors</strong></td>
<td style="text-align: center;">-1,1+<em>x</em></td>
<td style="text-align: center;">1,-1</td>
<td style="text-align: center;">0,0</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
<p>The surprising prediction is that this will <em>decrease</em> the frequency with which rock is played, and the larger <em>x</em> is, the larger the decrease will be. Simple rules like “When behavior is rewarded, it happens more often” don’t always work in strategic settings, and it takes some care to tell when they do work.</p>
<p>The caveat is that there is a reason that this particular idealisation is chosen, at least as the first attempt. There are a lot of stylised facts about people that we could use in a model of behavior. In game theory we concentrate on the ways in which people are, at least approximately much of the time, somewhat rational. People who prefer vanilla to chocolate really do buy vanilla more than chocolate. We could also choose stylised facts that are not particularly rational. But there is a worry that these will not remain facts, even approximately, when the stakes go up. And for some purposes, what people do in high stakes situations might be really important. If you think that people are more careful in high stakes situations, and that this care translates into more rational action, and it’s particularly important to make the right predictions in high stakes cases, it makes sense to focus on idealisations that are also true of perfectly rational people.</p>
<p>There is a tricky complication here that isn’t always attended to in theory, though in practice it’s less of a problem. Especially in decision theory, we idealise away from computational shortcomings, but not away from informational shortcomings. We take the chooser’s information as fixed, and ask how they’ll decide. In high stakes cases, people don’t just get better calculators, they get more information. If this is why we idealise, why don’t we idealise away from ignorance? The reason is that in a lot of cases, either it is impossible to get the evidence the chooser needs, because it is about the future, or it is challenging because there is someone else working just as hard to prevent the chooser getting the information. It’s not a coincidence that game theory, and decision theory, are most explanatory when the chooser’s ignorance is about the future, or about something that someone is trying to hide from them.</p>
<p>There is one surprising bonus from starting with these rational idealisations. Sometimes one gets a powerful kind of explanation from very carefully working out the ideal theory, and then relaxing one of the components. At a very high level of abstraction, that’s what happened with the development of cursed equilibrium <span class="citation" data-cites="EysterRabin2005">(<a href="#ref-EysterRabin2005" role="doc-biblioref">Eyster and Rabin 2005</a>)</span>. The explanations one gets these ways are, to my mind, very surprising. The models have people acting as if they have solved very complex equations, but have ignored simple facts, notably that other people may know more than they do. But if the model fits the data, it is worth taking seriously. And while it was logically possible to develop a model like cursed equilibrium without first developing an ideal model and then relaxing it, it seems not surprising that in fact that’s how the model was developed.</p>
</section>
<section id="sec-expect" class="level1" data-number="3">
<h1 data-number="3"><span class="header-section-number">3</span> Expectationist</h1>
<p>There is a strange split in contemporary decision theory. On the one hand, there are questions about the way to model attitudes to risk, largely organised around the challenge to orthodoxy from <span class="citation" data-cites="Quiggin1982">Quiggin (<a href="#ref-Quiggin1982" role="doc-biblioref">1982</a>)</span> and <span class="citation" data-cites="BuchakRisk">Buchak (<a href="#ref-BuchakRisk" role="doc-biblioref">2013</a>)</span>. On the other hand, there are questions about what to do in cases where the states are causally but not probabilistically independent of one’s actions, with the central case being Newcomb’s Problem <span class="citation" data-cites="Nozick1969">(<a href="#ref-Nozick1969" role="doc-biblioref">Nozick 1969</a>)</span>. The strange split is that these two literatures have almost nothing in common.<a href="#fn5" class="footnote-ref" id="fnref5" role="doc-noteref"><sup>5</sup></a></p>
<p>This split might seem to make sense when one reflects that there is no logical difficulty in endorsing any prominent answer to one set of questions with any prominent answer to the other set. But things get more difficult quickly. For one thing, one answer to questions about risk, what I’ll call the expectationist answer, is universally assumed by people working on issues around Newcomb’s Problem. For another, the argument forms used in the two debates are similar, and that should affect how the two arguments go.&nbsp;</p>
<p>Say that a normal decision problem is one where the states are probabilistically independent of the choices. A simple example is betting on a coin flip. In talking about normal decision problems I’ll normally label the states H, for Heads, or T for Tails. Unless otherwise stated coins are fair, so H and T are equiprobable.&nbsp;</p>
<p>Say that an abnormal decision problem is simply one that isn’t normal. A simple example is where the states are predictions of an arbitrarily accurate predictor. I’ll normally label such states as PX, where X is a choice the agent may make. In these cases the Predictor is arbitrarily accurate unless otherwise stated, but we will spend some time with more error prone predictors.&nbsp;</p>
<p>The view I call expectationism has two parts. First, it says that in normal decision problems, the rational agent maximises the expected value of something like the value of their action. Second, it says that something like this expected value plays an important role in the theory of abnormal decision problems. These definitions are vague, so there are possible borderline cases. But in practice this doesn’t arise, at least in the philosophy literature. Everyone working on abnormal problems is an expectationist. Indeed, most work assumes without even saying it that the first clause of expectationism is correct. Everyone working on normal problems makes it clear which side they fall on, so there is no vagueness there. And every game theory text is expectationist.&nbsp;</p>
<p>I’m going to mostly follow suit. So why am I belabouring this point? One small reason and one large reason. The small reason is that one of the arguments I’ll give concerning abnormal cases generalises to an argument for expectationism about normal cases. The other reason is dialectical.</p>
<p>In the debate about normal cases, the method of gathering intuitions about cases, and seeing which theory fits the intuitions best, does not favor expectationism. On the contrary, the Quiggin-Buchak theory does a much better job on that score. There is something incoherent about assuming expectationism is true for normal cases, and then thinking that the right way to theorise about abnormal cases is asking which theory fits intuitions best. If that’s the goal of decision theory, we shouldn’t be expectationist to start with.</p>
<p>The argument for expectationism is not that it fits the intuitions about cases best, but that it’s the only theory that is compatible with various highly plausible principles, such as the Sure Thing Principle. Again, the theorist working on abnormal cases who is an expectationist has a dialectical burden here. They don’t have to believe in the Sure Thing Principle, and indeed many expectationists don’t <span class="citation" data-cites="Gallownd">(<a href="#ref-Gallownd" role="doc-biblioref">Gallow, n.d.</a>)</span>. But they do have to believe in some principle that can be used to make an argument for expectationism. Especially when it comes to evidential decision theorists, I’m not sure what that principle might be. Still, I don’t have an argument that there is no such principle, so I’ll just note this is a challenge, not any kind of refutation.</p>
</section>
<section id="sec-causal" class="level1" data-number="4">
<h1 data-number="4"><span class="header-section-number">4</span> Causal</h1>
<p>It shouldn’t be controversial to claim that game theory textbooks are committed a broadly causal version of decision theory.<a href="#fn6" class="footnote-ref" id="fnref6" role="doc-noteref"><sup>6</sup></a> For one thing, they always recommend defecting in Prisoners’ Dilemma, even when playing with a twin. As David Lewis showed, this is equivalent to recommending two-boxing in Newcomb’s Problem <span class="citation" data-cites="Lewis1979e">(<a href="#ref-Lewis1979e" role="doc-biblioref">Lewis 1979</a>)</span>. They endorse the causal decision theorist’s signature argument form: the deletion of strongly dominated strategies. Indeed, the typical book introduces this before it introduces anything about probability. When they do get around to probabilities, they tend to define the expected value of a choice in a way only a causal decision theorist could endorse. In particular, they define expected values using unconditional, rather than conditional, probabilities.<a href="#fn7" class="footnote-ref" id="fnref7" role="doc-noteref"><sup>7</sup></a> And the probabilities are simply probabilities of states, not probabilities of any kind of counterfactual. Indeed, you can go entire textbooks without even getting a symbol for a counterfactual conditional.</p>
<p>What’s more controversial is that they are right to adopt a kind of causal decision theory (CDT).<a href="#fn8" class="footnote-ref" id="fnref8" role="doc-noteref"><sup>8</sup></a> In the recent literature, I think there are three main kinds of objections to CDT. First, it leaves one with too little money in Newcomb’s Problem. Second, it gives the wrong result in problems like Frustrator <span class="citation" data-cites="SpencerWells2019">(<a href="#ref-SpencerWells2019" role="doc-biblioref">Spencer and Wells 2019</a>)</span>. Third, it gives the wrong result in asymmetric Death in Damascus cases, as in <span class="citation" data-cites="Egan2007-EGASCT">Egan (<a href="#ref-Egan2007-EGASCT" role="doc-biblioref">2007</a>)</span>. Fourth, it gives strange results in Ahmed’s <em>Betting on the Past</em> and <em>Betting on the Laws</em> cases. I’m going to set those problems aside because (a) they require that an agent not always be aware of what actions are possible, and that’s inconsistent with the idealisations introduced in <a href="#sec-ideal">Section&nbsp;2</a>, and (b) they raise questions about just what it means for two things to be causally independent that go beyond the scope of this paper.</p>
<p>The intuitions behind the asymmetric Death in Damascus cases are inconsistent with the Exit Principle that I’ll discuss in <a href="#sec-indecisive">Section&nbsp;7</a>. The Frustrator cases are no problem for a version of CDT that says that idealised agents can always play mixed strategies. Like the game theorists, I will also assume mixed strategies are available, and I’ll come back in <a href="#sec-mixed">Section&nbsp;5</a> to why that assumption should be allowed.</p>
<p>That leaves the point that CDT leaves one poorly off in Newcomb’s Problem, while other theories, like evidential decision theory (EDT) leave one well off. This isn’t a particular mark against CDT, since other theories, like EDT, leave one poorly off in some situations. Here is one such case.</p>
<p>There are two demons, who will predict what Chooser will do. Both of them are arbitrarily good, though not quite perfect, and their errors are independent. Chooser will play either the left or right game in <a href="#tbl-edt-war">Table&nbsp;3</a>.</p>
<div id="tbl-edt-war" class="tbl-parent quarto-layout-panel anchored">
<div class="panel-caption table-caption">
<p>Table&nbsp;3: A Newcomb problem with two demons</p>
</div>
<div class="quarto-layout-row quarto-layout-valign-top">
<div id="tbl-war-left" class="quarto-layout-cell quarto-layout-cell-subref anchored" data-ref-parent="tbl-edt-war" style="flex-basis: 50.0%;justify-content: center;">
<table class="table">
<caption>(a) Demon-1 predicts Down</caption>
<tbody>
<tr class="odd">
<td style="text-align: right;"></td>
<td style="text-align: center;"><strong>PUp</strong></td>
<td style="text-align: center;"><strong>PDown</strong></td>
</tr>
<tr class="even">
<td style="text-align: right;"><strong>Up</strong></td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">3</td>
</tr>
<tr class="odd">
<td style="text-align: right;"><strong>Down</strong></td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">2</td>
</tr>
</tbody>
</table>
</div>
<div id="tbl-war-right" class="quarto-layout-cell quarto-layout-cell-subref anchored" data-ref-parent="tbl-edt-war" style="flex-basis: 50.0%;justify-content: center;">
<table class="table">
<caption>(b) Demon-1 predicts Up</caption>
<tbody>
<tr class="odd">
<td style="text-align: right;"></td>
<td style="text-align: center;"><strong>PUp</strong></td>
<td style="text-align: center;"><strong>PDown</strong></td>
</tr>
<tr class="even">
<td style="text-align: right;"><strong>Up</strong></td>
<td style="text-align: center;">1001</td>
<td style="text-align: center;">1003</td>
</tr>
<tr class="odd">
<td style="text-align: right;"><strong>Down</strong></td>
<td style="text-align: center;">1000</td>
<td style="text-align: center;">1002</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
<p>If Demon-1 predicts that Chooser will play Down, Demon-1 will offer Chooser <a href="#tbl-war-left">Table&nbsp;3 (a)</a>; if Demon-1 predicts that Chooser will play Up, Demon-1 will offer Chooser <a href="#tbl-war-right">Table&nbsp;3 (b)</a>. Then Demon-2’s prediction will be used for determining whether the payout is from column PU or PD. In almost all cases, if Chooser uses CDT, they will get 1001, while if they use EDT, they will get 2. So in this case, CDT will get more than EDT.</p>
<p>This case is not meant as an objection to EDT. It is perfectly fair for the evidential decision theorist to complain that they have simply been the victim of a Demon who intends to punish users of EDT, and reward users of CDT. That seems a perfectly fair complaint. But if the evidential decision theorist makes it, they cannot object when causal decision theorists, such as <span class="citation" data-cites="Lewis1981e">Lewis (<a href="#ref-Lewis1981e" role="doc-biblioref">1981</a>)</span>, use the same language to describe Newcomb’s Problem. The ‘objection’ that CDT leaves one poorly off in one particular case is equally an objection to everyone, and so it is an objection to no one.</p>
<p>One might worry at this stage that I haven’t shown that everyone is vulnerable to this kind of ‘objection’, just that CDT and EDT are equally vulnerable to it. In particular, so-called ‘resolute’ decision theories will choose one-box in Newcomb’s Problem, and Up in <a href="#tbl-edt-war">Table&nbsp;3</a>, and so be enriched both times. The so-called ‘foundational decision theory’ that <span class="citation" data-cites="LevinsteinSoares2020">Levinstein and Soares (<a href="#ref-LevinsteinSoares2020" role="doc-biblioref">2020</a>)</span> endorse also makes that pair of choices. But those theories are vulnerable to much more serious objections, that I’ll come to in <a href="#sec-dualmandate">Section&nbsp;8</a>.</p>
<p>So I conclude that there is no good objection to adopting a broadly causal decision theory, much as the game theorists do. But which version of CDT do they adopt, and are they right to do so? That will take us much more time.</p>
</section>
<section id="sec-mixed" class="level1" data-number="5">
<h1 data-number="5"><span class="header-section-number">5</span> Mixtures</h1>
<p>Perhaps the biggest difference between the decision theory found in game theory textbooks, and the one found in philosophy journals, concerns the status of mixed strategies. In the textbooks, mixed strategies are brought in almost without comment, or perhaps with a remark about their role in a celebrated theorem by Nash1944. In philosophy journals, the possibility of mixed strategies is often dismissed almost as quickly.</p>
<p>The philosophers’ dismissal is usually accompanied by one or both of these reasons.^These reasons are both offered, briefly, by Nozick - cite here - in the foundational document of modern decision theory. Other philosophers have followed him in using them, but</p>
</section>
<section id="sec-ratify" class="level1" data-number="6">
<h1 data-number="6"><span class="header-section-number">6</span> Ratificationist</h1>
<p>Solution concepts in game theory tend to be equilibria. And by an equilibria, everyone is happy with their moves knowing what all the moves of all the players are. (Or, at least, they are as happy as they can be.) Put in decision theoretic terms, that means that all solutions are ratifiable; Chooser is happy with their choice once it is made.</p>
<p>Ratificationism used to be a more popular view among decision theorists. Richard <span class="citation" data-cites="Jeffrey1983">Jeffrey (<a href="#ref-Jeffrey1983" role="doc-biblioref">1983</a>)</span> added a ratifiability constraint to a broadly evidential decision theory. And ratifiability was endorsed by causal theorists such as <span class="citation" data-cites="Weirich1985">Weirich (<a href="#ref-Weirich1985" role="doc-biblioref">1985</a>)</span> and <span class="citation" data-cites="Harper1986">W. Harper (<a href="#ref-Harper1986" role="doc-biblioref">1986</a>)</span>. It fell out of popularity, though it has been recently endorsed by <span class="citation" data-cites="Fuscond">Fusco (<a href="#ref-Fuscond" role="doc-biblioref">n.d.</a>)</span>. The loss of popularity was for two reasons.</p>
<p>One was the existence of cases where there is (allegedly) no ratifiable option. <a href="#tbl-no-pure">Table&nbsp;4</a> is one such case.</p>
<div id="tbl-no-pure" class="anchored">
<table class="table">
<caption>Table&nbsp;4: A case with no pure ratifiable options.</caption>
<tbody>
<tr class="odd">
<td style="text-align: right;"></td>
<td style="text-align: center;"><strong>PUp</strong></td>
<td style="text-align: center;"><strong>PDown</strong></td>
</tr>
<tr class="even">
<td style="text-align: right;"><strong>Up</strong></td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">5</td>
</tr>
<tr class="odd">
<td style="text-align: right;"><strong>Down</strong></td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">3</td>
</tr>
</tbody>
</table>
</div>
<p>If Chooser plays Up, they would prefer to play Down. If Chooser plays Down, they would prefer to play Up. Things get worse if we add an option that is ratifiable, but unfortunate, as in <a href="#tbl-bad-third">Table&nbsp;5</a>.</p>
<div id="tbl-bad-third" class="anchored">
<table class="table">
<caption>Table&nbsp;5: A case with only a bad pure ratifiable option.</caption>
<tbody>
<tr class="odd">
<td style="text-align: right;"></td>
<td style="text-align: center;"><strong>PUp</strong></td>
<td style="text-align: center;"><strong>PDown</strong></td>
<td style="text-align: center;"><strong>PX</strong></td>
</tr>
<tr class="even">
<td style="text-align: right;"><strong>Up</strong></td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">0</td>
</tr>
<tr class="odd">
<td style="text-align: right;"><strong>Down</strong></td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">0</td>
</tr>
<tr class="even">
<td style="text-align: right;"><strong>X</strong></td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
</tr>
</tbody>
</table>
</div>
<p>The only ratifiable option is X, but surely it is worse than Up or Down. One might avoid this example by saying that there is a weak dominance constraint on rational choices, as well as a ratifiability constraint. That won’t solve the problem, but it will turn it into a problem like <a href="#tbl-no-pure">Table&nbsp;4</a>, where there is no good solution. But that won’t help us much, as was pointed out by <span class="citation" data-cites="Skyrms1984">Skyrms (<a href="#ref-Skyrms1984" role="doc-biblioref">1984</a>)</span>, since in <a href="#tbl-verybad-third">Table&nbsp;6</a> there is no weakly dominant option, but X is surely still a bad play.</p>
<div id="tbl-verybad-third" class="anchored">
<table class="table">
<caption>Table&nbsp;6: A case with only a bad pure ratifiable option that is not weakly dominated.</caption>
<tbody>
<tr class="odd">
<td style="text-align: right;"></td>
<td style="text-align: center;"><strong>PUp</strong></td>
<td style="text-align: center;"><strong>PDown</strong></td>
<td style="text-align: center;"><strong>PX</strong></td>
</tr>
<tr class="even">
<td style="text-align: right;"><strong>Up</strong></td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">0</td>
</tr>
<tr class="odd">
<td style="text-align: right;"><strong>Down</strong></td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">0</td>
</tr>
<tr class="even">
<td style="text-align: right;"><strong>X</strong></td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">ε</td>
</tr>
</tbody>
</table>
</div>
<p>A better option is to insist, as <span class="citation" data-cites="Harper1986">W. Harper (<a href="#ref-Harper1986" role="doc-biblioref">1986</a>)</span> did, and as I argued in previous section, that if Chooser is rational, they can play a mixed strategy. In all three of these games, the mixed strategy of (0.5 U, 0.5 D) will be ratifiable, as long as Chooser forms the belief (upon choosing to play this), that Demon will play the mixed strategy (1/3 U, 2/3 D). And that’s a sensible thing for Demon to play, since it is the only strategy that is ratifiable for Demon if Demon thinks Chooser can tell what they are going to do. And given Chooser’s knowledge of Demon’s goals, Chooser can tell what Demon is going to do once they choose.</p>
<p>So if mixed strategies are allowed, none of the problems for ratifiability persist. And since mixed strategies should be allowed, since Chooser is an ideal practical actor, and not being able to play mixed strategies is an imperfection.</p>
<p>Moreover, ratifiability is an intuitive constraint. There is something very odd about saying that such-and-such is a rational thing to do, but whoever does it will regret it the moment they act. So I’ll follow the game theory textbooks in saying ratifiability should be part of the correct decision theory.</p>
<p>This does not mean that we need to have an explicit ratifiability clause in our theory. It could be, and arguably should be, that ratifiability is a consequence of the theory, not an explicit stipulation.</p>
<p>Could we defend ratifiability without appeal to mixed strategies? It’s not a completely impossible task, but nor is it an appealing one.</p>
<p><a href="#tbl-no-pure">Table&nbsp;4</a> poses no serious problem. Without mixed strategies, the case is simply a dilemma. And we know that there are dilemmas in decision theory. Here’s one familiar example. A sinner faces Judgment Day. Because of his sins, it is clear things will end badly for him. But he has done some good in his life, and that counts for something. The judge thinks he should get some days in the Good Place before being off to the Bad Place. But the judge can’t decide how many. So the judge says to the sinner to pick a natural number <em>n</em>, and the sinner will spend <em>n</em> days in the Good Place, and then goodbye. This clearly is a dilemma; for any large <em>n</em>, saying <em>n</em>! would be considerably better.<a href="#fn9" class="footnote-ref" id="fnref9" role="doc-noteref"><sup>9</sup></a> <span class="citation" data-cites="Ahmed2012">(<a href="#ref-Ahmed2012" role="doc-biblioref"><strong>Ahmed2012?</strong></a>)</span> says that it is an objection to a theory that it allows dilemmas in cases with finitely many options; dilemmas should only arise in infinite cases. But he doesn’t really argue for this, and I can’t see what an argument would be. Once you’ve allowed dilemmas of any kind, the door is open to all of them.</p>
<p>Nor does <a href="#tbl-bad-third">Table&nbsp;5</a> pose a problem, since as I said, the ratifiability theorist could add a weak dominance constraint and turn Second table into another dilemma.</p>
<p>The problem is <a href="#tbl-verybad-third">Table&nbsp;6</a>. There the ratifiability theorist who does not allow mixed strategies has to say that the case is an odd kind of Newcomb Problem, where the rational agent will predictably do badly. But it’s a very odd Newcomb Problem; by choosing X the chooser didn’t even make themselves better off. Indeed, they guaranteed the lowest payout in the game. I don’t have a knock-down argument here, and maybe there is more to be said. But this is where I think things go wrong for a ratificationist who does not allow mixed strategies.</p>
</section>
<section id="sec-indecisive" class="level1" data-number="7">
<h1 data-number="7"><span class="header-section-number">7</span> Indecisive</h1>
<p>Game theory is full of <em>solution concepts</em>; ideas for how to solve a game. That is, they are methods for determining the possible outcomes of a game played by rational players. Compared to philosophical decision theory, there are two big things to know about these solution concepts. One is that there are many of them. It isn’t like having a single theory to rule all cases. More complex theories tend to give more intuitive results on more cases. But the complexity is a cost, and in any case no theory gets all the intuitions about all the cases. The other thing is that these will often say that there are multiple possible outcomes for a game, and that knowing the players are rational doesn’t suffice to know what they will do. It’s this latter feature of game theory that I’ll argue here decision theory should imitate.</p>
<p>Say that a theory is <em>indecisive</em> if for at least one problem it says there are at least two options such that both are rationally permissible, and the options are not equally good. And say, following Ruth <span class="citation" data-cites="Chang2002">Chang (<a href="#ref-Chang2002" role="doc-biblioref">2002</a>)</span>, that two options are equally good if improving either of them by a small amount epsilon would make that one better, i.e., would make it the only permissible choice. So an indecisive theory says that sometimes, multiple choices are permissible, and stay permissible after one or other is sweetened by a small improvement. The vast majority of decision theories on the market are decisive. That’s because they first assign a numerical value to each option, and say to choose with the highest value. This allows multiple options iff multiple choices have the same numerical value. But sweetenings increase the value, so they destroy equality and hence the permissibility of each choice.</p>
<p>Perhaps the most intuitive case for indecisiveness involves what I’ll call Stag Hunt decisions.<a href="#fn10" class="footnote-ref" id="fnref10" role="doc-noteref"><sup>10</sup></a> Here is an example of a Stag Hunt decision.</p>
<div id="tbl-stag-hunt" class="anchored">
<table class="table">
<caption>Table&nbsp;7: An example of a Stag Hunt.</caption>
<tbody>
<tr class="odd">
<td style="text-align: right;"></td>
<td style="text-align: center;"><strong>PUp</strong></td>
<td style="text-align: center;"><strong>PDown</strong></td>
</tr>
<tr class="even">
<td style="text-align: right;"><strong>Up</strong></td>
<td style="text-align: center;">6</td>
<td style="text-align: center;">0</td>
</tr>
<tr class="odd">
<td style="text-align: right;"><strong>Down</strong></td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">2</td>
</tr>
</tbody>
</table>
</div>
<p>Note three things about this game. First, both Up and Down are ratifiable. Second, Up has a higher expected return than Down. Third, Up has a higher possible regret than Down. If Chooser plays Up and Demon is wrong, Chooser gets 2 less than they might have otherwise. (They get 0 but could have got 2.) If Chooser plays Down and Demon is wrong, Chooser only gets 1 less than they might have otherwise. (They get 5 but could have got 6.)</p>
<p>There is considerable disagreement about what this means for Chooser. EDT says that Chooser should play Up, as does the ratifiable variant of EDT in <span class="citation" data-cites="Jeffrey1983">Jeffrey (<a href="#ref-Jeffrey1983" role="doc-biblioref">1983</a>)</span>, and some causal decision theorists such as <span class="citation" data-cites="Arntzenius2008">Arntzenius (<a href="#ref-Arntzenius2008" role="doc-biblioref">2008</a>)</span>. On the other hand, several other theorists who endorse two-boxing in Newcomb’s Problem, like <span class="citation" data-cites="Wedgwood2013a">Wedgwood (<a href="#ref-Wedgwood2013a" role="doc-biblioref">2013</a>)</span>, <span class="citation" data-cites="Gallow2020">Gallow (<a href="#ref-Gallow2020" role="doc-biblioref">2020</a>)</span>, <span class="citation" data-cites="Podgorski2020">Podgorski (<a href="#ref-Podgorski2020" role="doc-biblioref">2020</a>)</span>, and <span class="citation" data-cites="Barnettnd">Barnett (<a href="#ref-Barnettnd" role="doc-biblioref">2022</a>)</span>, endorse playing Down on the ground of regret miminisation. I think both Up and Down are permissible.<a href="#fn11" class="footnote-ref" id="fnref11" role="doc-noteref"><sup>11</sup></a> I also think this is the intuitively right verdict, though I place no weight on that intuition. In general, I think in any problem that has the three features described in the last paragraph (two equilibria, one better according to EDT, the other with lower possible regret), either option is permissible. Since lightly sweetening either Up or Down in this problem doesn’t change either feature, that is why my theory is indecisive.</p>
<p>My argument for indecisiveness will turn on a case that all seven of the views mentioned in the last paragraph agree on, namely <a href="#tbl-coord">Table&nbsp;8</a>.</p>
<div id="tbl-coord" class="anchored">
<table class="table">
<caption>Table&nbsp;8: An example of a coordination game.</caption>
<tbody>
<tr class="odd">
<td style="text-align: right;"></td>
<td style="text-align: center;"><strong>PUp</strong></td>
<td style="text-align: center;"><strong>PDown</strong></td>
</tr>
<tr class="even">
<td style="text-align: right;"><strong>Up</strong></td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">0</td>
</tr>
<tr class="odd">
<td style="text-align: right;"><strong>Down</strong></td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">3</td>
</tr>
</tbody>
</table>
</div>
<p>All of them agree that Up is the uniquely rational play in this example, and I think intuition agrees with them. I’ll argue, however, that Down is permissible. The argument turns on a variation that embeds cite table in a more complicated problem. This problem involves two demons, each of whom are arbitrarily good at predicting Chooser. The (first version of) the problem involves the following sequence.</p>
<ol type="1">
<li>Both Demon-1 and Demon-2 predict Chooser, but do not reveal their prediction.</li>
<li>If Demon-1 predicts Chooser plays Up, they Exit with probability 0.5, and Chooser gets 0. If Demon-1 predicts Chooser plays Down, they do not Exit. (That is, they Exit with probability 0.) If they Exit, the problem ends, and Chooser is told this. Otherwise, we go to the next step.</li>
<li>Chooser chooses Up or Down.</li>
<li>Demon-2’s prediction is chosen, and that determines whether we are in state PU or state PD.</li>
<li>Chooser’s payouts are given by cite above table.</li>
</ol>
<p>I’ll call these Exit Problems, and it’s worth having the abstract structure of them in front of us.</p>
<div id="tbl-general-exit" class="tbl-parent quarto-layout-panel anchored">
<div class="panel-caption table-caption">
<p>Table&nbsp;9: The abstract form of an exit problem.</p>
</div>
<div class="quarto-layout-row quarto-layout-valign-top">
<div id="tbl-exit-param" class="quarto-layout-cell quarto-layout-cell-subref anchored" data-ref-parent="tbl-general-exit" style="flex-basis: 50.0%;justify-content: center;">
<table class="table">
<caption>(a) Exit Parameters</caption>
<tbody>
<tr class="odd">
<td style="text-align: center;">Exit Payout</td>
<td style="text-align: center;"><em>e</em></td>
</tr>
<tr class="even">
<td style="text-align: center;">Pr(Exit | PUp)</td>
<td style="text-align: center;"><em>x</em></td>
</tr>
<tr class="odd">
<td style="text-align: center;">Pr(Exit | PDown)</td>
<td style="text-align: center;"><em>y</em></td>
</tr>
</tbody>
</table>
</div>
<div id="tbl-exit-r2g" class="quarto-layout-cell quarto-layout-cell-subref anchored" data-ref-parent="tbl-general-exit" style="flex-basis: 50.0%;justify-content: center;">
<table class="table">
<caption>(b) Round 2 game</caption>
<tbody>
<tr class="odd">
<td style="text-align: right;"></td>
<td style="text-align: center;"><strong>PUp</strong></td>
<td style="text-align: center;"><strong>PDown</strong></td>
</tr>
<tr class="even">
<td style="text-align: right;"><strong>Up</strong></td>
<td style="text-align: center;"><em>a</em></td>
<td style="text-align: center;"><em>b</em></td>
</tr>
<tr class="odd">
<td style="text-align: right;"><strong>Down</strong></td>
<td style="text-align: center;"><em>c</em></td>
<td style="text-align: center;"><em>d</em></td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
<p>Our problem has this abstract structure with <em>b</em> = <em>c</em> = <em>e</em> = <em>y</em> = 0, <em>x</em> = 0.5, <em>a</em> = 4, <em>d</em> = 3.</p>
<p>Now consider a simple variant of the above 5 step problem. The same things happen, but steps 2 and 3 are reversed. That is, Chooser decides on Up or Down after Demons make their predictions, but before they are told whether Demon-1 decided to Exit. Still, their choice will only matter if Demon-1 decided not to Exit, since their choices do not make a difference if Demon-1 Exits. Call this variant the Early Choice version, and the original the Late Choice variant. I don’t have any clear intuitions about what to do in most Exit Problems, save for this constraint on choices.</p>
<ul>
<li><strong>Exit Principle</strong>: In any Exit Problem, the same choices are permissible in the Early Choice and Late Choice variants.</li>
</ul>
<p>The reason comes from thinking about what Chooser is doing in the Early Choice variant. They are making a decision about what to do if Demon-1 doesn’t Exit. The way to make that decision is just to assume that Demon-1 doesn’t Exit, and then decide what to do. It just is the same choice as they face in the Late Choice variant, except now they make it in the context of a conditional. So they should decide it the same way.</p>
<p>One could also argue, I think correctly, that anyone who violates Exit Principle will violate a plausible version of the Sure Thing Principle. Such an argument seems sound to me, but the Sure Thing Principle is controversial, and I prefer to put more weight on the argument from how conditional reasoning works in the previous paragraph. (Indeed, I think using the Exit Principle to motivate a version of the Sure Thing Principle is more plausible than the reverse argument.)</p>
<p>To put the point in game-theoretic terms, there is no difference between extensive form and normal form reasoning when a decider has only one possible choice to make.</p>
<p>Any plausible theory that says that only Up is rationally playable in problems like <a href="#tbl-coord">Table&nbsp;8</a> cite above table will violate Exit Principle. Think about what they will say <a href="#tbl-early-choice">Table&nbsp;10</a>.</p>
<div id="tbl-early-choice" class="anchored">
<table class="table">
<caption>Table&nbsp;10: The Early Choice decision.</caption>
<tbody>
<tr class="odd">
<td style="text-align: right;"></td>
<td style="text-align: center;"><strong>PUp</strong></td>
<td style="text-align: center;"><strong>PMixed</strong></td>
<td style="text-align: center;"><strong>PDown</strong></td>
</tr>
<tr class="even">
<td style="text-align: right;"><strong>Up</strong></td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">0</td>
</tr>
<tr class="odd">
<td style="text-align: right;"><strong>Down</strong></td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">3</td>
</tr>
</tbody>
</table>
</div>
<p>In this problem, PUp means that both demons predict Up, PDown means that they both predict Down, and PMixed means that one predicts one, and one the other. This possibility is arbitrarily improbable, and the two strategies have the same expected return given M in any case, so we can ignore it. So really this game comes to <a href="#tbl-early-choice-simplified">Table&nbsp;11</a>.</p>
<div id="tbl-early-choice-simplified" class="anchored">
<table class="table">
<caption>Table&nbsp;11: The Early Choice decision simplified.</caption>
<tbody>
<tr class="odd">
<td style="text-align: right;"></td>
<td style="text-align: center;"><strong>PUp</strong></td>
<td style="text-align: center;"><strong>PDown</strong></td>
</tr>
<tr class="even">
<td style="text-align: right;"><strong>Up</strong></td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">0</td>
</tr>
<tr class="odd">
<td style="text-align: right;"><strong>Down</strong></td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">3</td>
</tr>
</tbody>
</table>
</div>
<p>Now presumably if one prefers Up in above table, it is because one prefers Up in any game like <a href="#tbl-general-coord">Table&nbsp;12</a> Table below where <em>x</em> &gt; <em>y</em> &gt; 0.</p>
<div id="tbl-general-coord" class="anchored">
<table class="table">
<caption>Table&nbsp;12: General coordination game.</caption>
<tbody>
<tr class="odd">
<td style="text-align: right;"></td>
<td style="text-align: center;"><strong>PUp</strong></td>
<td style="text-align: center;"><strong>PDown</strong></td>
</tr>
<tr class="even">
<td style="text-align: right;"><strong>Up</strong></td>
<td style="text-align: center;"><em>x</em></td>
<td style="text-align: center;">0</td>
</tr>
<tr class="odd">
<td style="text-align: right;"><strong>Down</strong></td>
<td style="text-align: center;">0</td>
<td style="text-align: center;"><em>y</em></td>
</tr>
</tbody>
</table>
</div>
<p>How could it be otherwise? Given expectationism, it’s not like there is anything special about the numbers 4 and 3. But anyone who endorses this policy will play Down <a href="#tbl-early-choice-simplified">Table&nbsp;11</a> and so, presumably, in <a href="#tbl-early-choice">Table&nbsp;10</a>. And that means they will violate Exit Principle.</p>
<p>The only view that is consistent with Exit Principle in cases like <a href="#tbl-general-coord">Table&nbsp;12</a>. And since in any such case, improving Up or Down be a tiny amount wouldn’t materially change the case, they must both be permissible after small sweetenings. So, given Exit Principle, the only viable theories are indecisive.</p>
<p>Exit Principle also offers a response to some intuitions that have led people to question CDT in recent years. <a href="#tbl-frustrating-button">Table&nbsp;13</a> is an example that Jack <span class="citation" data-cites="Spencer2023">Spencer (<a href="#ref-Spencer2023" role="doc-biblioref">2023</a>)</span> used to model the kind of case that’s at issue. As he notes, it is similar to the psychopath button case <span class="citation" data-cites="Egan2007-EGASCT">(<a href="#ref-Egan2007-EGASCT" role="doc-biblioref">Egan 2007</a>)</span>, the asymmetric Death in Damascus case <span class="citation" data-cites="Richter1984">(<a href="#ref-Richter1984" role="doc-biblioref">Richter 1984</a>)</span>, and other puzzles for CDT.</p>
<div id="tbl-frustrating-button" class="anchored">
<table class="table">
<caption>Table&nbsp;13: Frustrating Button (from <span class="citation" data-cites="Spencer2023">Spencer (<a href="#ref-Spencer2023" role="doc-biblioref">2023</a>)</span>).</caption>
<tbody>
<tr class="odd">
<td style="text-align: right;"></td>
<td style="text-align: center;"><strong>PUp</strong></td>
<td style="text-align: center;"><strong>PDown</strong></td>
</tr>
<tr class="even">
<td style="text-align: right;"><strong>Up</strong></td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">10</td>
</tr>
<tr class="odd">
<td style="text-align: right;"><strong>Down</strong></td>
<td style="text-align: center;">15</td>
<td style="text-align: center;">0</td>
</tr>
</tbody>
</table>
</div>
<p>If we embed Frustrating Button in an exit problem, the intuitions shift.</p>
<div id="tbl-frustrating-exit" class="tbl-parent quarto-layout-panel anchored">
<div class="panel-caption table-caption">
<p>Table&nbsp;14: An exit problem with Frustrating Button in round 2.</p>
</div>
<div class="quarto-layout-row quarto-layout-valign-top">
<div id="tbl-exit-param-fb" class="quarto-layout-cell quarto-layout-cell-subref anchored" data-ref-parent="tbl-frustrating-exit" style="flex-basis: 50.0%;justify-content: center;">
<table class="table">
<caption>(a) Exit Parameters</caption>
<tbody>
<tr class="odd">
<td style="text-align: center;">Exit Payout</td>
<td style="text-align: center;">-50</td>
</tr>
<tr class="even">
<td style="text-align: center;">Pr(Exit | PUp)</td>
<td style="text-align: center;">0.8</td>
</tr>
<tr class="odd">
<td style="text-align: center;">Pr(Exit | PDown)</td>
<td style="text-align: center;">0</td>
</tr>
</tbody>
</table>
</div>
<div id="tbl-exit-r2g-fb" class="quarto-layout-cell quarto-layout-cell-subref anchored" data-ref-parent="tbl-frustrating-exit" style="flex-basis: 50.0%;justify-content: center;">
<table class="table">
<caption>(b) Frustrating Button</caption>
<tbody>
<tr class="odd">
<td style="text-align: right;"></td>
<td style="text-align: center;"><strong>PUp</strong></td>
<td style="text-align: center;"><strong>PDown</strong></td>
</tr>
<tr class="even">
<td style="text-align: right;"><strong>Up</strong></td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">10</td>
</tr>
<tr class="odd">
<td style="text-align: right;"><strong>Down</strong></td>
<td style="text-align: center;">15</td>
<td style="text-align: center;">0</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
<p>The Early Version of <a href="#tbl-frustrating-exit">Table&nbsp;14</a> is <a href="#tbl-ev-fe">Table&nbsp;15</a>.</p>
<div id="tbl-ev-fe" class="anchored">
<table class="table">
<caption>Table&nbsp;15: Early Version of <a href="#tbl-frustrating-exit">Table&nbsp;14</a>.</caption>
<tbody>
<tr class="odd">
<td style="text-align: right;"></td>
<td style="text-align: center;"><strong>PUp</strong></td>
<td style="text-align: center;"><strong>PDown</strong></td>
</tr>
<tr class="even">
<td style="text-align: right;"><strong>Up</strong></td>
<td style="text-align: center;">-38</td>
<td style="text-align: center;">10</td>
</tr>
<tr class="odd">
<td style="text-align: right;"><strong>Down</strong></td>
<td style="text-align: center;">-37</td>
<td style="text-align: center;">0</td>
</tr>
</tbody>
</table>
</div>
<p>And if there is an intuition here, it is that it’s better to choose Down rather than Up.<a href="#fn12" class="footnote-ref" id="fnref12" role="doc-noteref"><sup>12</sup></a> This violates Exit Principle, and it seems incoherent to say that one would choose Down in this game, when Down just means playing Down in round 2, and if one were to reach round 2, one would prefer Up.</p>
<p>Exit Principle can also be used to argue against the non-expectationist theory offered by Lara <span class="citation" data-cites="BuchakRisk">Buchak (<a href="#ref-BuchakRisk" role="doc-biblioref">2013</a>)</span>, but that argument is more complicated, and I’ll leave it to an appendix.</p>
</section>
<section id="sec-dualmandate" class="level1" data-number="8">
<h1 data-number="8"><span class="header-section-number">8</span> Dual Mandate</h1>
<p>Say a decision tree is a series of steps with the following characteristics.</p>
<ul>
<li>At every step, Chooser either receives some information, or makes a choice.</li>
<li>Chooser knows before the first step what possible choices will be available at each step, given the prior steps, or what possible pieces of information could be received.</li>
<li>No matter what happens, the tree ends after finitely many steps. (Though it may end after more or fewer steps depending on what happens).</li>
<li>Chooser knows before the first step what payout they will receive given each possible sequence of choices and information.</li>
<li>Before the first step, chooser has a probability for each possible piece of information they could receive, given the prior steps in the tree.</li>
</ul>
<p>That’s incredibly abstract, but it excludes some possibilities. It excludes cases where Chooser learns along the way that they have hitherto unknown abilities. It excludes cases where Chooser gains the capacity to think new relevant thoughts along the way, say by meeting a new person and gaining the capacity to have singular thoughts about them.<a href="#fn13" class="footnote-ref" id="fnref13" role="doc-noteref"><sup>13</sup></a> Still, it does cover a lot of cases.</p>
<p>Say a strategy for a decision tree is a plan for what to do in every possible choice situation. Following the game theory textbooks, I really do mean <em>every</em> here. A strategy should say what to do in cases that are ruled out by Chooser’s prior choices. A strategy for playing chess as White might say to start with e4, but also include plans for what to do if you inexplicably start Na3. There are both mathematical and philosophical reasons for having such an expansive conception of strategies, but going into why is beyond the scope of this paper.</p>
<p>In philosophy, there are two common approaches to decision trees. The so-called resolute approach<a href="#fn14" class="footnote-ref" id="fnref14" role="doc-noteref"><sup>14</sup></a> says that one should simply treat the problem as like the kind of one-shot decisions we have discussed so far, except now one is choosing a strategy. Whatever one’s theory of choice is, one should simply apply it to the question of which strategy is best. The so-called sophisticated approach says that one should make the current choices that make best sense given one’s views about what one’s future self will do.</p>
<p>The orthodoxy in game theory, going back to at least <span class="citation" data-cites="Selten1965">Selten (<a href="#ref-Selten1965" role="doc-biblioref">1965</a>)</span>, is that both views are correct. When faced with a decision tree, Chooser should follow the advice of the sophisticated theorists, and (given they are ideally rational) do what would be best on the assumption that future choices will be rational. But in doing so, they should instantiate (part of) a strategy that could be rationally chosen by the resolute chooser. I call this the Dual Mandate approach, and I am going to defend it.</p>
<p>Start with why it is bad to just have a resolute approach.<a href="#fn15" class="footnote-ref" id="fnref15" role="doc-noteref"><sup>15</sup></a> Game theorists usually reject this approach because it means sometimes making a decision that one knows will have worse consequences than an available alternative. I’ll go over an example of this, though I should note it is rather violent. This is unavoidable; it is only in these violent cases that we can be sure the Chooser is really making things worse, and not acting for a strategic or reputational goal.</p>
<p>Chooser is the Prime Minister of a small country, and they are threatened by a large neighbour. Unfortunately, neighbour is thinking of carpet bombing Chooser’s capital, in retaliation for some perceived slight. Chooser has no air defences that would prevent a great destruction, and no allies who will rally to help. Fortunately, Chooser has a mighty weapon, a Doomsday device, that could destroy neighbour. Chooser has obviously threatened to use this, but neighbour suspects it is a bluff. This is for a good reason; the doomsday device would also destroy Chooser’s own country. Neighbour is known to employ a Demon who is at least 99% accurate in predicting what military plans Chooser will take. So Chooser can do Nothing (N), or use the Doomsday device (D), should neighbour attack. Chooser would obviously prefer no attack, and would certainly not use the device pre-emptively. So here is the table.</p>
<div id="tbl-retaliation" class="anchored">
<table class="table">
<caption>Table&nbsp;16: Deciding whether to retaliate.</caption>
<tbody>
<tr class="odd">
<td style="text-align: right;"></td>
<td style="text-align: center;"><strong>PN</strong></td>
<td style="text-align: center;"><strong>PD</strong></td>
</tr>
<tr class="even">
<td style="text-align: right;"><strong>N</strong></td>
<td style="text-align: center;">-1</td>
<td style="text-align: center;">0</td>
</tr>
<tr class="odd">
<td style="text-align: right;"><strong>D</strong></td>
<td style="text-align: center;">-50</td>
<td style="text-align: center;">-50</td>
</tr>
</tbody>
</table>
</div>
<p>In the top left, neighbour bombs Chooser’s capital, thinking correctly that Chooser will not retaliate. In the top right and lower right, neighbour is sufficiently scared of the doomsday device that they do nothing. But in the bottom left, neighbour attacks, and Chooser retaliates, creating a disaster for everyone, something 50 times worse than even the horrors of the carpet bombing.</p>
<p>Still, if Chooser is picking a strategy before anything starts, the strategy with the highest expected return is to plan to retaliate. This has an expected return of -0.5; since one time in a hundred it returns -50, and otherwise it returns 0. The resolute theorist says that’s what Chooser should do, even if they see the bombers coming, and they realise their bluff has failed. This seems absurd to me, and it is the kind of result that drives game theorists to the dual mandate, but resolute theorists are familiar with the point that their theory says that sometimes one should carry out a plan now known to be pointless. So instead of resting on this case, as decisive as it seems to many, I’ll run through two more arguments against a purely resolute theory.</p>
<p>Change the example so that Chooser has two advisors who are talking to him as the bombers come in. One of them says that the Demon is 99% reliable. The other says that the Demon is 97% reliable. Whether Chooser launches the doomsday device should, according to the resolute theorist, depend on which advisor Chooser believes. This is just absurd. A debate about the general accuracy of a demon can’t possibly be what these grave military decisions are based on.</p>
<p>Change the example again, and make it a bit more realistic. Chooser has the same two advisors, with the same views. Chooser thinks the one who says the Demon is 99% reliable is 60% likely to be right, and the other 40% likely. So Chooser forms the plan to retaliate, because right now that’s the strategy with highest expected return. But now, to everyone’s surprise, neighbour attacks. The resolute theorist will say that Chooser should stick to their (overpowered) guns. But think about how the choice of plans looks to Chooser now. The actions of neighbour are evidence about the reliability of the demon. And a simple application of Bayes’ Rule says that Chooser should now think the advisor who thought the demon was 97% reliable is 2/3 likely to be right. That is, given Chooser’s current evidence, retaliating wasn’t even the utility maximising strategy to start with. Yet it is what the resolute theorist, or at least the resolute theorist who is not also sophisticated, would have Chooser do. This is, again, absurd, and enough reason to give up on such a theory.</p>
<p>What about the other direction? Is it sensible to have a sophisticated theory that is not resolute? There does seem to be something puzzling about such theories. They are “diachronically exploitable” in the sense described by <span class="citation" data-cites="Spencer2021">Spencer (<a href="#ref-Spencer2021" role="doc-biblioref">2021a</a>)</span>. Let’s start with one example. Extend the theory offered by <span class="citation" data-cites="Gallow2020">Gallow (<a href="#ref-Gallow2020" role="doc-biblioref">2020</a>)</span> to make it a pure sophisticated dynamic theory. That is, in a decision tree, the chooser values future choices by the expected value of the choice they’ll make, and if that choice is guaranteed to end the decision tree, they use Gallow’s theory. Chooser is now offered the following two-step option. At step 1 they can choose to receive 1 or play the game in <a href="#tbl-gallow-sophisticated">Table&nbsp;17</a>.</p>
<div id="tbl-gallow-sophisticated" class="anchored">
<table class="table">
<caption>Table&nbsp;17: A challenge for pure sophisticated decision.</caption>
<tbody>
<tr class="odd">
<td style="text-align: right;"></td>
<td style="text-align: center;"><strong>PU</strong></td>
<td style="text-align: center;"><strong>PD</strong></td>
</tr>
<tr class="even">
<td style="text-align: right;"><strong>U</strong></td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">2</td>
</tr>
<tr class="odd">
<td style="text-align: right;"><strong>D</strong></td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">0</td>
</tr>
</tbody>
</table>
</div>
<p>If Chooser gets to step 2, they’ll play D, since it is the best option according to Gallow’s theory. So at step 1 they’ll choose the 1 rather than playing this game. But that’s absurd; they know they could have done better by simply playing the game and choosing U.</p>
<p>What the Dual Mandate says is that the last step of reasoning here is sound; it is a fair criticism of an agent to say that their strategy doesn’t make sense even if every step makes sense taken on its own. Since this does seem like a fair criticism, it is reasonable to adopt the Dual Mandate.</p>
<p>If one has a decisive theory, then a huge number of decision trees will be dilemmas, since it is unlikely that the optimal strategy matches the series of optimal choices. This is not a reason to reject the Dual Mandate; it’s another reason to reject decisiveness.</p>
<p>You might worry that the argument based around <a href="#tbl-gallow-sophisticated">Table&nbsp;17</a> is not really an objection to theories that reject the Dual Mandate, but just to the combination of that rejection and the endorsement of Gallow’s particular theory of decision. That worry is half right. This result is a problem for Gallow’s theory. But that doesn’t mean it isn’t also an argument for the Dual Mandate. The point of the Dual Mandate is not to criticise individual decisions, like taking the 1 in this game. It’s to criticise theories that endorse those decisions. It’s true that once we find the right theory of synchronic choice, the Dual Mandate will be unnecessary, since it will be automatically satisfied. But the Dual Mandate plays an essential role in selecting that theory.</p>
<p>Jack <span class="citation" data-cites="Spencer2021">Spencer (<a href="#ref-Spencer2021" role="doc-biblioref">2021a</a>)</span> has an example which he thinks tells against the Dual Mandate, or what he calls the requirement that Chooser not be diachronically exploitable.<a href="#fn16" class="footnote-ref" id="fnref16" role="doc-noteref"><sup>16</sup></a> The agent will play first the left and then the right game, and their payouts (shown in dollars) will be summed over the game. They won’t be told between the games what they got from the first game.<a href="#fn17" class="footnote-ref" id="fnref17" role="doc-noteref"><sup>17</sup></a></p>
<div id="tbl-newcomb-insurance" class="tbl-parent quarto-layout-panel anchored">
<div class="panel-caption table-caption">
<p>Table&nbsp;18: Ahmed Insurance (from <span class="citation" data-cites="Spencer2021">Spencer (<a href="#ref-Spencer2021" role="doc-biblioref">2021a</a>)</span>).</p>
</div>
<div class="quarto-layout-row quarto-layout-valign-top">
<div id="tbl-ni-left" class="quarto-layout-cell quarto-layout-cell-subref anchored" data-ref-parent="tbl-newcomb-insurance" style="flex-basis: 50.0%;justify-content: center;">
<table class="table">
<caption>(a) First game</caption>
<tbody>
<tr class="odd">
<td style="text-align: right;"></td>
<td style="text-align: center;"><strong>PU<sub>1</sub></strong></td>
<td style="text-align: center;"><strong>PD<sub>1</sub></strong></td>
</tr>
<tr class="even">
<td style="text-align: right;"><strong>U<sub>1</sub></strong></td>
<td style="text-align: center;">50</td>
<td style="text-align: center;">-50</td>
</tr>
<tr class="odd">
<td style="text-align: right;"><strong>D<sub>1</sub></strong></td>
<td style="text-align: center;">60</td>
<td style="text-align: center;">-40</td>
</tr>
</tbody>
</table>
</div>
<div id="tbl-ni-right" class="quarto-layout-cell quarto-layout-cell-subref anchored" data-ref-parent="tbl-newcomb-insurance" style="flex-basis: 50.0%;justify-content: center;">
<table class="table">
<caption>(b) Second game</caption>
<tbody>
<tr class="odd">
<td style="text-align: right;"></td>
<td style="text-align: center;"><strong>Correct</strong></td>
<td style="text-align: center;"><strong>Incorrect</strong></td>
</tr>
<tr class="even">
<td style="text-align: right;"><strong>U<sub>2</sub></strong></td>
<td style="text-align: center;">25</td>
<td style="text-align: center;">-75</td>
</tr>
<tr class="odd">
<td style="text-align: right;"><strong>D<sub>2</sub></strong></td>
<td style="text-align: center;">-25</td>
<td style="text-align: center;">75</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
<p>Note that in <a href="#tbl-ni-right">Table&nbsp;18 (b)</a>, the states are not the usual ones about Demon’s predictions. Rather, they are that the Demon made the Correct, or Incorrect, prediction in <a href="#tbl-ni-left">Table&nbsp;18 (a)</a>. There are eight strategies in this game, but since the Demon doesn’t care about what happens at non-chosen nodes, we won’t care either, and just focus on the four combinations of moves Chooser might make, and how they interact with Demon’s prediction. If we do that, we get the following table (also given by Spencer, and also with payouts in dollars).</p>
<div id="tbl-ni-strategic" class="anchored">
<table class="table">
<caption>Table&nbsp;19: Strategic form of Ahmed Insurance.</caption>
<tbody>
<tr class="odd">
<td style="text-align: right;"></td>
<td style="text-align: center;"><strong>PU<sub>1</sub></strong></td>
<td style="text-align: center;"><strong>PD<sub>1</sub></strong></td>
</tr>
<tr class="even">
<td style="text-align: right;"><strong>U<sub>1</sub>U<sub>2</sub></strong></td>
<td style="text-align: center;">75</td>
<td style="text-align: center;">-125</td>
</tr>
<tr class="odd">
<td style="text-align: right;"><strong>U<sub>1</sub>D<sub>2</sub></strong></td>
<td style="text-align: center;">25</td>
<td style="text-align: center;">25</td>
</tr>
<tr class="even">
<td style="text-align: right;"><strong>D<sub>1</sub>U<sub>2</sub></strong></td>
<td style="text-align: center;">-15</td>
<td style="text-align: center;">-15</td>
</tr>
<tr class="odd">
<td style="text-align: right;"><strong>D<sub>1</sub>D<sub>2</sub></strong></td>
<td style="text-align: center;">135</td>
<td style="text-align: center;">-65</td>
</tr>
</tbody>
</table>
</div>
<p>Spencer argues that even though D<sub>1</sub>U<sub>2</sub> is dominated by U<sub>1</sub>D<sub>2</sub> it might be rational to play it. After all, it is rational to bet on Demon being correct in <a href="#tbl-ni-right">Table&nbsp;18 (b)</a>, since Demon is arbitrarily good. And if one knows one is going to do that, one may as well take the sure extra $10 that playing U<sub>1</sub> rather than D<sub>1</sub> gives. So diachronic exploitability is consistent with rationality.</p>
<p>The reasoning of the previous paragraph fails because neither CDT, nor any other sensible decision theory, recommends taking two boxes in Newcomb Problems embedded in strategic interactions. This would be like thinking that CDT recommended always defecting in Iterated Prisoners’ Dilemma, even it was chancy whether the iterations came to an end after each round, so backward induction reasoning was unavailable. If Chooser has convinced themselves that they will play U<sub>2</sub>, and we’ll come back to whether they should believe that, then the choice in <a href="#tbl-ni-left">Table&nbsp;18 (a)</a> comes down to this.</p>
<div id="tbl-ni-new-left" class="anchored">
<table class="table">
<caption>Table&nbsp;20: First game in Ahmed Insurance, if D<sub>2</sub> will be played.</caption>
<tbody>
<tr class="odd">
<td style="text-align: right;"></td>
<td style="text-align: center;"><strong>PU<sub>1</sub></strong></td>
<td style="text-align: center;"><strong>PD<sub>1</sub></strong></td>
</tr>
<tr class="even">
<td style="text-align: right;"><strong>U<sub>1</sub></strong></td>
<td style="text-align: center;">75</td>
<td style="text-align: center;">-125</td>
</tr>
<tr class="odd">
<td style="text-align: right;"><strong>D<sub>1</sub></strong></td>
<td style="text-align: center;">-15</td>
<td style="text-align: center;">-15</td>
</tr>
</tbody>
</table>
</div>
<p>This game has two pure strategy equilibria, and on its own I think (because of the arguments in <a href="#sec-indecisive">Section&nbsp;7</a>) that either play is acceptable. In context though, either play is clearly unacceptable. Given that one chooses either U<sub>1</sub> or D<sub>1</sub>, the only reasonable thing to believe is that Demon has almost certainly predicted this, so it makes to play U<sub>2</sub>, since Demon is almost certainly correct. So one ends up playing U<sub>1</sub>U<sub>2</sub> or D<sub>1</sub>U<sub>2</sub>, both of which are dominated and hence absurd strategies.</p>
<p>Spencer argues that since the Demon is almost certainly accurate, Chooser should play U<sub>2</sub>, so they should play a dominated strategy, so the Dual Mandate doesn’t apply. (This assumes that synchronic choice rules out strictly dominated options in cases like this, but Spencer agrees that it does.) This argument only goes through if Chooser doesn’t have access to mixed strategies; i.e., if Chooser is not ideally practically rational. If Chooser does have access to mixed strategies, they should play a 50/50 mix of U<sub>1</sub> and D<sub>1</sub>, then choose D<sub>2</sub>. That is ratifiable as long as Chooser believes Demon plays PU<sub>1</sub> with probability 0.45, and PD<sub>1</sub> with probability 0.55. Since that’s the only ratifiable play for Demon, it’s reasonable for Chooser to believe this. If mixed strategies are allowed, this is not a case where the Dual Mandate fails.</p>
<p>In general, if mixed strategies are not allowed, the Dual Mandate is implausible. But that’s because without mixed strategies, cases like <a href="#tbl-newcomb-insurance">Table&nbsp;18</a> are dilemmas; they have no ratifiable choices. And it’s true that the Dual Mandate is implausible in dilemmas. Think back to the sinner described in <a href="#sec-ratify">Section&nbsp;6</a>. Imagine that sinner will in fact say that they get <em>d</em> days in heaven. Now complicate the case; they are offered a choice of <em>d</em>! days in heaven, or to make their own choice. If they will in fact choose <em>d</em>, they should simply take <em>d</em>!, even though there are strategies available, like choosing <em>d</em>!! days, that are better. Weird things happen when there are dilemmas around, and we shouldn’t judge decision theories against these cases.</p>
<p>The Dual Mandate is also implausible if Chooser thinks they will be irrational, or that they will have different preferences. Indeed, it is implausible if Chooser thinks they might either change or lose their mind. For example, Odysseus binds himself to the mast because he does not approve of future-Odysseus’s preferences. Professor Procrastinate<a href="#fn18" class="footnote-ref" id="fnref18" role="doc-noteref"><sup>18</sup></a> cite turns down a referee request because he does not trust his future self to be practically rational. Both of them deliberately turn down strategies that would be better than where they end up, because they do not trust their future selves to carry them out. They are alienated in this way from their future selves. When one does not endorse one’s future preferences, or does not trust one’s rationality in the future, it makes sense to be alienated from one’s future self in this way. In such cases, one’s future self is just another part of the world that must be predicted and worked around. And so it might make sense to forego, as Odysseus and Procrastinate forego, strategies that one’s future self will not be so kind as to carry out.</p>
<p>My main claim here is when neither of those two conditions obtain, i.e., when one knows that one’s future self will be rational and have the same preferences, one’s choices should make strategic sense. That is, they should satisfy the fairly weak condition that they are part of some strategy that one could choose if one was simply choosing a strategy for the whole tree. Unless one fears future irrationality, or future change of preference, one should not be alienated from one’s future self. If Chooser takes 1 rather than play <a href="#tbl-gallow-sophisticated">Table&nbsp;17</a>, they are alienated in this way. They have to think, I know I’d be better off if I played U. But that fool future-me will play D instead, and blow up the plan. But future-them is not a fool; by hypothesis they are known to be ideally rational. So it isn’t coherent to think this way, and that reveals that it is incoherent to ‘rationally’ take the 1. And that is why the Dual Mandate requires that one’s strategy be rational, and not just the moves that make up the strategy.</p>
</section>
<section id="sec-substantive" class="level1" data-number="9">
<h1 data-number="9"><span class="header-section-number">9</span> Substantive</h1>
<p>Here are two interesting characters. Piz wants to put mud on his pizza. This won’t bring him joy, or any other positive emotions; he has a non-instrumental desire for mud pizza. Za wants to eat a tasty pizza, and believes that putting mud on his pizza will make it tasty. There is a long tradition of saying that the point of philosophical decision theory is not to evaluate beliefs and desires, but merely to say what actions those beliefs and desires do or should issue in. On such a view, both Piz and Za should (or at least will) put mud on their pizzas. Here is David Lewis expressing such a view.<a href="#fn19" class="footnote-ref" id="fnref19" role="doc-noteref"><sup>19</sup></a></p>
<blockquote class="blockquote">
<p>The central question of decision theory is: which choices are the ones that serve one’s desires according to one’s beliefs? <span class="citation" data-cites="Lewis-Gorman-10071979">(<a href="#ref-Lewis-Gorman-10071979" role="doc-biblioref">Lewis 2020, 472</a>)</span></p>
</blockquote>
<p>We need one caveat on this. Philosophical decision theories typically do not issue verdicts unless the chooser satisfies some coherence constraints. So it’s not quite that the theory says nothing about what the beliefs and desires should be. It’s that it says nothing <em>substantive</em> about what the beliefs and desires should be. Purely structural constraints, like transitivity of preferences, or belief in the law of excluded middle, may be imposed.</p>
<p>At least sometimes, game theorists impose non-structural, substantive conditions on the beliefs of players. Most notably, the “intuitive criterion” of <span class="citation" data-cites="ChoKreps1987">Cho and Kreps (<a href="#ref-ChoKreps1987" role="doc-biblioref">1987</a>)</span> is meant to be continuous with other equilibrium conditions, and is a substantive constraint. Someone who violates it has coherent beliefs that don’t conform to their evidence. The intuitive criterion takes some time to set up, but I’ll get to a simplified version of it later in this section.</p>
<p>First, I’ll note some general reasons for scepticism about this use of the substantive-structural distinction. One obvious point is that Piz and Za do not look like rational choosers. Another is that this draws distinctions between overly similar characters, such as these two, Cla and Sic. Both of them have taken classes in classical statistics, but only skimmed the textbooks without attending to the details. Cla came away with the belief that any experiment with a <em>P</em> value less than 0.05 proved that its hypothesis is true. Sic came away with a standing disposition to belief the hypothesis whenever there was an experiment with a <em>P</em> value less than 0.05. Cla is incoherent; there is no possible world where that belief is true. Sic is coherent; any one of their beliefs could be true. It’s just they just have a disposition to often form substantially irrational beliefs. Personally, I don’t think the difference between Cla and Sic is important enough to be philosophically load bearing. Lastly, it has proven incredibly hard to even define what makes a norm structural. The most important recent attempt is in Alex Worsnip’s book Fitting Things Together: Coherence and the Demands of Structural Rationality <span class="citation" data-cites="Worsnip2021">(<a href="#ref-Worsnip2021" role="doc-biblioref">Worsnip 2021</a>)</span>. Here’s his definition:</p>
<blockquote class="blockquote">
<p><em>Incoherence Test</em>. A set of attitudinal mental states is jointly incoherent iff it is (partially) constitutive of (at least some of) the states in the set that any agent who holds this set of states has a disposition, when conditions of full transparency are met, to revise at least one of the states. <span class="citation" data-cites="Worsnip2021">(<a href="#ref-Worsnip2021" role="doc-biblioref">Worsnip 2021, 132</a>)</span></p>
</blockquote>
<p>This won’t capture nearly enough. If probabilism is correct, then non-probabilists about uncertainty like Glenn <span class="citation" data-cites="Shafer1976">Shafer (<a href="#ref-Shafer1976" role="doc-biblioref">1976</a>)</span> endorse incoherent views. If expectationalism is correct, then non-expectationalist decision theorists, like Lara <span class="citation" data-cites="BuchakRisk">Buchak (<a href="#ref-BuchakRisk" role="doc-biblioref">2013</a>)</span>, endorse incoherent views. If classical logic is correct, then intuitionist logicians like Crispin <span class="citation" data-cites="WrightVaguenessCollection">Wright (<a href="#ref-WrightVaguenessCollection" role="doc-biblioref">2021</a>)</span> are incoherent. Those three all seem to meet Worsnip’s conditions of full transparency, and don’t seem disposed to revise their beliefs. Maybe this is just a problem with Worsnip’s definition, but it is also a reason to be sceptical that there even is a distinction to be drawn here.</p>
<p>Even if the substantive/structural distinction can be made precise, and shown to do philosophical work, it won’t track the notion game theorists most care about. We can see this with a version of the beer-quiche game <span class="citation" data-cites="ChoKreps1987">Cho and Kreps (<a href="#ref-ChoKreps1987" role="doc-biblioref">1987</a>)</span>, here translated into decision-theoretic language.</p>
<p>There are five steps in the game.</p>
<ol type="1">
<li>A coin will be flipped, landing Heads or Tails. It is biased, 60% likely to land Heads. It will be shown to Chooser, but not to Demon.</li>
<li>Chooser will say either Heads or Tails.</li>
<li>Demon, knowing what Chooser has said, and being arbitrarily good at predicting Chooser’s strategy<a href="#fn20" class="footnote-ref" id="fnref20" role="doc-noteref"><sup>20</sup></a>. will say Heads if it is more probable the coin landed Heads, and Tails if it is more probable the coin landed Tails.<a href="#fn21" class="footnote-ref" id="fnref21" role="doc-noteref"><sup>21</sup></a></li>
<li>Chooser is paid $30 if Demon says Heads, and nothing if Demon says Tails.</li>
<li>Chooser is paid $10 if what they say matches how the coin landed, and nothing otherwise. This is on top of the payment at step 4, so Chooser could make up to $40.</li>
</ol>
<p>If you prefer things in table form, here are the payouts chooser gets, given what happens at steps 1-3.</p>
<div id="tbl-cho-kreps" class="anchored">
<table class="table">
<caption>Table&nbsp;21: The coin game.</caption>
<tbody>
<tr class="odd">
<td style="text-align: center;"><strong>Coin</strong></td>
<td style="text-align: center;"><strong>Chooser</strong></td>
<td style="text-align: center;"><strong>Demon</strong></td>
<td style="text-align: center;"><strong>Dollars</strong></td>
</tr>
<tr class="even">
<td style="text-align: center;">H</td>
<td style="text-align: center;">H</td>
<td style="text-align: center;">H</td>
<td style="text-align: center;">40</td>
</tr>
<tr class="odd">
<td style="text-align: center;">H</td>
<td style="text-align: center;">H</td>
<td style="text-align: center;">T</td>
<td style="text-align: center;">10</td>
</tr>
<tr class="even">
<td style="text-align: center;">H</td>
<td style="text-align: center;">T</td>
<td style="text-align: center;">H</td>
<td style="text-align: center;">30</td>
</tr>
<tr class="odd">
<td style="text-align: center;">H</td>
<td style="text-align: center;">T</td>
<td style="text-align: center;">T</td>
<td style="text-align: center;">0</td>
</tr>
<tr class="even">
<td style="text-align: center;">T</td>
<td style="text-align: center;">H</td>
<td style="text-align: center;">H</td>
<td style="text-align: center;">30</td>
</tr>
<tr class="odd">
<td style="text-align: center;">T</td>
<td style="text-align: center;">H</td>
<td style="text-align: center;">T</td>
<td style="text-align: center;">0</td>
</tr>
<tr class="even">
<td style="text-align: center;">T</td>
<td style="text-align: center;">T</td>
<td style="text-align: center;">H</td>
<td style="text-align: center;">40</td>
</tr>
<tr class="odd">
<td style="text-align: center;">T</td>
<td style="text-align: center;">T</td>
<td style="text-align: center;">T</td>
<td style="text-align: center;">10</td>
</tr>
</tbody>
</table>
</div>
<p>What will Chooser do? There are two coherent things for Chooser to do, though each of them is only coherent given a background belief that isn’t entailed by the evidence.</p>
<ol type="1">
<li>Chooser could say Heads however the coin lands. Demon gets no information from Chooser, so their probability that the coin landed Heads is 0.6, so they will say Heads. Further, Chooser believes that if they were to say Tails, Demon would say Tails, so saying Heads produces the best expected return even after seeing the coin.</li>
<li>Chooser could say Tails however the coin lands. Demon gets no information from Chooser, so their probability that the coin landed Heads is 0.6, so they will say Heads. Further, Chooser believes that if they were to say Heads, Demon would say Tails, so saying Tails produces the best expected return even after seeing the coin.</li>
</ol>
<p>While both of these are coherent, there is something very odd, very unintuitive about option 2. I guess we’ve been trained to be sceptical when philosophers report intuitions, but here we have a very large data pool to draw on. Cho and Kreps reported essentially the same intuition. Their paper has been cited tens of thousands of times, and I don’t think this intuition has been often questioned. Option 2, while coherent, is unintuitive. It is the kind of option that the theory of rationality behind game theory, and behind decision theory, should rule out.</p>
<p>But what about it is incoherent? One might think it is because it has an expected return of $34, while option 1 has an expected return of $36. But we showed in section Indecisive ref that using expected returns to choose between coherent options leads to implausible results. Moreover, if you change the payout in the bottom row to $50, the intuition doesn’t really go away, but the expected return of option 2 is now $38; higher option 1’s payout.<a href="#fn22" class="footnote-ref" id="fnref22" role="doc-noteref"><sup>22</sup></a> Alternatively, one might think it is because option 2 requires Chooser to believe a counterfactual that is not entailed by the evidence. But option 1 also requires Chooser to believe a counterfactual that is not entailed by the evidence. That can’t be the difference between them, but it is closer to the truth.</p>
<p>What Cho and Kreps argue, persuasively, is that the difference between the options is that in one case the counterfactual belief is reasonable, and in the other it is unreasonable. Assume Chooser plans to adopt option 1. But when it becomes time to play, they change their mind, and say Tails. What would explain that? Not the coin landing Heads - given their plan, they will get the maximum possible payout by sticking to the plan (assuming Demon has done their job). No, the only plausible explanation is the coin landed Tails, and Chooser was (foolishly) chasing the extra $10. In option 1, Chooser believes the counterfactual that’s grounded in Demon picking an explanation that makes sense. What about in option 2? Here, everything is back to front. If Chooser is ever going to depart from their plan, it’s when the coin lands Heads. Then Chooser might chase the extra $10 by saying Heads. But Chooser has to believe that were they to depart from the plan, Demon would draw the explanation that makes no sense whatsoever, that they gave up on their plan even though it was about to lead to the best possible outcome. This makes no sense at all. And in fact it makes less sense the more you increase the payout in line 8.</p>
<p>So that’s why decision theory requires substantive rationality. The right decision theory should say to take option 1. And the argument against option 2 is not that it is incoherent, but that carrying it out requires believing Demon will do things that make no sense given Demon’s evidence. It is substantive, not structural, rationality that rules out option 2. And yet, as the game theorists have insisted, option 2 must be ruled out. So decision theory should be sensitive to substantial rationality.</p>
</section>
<section id="sec-weak" class="level1" data-number="10">
<h1 data-number="10"><span class="header-section-number">10</span> Weak Dominance, Once</h1>
<p>Why weak dominance?</p>
<p>PU PD U 1 1 D 0 1</p>
<ol type="1">
<li>D seems like a needless risk</li>
<li>If Pr(Demon Correct) &lt; 1, then D isn’t ratifiable</li>
</ol>
<p>Why only once?</p>
<ol type="1">
<li><p>Order variance (Bonano)</p></li>
<li><p>Money burning (cite Stalnaker)</p></li>
<li><p>This game</p>
<p>PU PD PX U 1 1 0 D 0 1 2 X 0 1 1</p></li>
</ol>
<p>Iterated weak dominance would first rule out X, then PX, then D, leaving U as the only play. But D makes sense, even if the demon might make a mistake. Indeed, it makes more sense than U by some measures.</p>
</section>
<section id="sec-conclusion" class="level1" data-number="11">
<h1 data-number="11"><span class="header-section-number">11</span> Conclusion</h1>
<ul>
<li>State positive theory</li>
<li>Include clause for weak dominance</li>
<li>Note that it can be dropped if you insist that Pr(Demon correct) &lt; 1.</li>
<li>Note that it meets these 9 conditions</li>
<li>Final flourish</li>
</ul>

</section>


<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" role="doc-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" role="list">
<div id="ref-Akerlof1970" class="csl-entry" role="listitem">
Akerlof, George. 1970. <span>“The Market for "Lemons": Quality Uncertainty and the Market Mechanism.”</span> <em>Quarterly Journal of Economics</em> 84 (3): 488–500. <a href="https://doi.org/10.2307/1879431">https://doi.org/10.2307/1879431</a>.
</div>
<div id="ref-Arntzenius2008" class="csl-entry" role="listitem">
Arntzenius, Frank. 2008. <span>“No Regrets; or, Edith Piaf Revamps Decision Theory.”</span> <em>Erkenntnis</em> 68 (2): 277–97. <a href="https://doi.org/10.1007/s10670-007-9084-8">https://doi.org/10.1007/s10670-007-9084-8</a>.
</div>
<div id="ref-Barnettnd" class="csl-entry" role="listitem">
Barnett, David James. 2022. <span>“Graded Ratifiability.”</span> <em>Journal of Philosophy</em> 119 (2): 57–88. <a href="https://doi.org/10.5840/jphil202211925">https://doi.org/10.5840/jphil202211925</a>.
</div>
<div id="ref-Bonanno2018" class="csl-entry" role="listitem">
Bonanno, Giacomo. 2018. <span>“Game Theory.”</span> Davis, CA: Kindle Direct Publishing. 2018. <a href="http://faculty.econ.ucdavis.edu/faculty/bonanno/GT_Book.html">http://faculty.econ.ucdavis.edu/faculty/bonanno/GT_Book.html</a>.
</div>
<div id="ref-BuchakRisk" class="csl-entry" role="listitem">
Buchak, Lara. 2013. <em>Risk and Rationality</em>. Oxford: Oxford University Press.
</div>
<div id="ref-Chang2002" class="csl-entry" role="listitem">
Chang, Ruth. 2002. <span>“The Possibility of Parity.”</span> <em>Ethics</em> 112 (4): 659–88. <a href="https://doi.org/10.1086/339673">https://doi.org/10.1086/339673</a>.
</div>
<div id="ref-ChoKreps1987" class="csl-entry" role="listitem">
Cho, In-Koo, and David M. Kreps. 1987. <span>“Signalling Games and Stable Equilibria.”</span> <em>The Quarterly Journal of Economics</em> 102 (2): 179–221. <a href="https://doi.org/10.2307/1885060">https://doi.org/10.2307/1885060</a>.
</div>
<div id="ref-Davey2011" class="csl-entry" role="listitem">
Davey, Kevin. 2011. <span>“Idealizations and Contextualism in Physics.”</span> <em>Philosophy of Science</em> 78 (1): 16–38. <a href="https://doi.org/10.1086/658093">https://doi.org/10.1086/658093</a>.
</div>
<div id="ref-Egan2007-EGASCT" class="csl-entry" role="listitem">
Egan, Andy. 2007. <span>“<span class="nocase">Some Counterexamples to Causal Decision Theory</span>.”</span> <em>Philosophical Review</em> 116 (1): 93–114. <a href="https://doi.org/10.1215/00318108-2006-023">https://doi.org/10.1215/00318108-2006-023</a>.
</div>
<div id="ref-Elliot2019" class="csl-entry" role="listitem">
Elliott, Edward. 2019. <span>“Normative Decision Theory.”</span> <em>Analysis</em> 79 (4): 755–72. <a href="https://doi.org/10.1093/analys/anz059">https://doi.org/10.1093/analys/anz059</a>.
</div>
<div id="ref-EysterRabin2005" class="csl-entry" role="listitem">
Eyster, Erik, and Matthew Rabin. 2005. <span>“Cursed Equilibrium.”</span> <em>Econometrica</em> 73 (5): 1623–72. <a href="https://10.1111/j.1468-0262.2005.00631.x">10.1111/j.1468-0262.2005.00631.x</a>.
</div>
<div id="ref-Fuscond" class="csl-entry" role="listitem">
Fusco, Melissa. n.d. <span>“Absolution of a Causal Decision Theorist.”</span> No<span>û</span>s. <a href="https://doi.org/10.1111/nous.12459">https://doi.org/10.1111/nous.12459</a>.
</div>
<div id="ref-Gallow2020" class="csl-entry" role="listitem">
Gallow, J. Dmitri. 2020. <span>“The Causal Decision Theorist’s Gudie to Managing the News.”</span> <em>The Journal of Philosophy</em> 117 (3): 117–49.
</div>
<div id="ref-Gallownd" class="csl-entry" role="listitem">
———. n.d. <span>“The Sure Thing Principle Leads to Instability.”</span> Philosophical Quarterly. <a href="https://philpapers.org/archive/GALTST-2.pdf">https://philpapers.org/archive/GALTST-2.pdf</a>.
</div>
<div id="ref-Goodsellnd" class="csl-entry" role="listitem">
Goodsell, Zachary. n.d. <span>“Decision Theory Unbound.”</span> No<span>û</span>s. <a href="https://doi.org/10.1111/nous.12473">https://doi.org/10.1111/nous.12473</a>.
</div>
<div id="ref-Harper1986" class="csl-entry" role="listitem">
Harper, William. 1986. <span>“Mixed Strategies and Ratifiability in Causal Decision Theory.”</span> <em>Erkenntnis</em> 24 (1): 25–36. <a href="https://doi.org/10.1007/BF00183199">https://doi.org/10.1007/BF00183199</a>.
</div>
<div id="ref-Harper1988" class="csl-entry" role="listitem">
Harper, William L. 1988. <span>“Causal Decision Theory and Game Theory: A Classic Argument for Equilibrium Solutions, a Defense of Weak Equilibria, and a New Problem for the Normal Form Representation.”</span> In <em>Causation in Decision, Belief Change, and Statistics: Proceedings of the Irvine Conference on Probability and Causation</em>, edited by William L. Harper and Brian Skyrms, 25–48. Dordrecht: Springer Netherlands. <a href="https://doi.org/10.1007/978-94-009-2865-7_2">https://doi.org/10.1007/978-94-009-2865-7_2</a>.
</div>
<div id="ref-JacksonPargetter1986" class="csl-entry" role="listitem">
Jackson, Frank, and Robert Pargetter. 1986. <span>“Oughts, Options, and Actualism.”</span> <em>Philosophical Review</em> 95 (2): 233–55. <a href="https://doi.org/10.2307/2185591">https://doi.org/10.2307/2185591</a>.
</div>
<div id="ref-Jeffrey1983" class="csl-entry" role="listitem">
Jeffrey, Richard. 1983. <span>“Bayesianism with a Human Face.”</span> In <em>Testing Scientific Theories</em>, edited by J. Earman (ed.). Minneapolis: University of Minnesota Press.
</div>
<div id="ref-LevinsteinSoares2020" class="csl-entry" role="listitem">
Levinstein, Benjamin Anders, and Nate Soares. 2020. <span>“Cheating Death in Damascus.”</span> <em>Journal of Philosophy</em> 117 (5): 237–66. <a href="https://doi.org/10.5840/jphil2020117516">https://doi.org/10.5840/jphil2020117516</a>.
</div>
<div id="ref-Lewis1979e" class="csl-entry" role="listitem">
Lewis, David. 1979. <span>“Prisoners’ Dilemma Is a <span>N</span>ewcomb Problem.”</span> <em>Philosophy and Public Affairs</em> 8 (3): 235–40.
</div>
<div id="ref-Lewis1981e" class="csl-entry" role="listitem">
———. 1981. <span>“Why Ain’cha Rich?”</span> <em>No<span>û</span>s</em> 15 (3): 377–80. <a href="https://doi.org/10.2307/2215439">https://doi.org/10.2307/2215439</a>.
</div>
<div id="ref-Lewis-Gorman-10071979" class="csl-entry" role="listitem">
———. 2020. <span>“Letter to Gregory Kavka, 19 April 1989.”</span> In <em>Philosophical Letters of David <span>K</span>. Lewis</em>, edited by Helen Beebee and A. R. J. Fisher, 2:472–73. Oxford: Oxford University Press.
</div>
<div id="ref-McClennan1990" class="csl-entry" role="listitem">
McClennan, Edward. 1990. <em>Rationality and Dynamic Choice</em>. Cambridge: Cambridge University Press.
</div>
<div id="ref-Nozick1969" class="csl-entry" role="listitem">
Nozick, Robert. 1969. <span>“Newcomb’s Problem and Two Principles of Choice.”</span> In <em>Essays in Honor of Carl g. Hempel: A Tribute on the Occasion of His Sixty-Fifth Birthday</em>, edited by Nicholas Rescher, 114–46. Riedel: Springer.
</div>
<div id="ref-Podgorski2020" class="csl-entry" role="listitem">
Podgorski, Aberlard. 2020. <span>“Tournament Decision Theory.”</span> <em>No<span>û</span>s</em> tbc (tbc): xx–. <a href="https://doi.org/10.1111/nous.12353">https://doi.org/10.1111/nous.12353</a>.
</div>
<div id="ref-Quiggin1982" class="csl-entry" role="listitem">
Quiggin, John. 1982. <span>“A Theory of Anticipated Utility.”</span> <em>Journal of Economic Behavior &amp; Organization</em> 3 (4): 323–43. <a href="https://doi.org/10.1016/0167-2681(82)90008-7">https://doi.org/10.1016/0167-2681(82)90008-7</a>.
</div>
<div id="ref-Richter1984" class="csl-entry" role="listitem">
Richter, Reed. 1984. <span>“Rationality Revisited.”</span> <em>Australasian Journal of Philosophy</em> 62 (4): 393–404. <a href="https://doi.org/10.1080/00048408412341601">https://doi.org/10.1080/00048408412341601</a>.
</div>
<div id="ref-Selten1965" class="csl-entry" role="listitem">
Selten, Reinhard. 1965. <span>“Spieltheoretische Behandlung Eines Oligopolmodells Mit Nachfragetr<span>ä</span>gheit.”</span> <em>Zeitschrift f<span>ü</span>r Die Gesamte Staatswissenschaft</em> 121 (2): 301–24.
</div>
<div id="ref-Shafer1976" class="csl-entry" role="listitem">
Shafer, Glenn. 1976. <em>A Mathematical Theory of Evidence</em>. Princeton: Princeton University Press.
</div>
<div id="ref-Skyrms1984" class="csl-entry" role="listitem">
Skyrms, Brian. 1984. <em>Pragmatics and Empiricism</em>. New Haven, CT: Yale University Press.
</div>
<div id="ref-Skyrms2004" class="csl-entry" role="listitem">
———. 2004. <em>The Stag Hunt and the Evolution of Social Structure</em>. Cambridge: <span>C</span>ambridge <span>U</span>niversity <span>P</span>ress.
</div>
<div id="ref-Spencer2021" class="csl-entry" role="listitem">
Spencer, Jack. 2021a. <span>“An Argument Against Causal Decision Theory.”</span> <em>Analysis</em> 81 (1): 52–61. <a href="https://doi.org/10.1093/analys/anaa037">https://doi.org/10.1093/analys/anaa037</a>.
</div>
<div id="ref-Spencer2021b" class="csl-entry" role="listitem">
———. 2021b. <span>“Rational Monism and Rational Pluralism.”</span> <em>Philosophical Studies</em> 178: 1769–1800. <a href="https://doi.org/10.1007/s11098-020-01509-9">https://doi.org/10.1007/s11098-020-01509-9</a>.
</div>
<div id="ref-Spencer2023" class="csl-entry" role="listitem">
———. 2023. <span>“Can It Be Irrational to Knowingly Choose the Best?”</span> <em>Australasian Journal of Philosophy</em> 101 (1): 128–39. <a href="https://doi.org/10.1080/00048402.2021.1958880">https://doi.org/10.1080/00048402.2021.1958880</a>.
</div>
<div id="ref-SpencerWells2019" class="csl-entry" role="listitem">
Spencer, Jack, and Ian Wells. 2019. <span>“Why Take Both Boxes?”</span> <em><span>P</span>hilosophy and <span>P</span>henomenological <span>R</span>esearch</em> 99 (1): 27–48. <a href="https://doi.org/10.1111/phpr.12466">https://doi.org/10.1111/phpr.12466</a>.
</div>
<div id="ref-Stalnaker2008" class="csl-entry" role="listitem">
Stalnaker, Robert. 2008. <em>Our Knowledge of the Internal World</em>. Oxford: Oxford University Press.
</div>
<div id="ref-Strevens2008" class="csl-entry" role="listitem">
Strevens, Michael. 2008. <em>Depth: An Account of Scientific Explanations</em>. Cambridge, MA: Harvard University Press.
</div>
<div id="ref-Sutton2000" class="csl-entry" role="listitem">
Sutton, John. 2000. <em>Marshall’s Tendencies: What Can Economists Know?</em> Cambridge, MA: <span>MIT</span> Press.
</div>
<div id="ref-Wedgwood2013a" class="csl-entry" role="listitem">
Wedgwood, Ralph. 2013. <span>“Gandalf’s Solution to the Newcomb Problem.”</span> <em>Synthese</em> 190: 2643–75. <a href="https://doi.org/10.1007/s11229-011-9900-1">https://doi.org/10.1007/s11229-011-9900-1</a>.
</div>
<div id="ref-Weirich1985" class="csl-entry" role="listitem">
Weirich, Paul. 1985. <span>“Decision Instability.”</span> <em>Australasian Journal of Philosophy</em> 63 (4): 465–72. <a href="https://doi.org/10.1080/00048408512342061">https://doi.org/10.1080/00048408512342061</a>.
</div>
<div id="ref-Wilson1967" class="csl-entry" role="listitem">
Wilson, Robert B. 1967. <span>“Competitive Bidding with Asymmetric Information.”</span> <em>Management Science</em> 13 (11): 816–20. <a href="https://doi.org/10.1287/mnsc.13.11.816">https://doi.org/10.1287/mnsc.13.11.816</a>.
</div>
<div id="ref-Worsnip2021" class="csl-entry" role="listitem">
Worsnip, Alex. 2021. <em>Fitting Things Together: Coherence and the Demands of Structural Rationality</em>. Oxford: Oxford University Press.
</div>
<div id="ref-WrightVaguenessCollection" class="csl-entry" role="listitem">
Wright, Crispin. 2021. <em>The Riddle of Vagueness: Selected Essays 1975-2020</em>. Oxford: Oxford University Press.
</div>
</div></section><section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes"><h2 class="anchored quarto-appendix-heading">Footnotes</h2>

<ol>
<li id="fn1"><p>This is a special case of Lipsey and Lancaster’s Theory of the Second Best <span class="citation" data-cites="LipseyLancaster1956">(<a href="#ref-LipseyLancaster1956" role="doc-biblioref"><strong>LipseyLancaster1956?</strong></a>)</span>. If you don’t have control over every parameter, setting the parameters you do control to the ideal values is generally inadvisable.<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2"><p>I’m drawing here on work on the nature of idealisations by Michael <span class="citation" data-cites="Strevens2008">Strevens (<a href="#ref-Strevens2008" role="doc-biblioref">2008</a>)</span> and by Kevin <span class="citation" data-cites="Davey2011">Davey (<a href="#ref-Davey2011" role="doc-biblioref">2011</a>)</span>.<a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn3"><p>I learned about this paper from the excellent discussion of the case in <span class="citation" data-cites="Sutton2000">Sutton (<a href="#ref-Sutton2000" role="doc-biblioref">2000</a>)</span>.<a href="#fnref3" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn4"><p>A somewhat similar point is made in the example of the drowning dog on page 216 of <span class="citation" data-cites="Bonanno2018">Bonanno (<a href="#ref-Bonanno2018" role="doc-biblioref">2018</a>)</span>.<a href="#fnref4" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn5"><p>There is a survey article from a few years ago - <span class="citation" data-cites="Elliot2019">Elliott (<a href="#ref-Elliot2019" role="doc-biblioref">2019</a>)</span> - that has summaries of the then state-of-the-art on these two questions. And it makes it very striking how little the literatures on each of them overlap.<a href="#fnref5" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn6"><p>This point is made by <span class="citation" data-cites="Harper1988">W. L. Harper (<a href="#ref-Harper1988" role="doc-biblioref">1988</a>)</span>, and many (though not all) of the conclusions I draw in this paper will be similar to ones he drew.<a href="#fnref6" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn7"><p>See, for instance, the introduction of them on page 136 of <span class="citation" data-cites="Bonanno2018">Bonanno (<a href="#ref-Bonanno2018" role="doc-biblioref">2018</a>)</span>. And note that we get 135 pages before the notion of an expectation is introduced; that’s how much is done simply with dominance reasoning<a href="#fnref7" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn8"><p>Note that I’m using <em>CDT</em> here as the name of a family of theories, not a particular theory. So it’s not a great name; Causal Decision Theory is not a theory. Different versions of CDT can, and do, differ in what they say about the Stag Hunt cases I’ll discuss in <a href="#sec-indecisive">Section&nbsp;7</a>. But the label seems entrenched, so I’ll use it. In contrast, evidential decision theory, EDT, is a theory; it is a full account of what to do in all cases.<a href="#fnref8" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn9"><p>Note that this is true even if days in heaven have diminishing marginal utility, so the dilemma can arise even if we work within bounded utility theory. This is not just the kind of problem, as discussed by <span class="citation" data-cites="Goodsellnd">Goodsell (<a href="#ref-Goodsellnd" role="doc-biblioref">n.d.</a>)</span>, that arises in decision theory with unbounded utilities.<a href="#fnref9" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn10"><p>For much more on the philosophical importance of Stag Hunts, see <span class="citation" data-cites="Skyrms2004">Skyrms (<a href="#ref-Skyrms2004" role="doc-biblioref">2004</a>)</span>.<a href="#fnref10" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn11"><p>The view I’m going to develop is hence similar to the ‘permissive CDT’ defended by <span class="citation" data-cites="Fuscond">Fusco (<a href="#ref-Fuscond" role="doc-biblioref">n.d.</a>)</span>.<a href="#fnref11" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn12"><p>The theory offered in <span class="citation" data-cites="Spencer2021b">Spencer (<a href="#ref-Spencer2021b" role="doc-biblioref">2021b</a>)</span> agrees with intuition here.<a href="#fnref12" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn13"><p>Following <span class="citation" data-cites="Stalnaker2008">Stalnaker (<a href="#ref-Stalnaker2008" role="doc-biblioref">2008</a>)</span>, I think it excludes the Sleeping Beauty case, since there Beauty gains the capacity to have singular thoughts about a time, the ‘now’ when she awakes, that she did not previously have.<a href="#fnref13" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn14"><p>Most notably defended by <span class="citation" data-cites="McClennan1990">McClennan (<a href="#ref-McClennan1990" role="doc-biblioref">1990</a>)</span>.<a href="#fnref14" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn15"><p>The so-called Foundational Decision Theory of <span class="citation" data-cites="LevinsteinSoares2020">Levinstein and Soares (<a href="#ref-LevinsteinSoares2020" role="doc-biblioref">2020</a>)</span> agrees with the resolute approach in the special case where the only information Chooser will receive are the results of predictions, and is subject to the criticisms I’ll make of resolute theories.<a href="#fnref15" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn16"><p>Spencer’s non-exploitability isn’t quite the same thing as the Dual Mandate, but it’s close enough for these purposes. Spencer rejects non-exploitability, but endorses a weaker constraint he calls the Guaranteed Principle. I don’t see any reason to distinguish between these constraints, in part because of the argument that follows in the text.<a href="#fnref16" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn17"><p>Assume Chooser is reasonably risk-neutral over dollars over this range of outcomes.<a href="#fnref17" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn18"><p>A famous character in <span class="citation" data-cites="JacksonPargetter1986">Jackson and Pargetter (<a href="#ref-JacksonPargetter1986" role="doc-biblioref">1986</a>)</span>.<a href="#fnref18" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn19"><p>I’m using Lewis as an example of the orthodox view that decision theory does not care about whether beliefs and desires are substantively rational, just that they are coherent. But note that Lewis has an idiosyncratic view in the neighbourhood of this one. He denies that the point of decision theory is to guide or judge action. He thinks that decision theory is primarily description, not normative. I agreed with that in <a href="#sec-ideal">Section&nbsp;2</a>. But he thinks its descriptive role is primarily in defining belief and desire; I think it is in explaining social phenomena.<a href="#fnref19" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn20"><p>That is, what Chooser will do if Heads, and what they will do if Tails.<a href="#fnref20" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn21"><p>If both are equally likely, Demon will flip a fair coin and say how it lands.<a href="#fnref21" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn22"><p>I believe if you change that payout to $65, the various regret based theories I discussed in <a href="#sec-indecisive">Section&nbsp;7</a> also start preferring option 2. But applying these theories to complex cases is hard, so I’m not quite sure about this.<a href="#fnref22" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section></div></main>
<!-- /main column -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->



</body></html>