---
title: "When are Philosophy Articles Cited?"
abstract: |
  It's natural to believe that philosophy citations are typically to long ago pieces. We're still talking about philosophers from millenia ago. More strikingly, we're still talking about papers from half a century ago not as historical papers, but as part of the contemporary debate. But a systematic look at the citation data shows that these cases are outliers. Most citations are to recently published works. Surprisingly, this is less true in recent years than it used to be. The effect of electronic publishing and communication has been to make citations, on average, older. After we adjust for the typical age of philosophy citations, and this changing trend, it turns out that the 2000s were a particularly influential time in philosophy publishing. Articles published in that decade are cited more than earlier or later articles, once we adjust for the typical times articles are cited, and the changing patterns of citation. This is arguably related to broad changes in the interests of philosophers, towards social philosophy, and epistemology.
execute:
  echo: false
  warning: false
date: today
bibliography: 
 - /Users/weath/Documents/quarto-articles/brian-quarto.bib
 - /Users/weath/Documents/citations-2025/autobib.bib
number-sections: true
keep-tex: true
ergo:
  volume: "TBC"
  issue: "TBC"
  year: "2025"
  doi: "https://doi.org/TBC"
  startpage: 1
crossref:
  custom:
    - kind: float
      key: apptbl
      latex-env: apptbl
      reference-prefix: Table A
      space-before-numbering: false
author:
  - name: "Anon"
    affiliation: "Anon Institution"
    email: "anon@institution.edu"
format:
  html:
    fig-format: png
    fig-width: 10
    fig-height: 7
  docx:
    reference-doc: my-template.docx
  pdf: 
    pdf-engine: pdflatex
    documentclass: ergoclass
    fontfamily: mathpazo  # This tells Quarto to use mathpazo instead of lmodern
    template-partials:
      - before-body.tex
      - title.tex
    include-in-header:
      - preamble.tex
    keep-tex: true
    cite-method: natbib
    biblio-style: "apalike"
    output-file: "Citation Ages in Philosophy.pdf"
---

```{r}
#| label: loader
#| cache: false

require(tidyverse)
require(slider)
require(stringr)
require(knitr)
require(lsa)
require(wesanderson)

load("/Users/weath/Documents/citations-2025/philo_bib_through_2024.RData")
load("/Users/weath/Documents/citations-2025/philo_cite_through_2024.RData")
load("/Users/weath/Documents/articles/apc/active_journal_list.RData")

if(knitr::is_latex_output()) {
  knitr::opts_chunk$set(dev = 'cairo_pdf')
}

# Graph Themes
old <- theme_set(theme_minimal())
theme_set(old)
theme_update(plot.title = element_text(family = "Scala Pro", size = 24, face = "bold"),
             plot.subtitle = element_text(family = "Scala Sans Pro", size = 30),
             axis.text = element_text(family = "Scala Sans Pro", size = 18),
             title = element_text(family = "Scala Sans Pro", size = 18),
             plot.background = element_rect(fill = "#F9FFFF"),
             panel.background = element_rect(fill = "white"),
             legend.background = element_rect(fill = "#F9FFFF"),
             panel.grid = element_line(color = "grey92"),
             legend.text = element_text(family = "Scala Sans Pro", size = 20),
             strip.text = element_text(family = "Scala Sans Pro", size = 20),
             legend.key.spacing.y = unit(0.5, 'lines'),
             legend.key.spacing.x = unit(1, 'cm')
  )

if(knitr::is_latex_output()) {
theme_update(axis.title = element_text(family = "Palatino", size = 11),
             plot.title = element_text(family = "Europa-Bold", size = 14),
             plot.subtitle = element_text(family = "Palatino", size = 11),
             axis.text = element_text(family = "Palatino", size = 10),
             plot.background = element_rect(fill = "white"),
             panel.background = element_rect(fill = "white"),
             legend.background = element_rect(fill = "white"),
             panel.grid = element_line(color = "grey92"),
             legend.text = element_text(family = "Palatino", size = 11),
             strip.text = element_text(family = "Palatino", size = 12),
             legend.key.spacing.y = unit(-0.3, 'lines'),
             legend.key.spacing.x = unit(0, 'cm')
  )
}
```

```{r}
#| label: buildgraphs
#| cache: true

start_year <- 1956
end_year <- 2024
min_data <- 5

# New attempt
# Two categories: available and typical
# Available means published before citing article
# Typical means published 3-10 years before citing article
# The 3 is because weird things have happened with recent cites in recent years

typical_low <- 3
typical_high <- 10

# This sets the color for one-color graphs

point_col <- wes_palette("AsteroidCity1")[3]

active_philo_bib <- philo_bib_through_2024 |>
  filter(year >= start_year, year <= end_year)

active_philo_cite <- philo_cite_through_2024 |>
  filter(
    id %in% active_philo_bib$id,
    refs %in% active_philo_bib$id
  )

citation_tibble <- active_philo_cite |>
  as_tibble() |>
  rename(new = id, old = refs) |>
  left_join(active_philo_bib |>
            select(id, year), 
            by = c("old" = "id")) |>
  rename(old_year = year)  |>
  left_join(active_philo_bib |>
            select(id, year), by = c("new" = "id")) |>
  rename(new_year = year) |>
  filter(old_year >= start_year,
         new_year <= end_year,
         old_year >= start_year,
         new_year <= end_year)

# Find the highly cited articles, and count their citations separately

high_threshold <- 15

highly_cited <- citation_tibble |>
  group_by(old) |>
  tally(name = "citations") |>
  filter(citations >= high_threshold) |>
  rename(id = old)

highly_cited_per_year <- active_philo_bib |>
  filter(id %in% highly_cited$id) |>
  group_by(year) |>
  tally(name = "high_articles") 

# Now a tibble of how many times articles in year x are cited in year y

year_in_year_out <- citation_tibble |>
  filter(old_year >= 1956) |>
  group_by(old_year, new_year) |>
  tally(name = "citations") |> # Now add the 'missing' pairs
  ungroup() |>
  complete(old_year, new_year, fill = list(citations = 0)) |>
  left_join(citation_tibble |>
              group_by(old) |>
              filter(n() >= high_threshold) |>
              group_by(old_year, new_year) |>
              tally(name = "high_citations") |> # Now add the 'missing' pairs
              ungroup() |>
              complete(old_year, new_year, fill = list(high_citations = 0)),
            by = c("old_year", "new_year")) |>
  replace_na(list(high_citations = 0)) |>
  mutate(low_citations = citations - high_citations)

# This works out how many citations there are each year to 3-10 year old articles

citations_in_typical_year <- year_in_year_out |>
  mutate(age = new_year - old_year) |>
  filter(age >= typical_low, age <= typical_high) |>
  group_by(new_year) |>
  summarise(typical_citations = sum(citations)) 

# I'm going to count the 'typical' articles as those published between 3 and 10 years before the citing year
# The 'available' articles are those published before the time

# Tibble for number of publications each year, and cumulative, or 'available'

articles_per_year <- active_philo_bib |>
  rename(old_year = year) |>
  group_by(old_year) |>
  tally(name = "articles") |>
  mutate(available = cumsum(articles)) |>
  mutate(typical_articles = slide_dbl(articles, sum, .before  = typical_high) - slide_dbl(articles, sum, .before = typical_low - 1)) |>
  filter(old_year >= 1956) |>
  left_join(highly_cited_per_year, by = c("old_year" = "year")) |>
  mutate(low_articles = articles - high_articles)

articles_per_year_plot <- articles_per_year |>
  ggplot(aes(x = old_year, y = articles)) +
  geom_point(color = point_col) +
  xlab(element_blank()) +
  ylab("Number of indexed articles")

typical_plot <- articles_per_year |>
  ggplot(aes(x = old_year, y = typical_articles)) +
  geom_point(color = point_col) +
  xlab(element_blank()) +
  ylab("Number of typical indexed articles")

# Same for citations

all_citations_per_year <- citation_tibble |>
  group_by(new_year) |>
  tally(name = "citations") 

all_citations_per_year_plot <- all_citations_per_year |>
  ggplot(aes(x = new_year, y = citations)) +
  geom_point(color = point_col) +
  xlab(element_blank()) +
  ylab("Citations to indexed articles")

typical_citations_per_year <- citation_tibble |>
  filter(new_year >= old_year + typical_low, new_year <= old_year + typical_high) |>
  group_by(new_year) |>
  tally(name = "citations") 

typical_citations_per_year_plot <- typical_citations_per_year |>
  ggplot(aes(x = new_year, y = citations)) +
  geom_point(color = point_col) +
  xlab(element_blank()) +
  ylab("Citations to indexed articles from typical years")


# Outbound citations

outbound_citations <- left_join(
  articles_per_year,
  all_citations_per_year,
  by = c("old_year" = "new_year")
) |>
  mutate(outbound_rate = citations/articles) |>
  mutate(outbound = round(outbound_rate, 2))

outbound_citations_plot <- outbound_citations |>
  filter(old_year != 1955) |>
  ggplot(aes(x = old_year, y = outbound)) +
  geom_point(color = point_col) +
  xlab(element_blank()) +
  ylab("Outbound citations per indexed articles")

# Citations per typical article

typical_citation_rate_per_year <- typical_citations_per_year |>
  left_join(articles_per_year, by = c("new_year" = "old_year")) |>
  #filter(new_year >= start_year + typical_high) |>
  left_join(citations_in_typical_year, by = "new_year") |>
  mutate(mean_cites = typical_citations/typical_articles)

typical_citation_rate_per_year_plot <- typical_citation_rate_per_year |>
  ggplot(aes(x = new_year, y = mean_cites)) +
  geom_point(color = point_col) +
  xlab(element_blank()) +
  ylab("Annual citation rate of typical articles.")

# All citations to typical articles in a year
ct_all <- citation_tibble |>
  filter(new_year >= old_year + typical_low, new_year <= old_year + typical_high) |>
  group_by(new_year) |>
  tally(name = "typical_citations")

age_effect_tibble <- year_in_year_out |>
  filter(old_year >= start_year, old_year <= end_year + 1 - min_data, new_year >= start_year + typical_high) |>
  filter(new_year >= old_year) |>
  left_join(
    select(
      articles_per_year, 
      old_year, 
      articles,
      high_articles,
      low_articles), 
    by = "old_year") |>
  left_join(
    select(
      articles_per_year, 
      old_year, 
      typical_articles), 
    by = c("new_year" = "old_year")) |>
  left_join(ct_all, by = "new_year") |> 
  mutate(age = new_year - old_year) |>
  mutate(cite_ratio = (citations/articles)/(typical_citations/typical_articles)) |>
  mutate(high_cite_ratio = (high_citations/high_articles)/(typical_citations/typical_articles))  |>
  mutate(low_cite_ratio = (low_citations/low_articles)/(typical_citations/typical_articles)) 

age_effect_tibble_plot <- age_effect_tibble |>
  filter(old_year >= start_year + 1, old_year <= end_year - min_data, new_year >= start_year) |>
  ggplot(aes(x = new_year, y = cite_ratio)) +
  geom_point(size = 0.25, color = point_col) +
  facet_wrap(~old_year, ncol = 6) +
  xlab(element_blank()) +
  ylab(element_blank()) +
  theme(axis.text = element_text(size = 10),
        strip.text = element_text(size = 12))

age_effect_grouped <- age_effect_tibble |>
  filter(new_year >= old_year) |>
  filter(new_year <= old_year + end_year - start_year + 1 - min_data) |>
  mutate(age = new_year - old_year) |>
  group_by(age) |>
  summarise(mean_effect = mean(cite_ratio),
            high_mean_effect = mean(high_cite_ratio),
            low_mean_effect = mean(low_cite_ratio))

age_effect_tibble_adj <- age_effect_tibble |>
  mutate(age = new_year - old_year) |>
  filter(age <= end_year - start_year - min_data) |>
  left_join(age_effect_grouped, by = "age")

age_effect_grouped_plot <- age_effect_grouped |>
  ggplot(aes(x = age, y = mean_effect)) +
  geom_point() +
  xlab("Article age") +
  ylab("Mean citation ratio")

year_by_year_with_effect <- year_in_year_out |>
  filter(new_year >= old_year) |>
  filter(new_year <= end_year) |>
  filter(old_year >= start_year, old_year <= end_year - min_data + 1, new_year >= start_year + typical_high) |>
  mutate(age = new_year - old_year) |>
  filter(age <= end_year - start_year - min_data) |>
  left_join(age_effect_grouped, by = "age") |>
  left_join(
    select(
      age_effect_tibble, old_year, new_year, cite_ratio
    ), by = c("old_year", "new_year")
  ) |>
  mutate(surplus = cite_ratio - mean_effect) |>
  arrange(-surplus)

# The next one calculates the difference between each year and the average. 
# But this has odd effects at the periphery, and compares each year to something it is part of.
# Below, in yiyo_extended, I try to work out what happens when each year is compared to the other years
# This is more work because you have to calculate the 'other years' value again each time

year_by_year_average <- year_by_year_with_effect |>
#  filter(age <= 7) |>
#  filter(old_year != 1973) |>
  group_by(old_year) |>
  summarise(mean_surplus = mean(surplus))

year_by_year_average_plot <- year_by_year_average |>
  mutate(rolling = slide_mean(mean_surplus, before = 4, after = 4)) |>
  ggplot(aes(x = old_year, y = mean_surplus)) +
  geom_line(
    aes(x = old_year, y = rolling),
    linewidth = 0.5,
    alpha = 0.5,
    color = point_col
  ) +
  geom_point(color = point_col)  +
  xlab(element_blank()) +
  scale_x_continuous(breaks = (98:100)*20) +
  ylab("Mean annual citations above average") +
  scale_y_continuous(labels = scales::percent)

year_by_year_average_plot_short <- year_by_year_with_effect |>
  filter(age <= 7) |>
#  filter(old_year != 1973) |>
  group_by(old_year) |>
  summarise(mean_surplus = mean(surplus)) |>
  mutate(rolling = slide_mean(mean_surplus, before = 4, after = 4)) |>
  ggplot(aes(x = old_year, y = mean_surplus)) +
  geom_point(color = point_col)  +
  geom_line(
    aes(x = old_year, y = rolling),
    linewidth = 0.5,
    alpha = 0.5,
    color = point_col
  ) +
  xlab(element_blank()) +
  scale_x_continuous(breaks = (98:100)*20) +
  ylab("Mean annual citations above average") +
  scale_y_continuous(labels = scales::percent)

year_by_year_average_plot_long<- year_by_year_with_effect |>
  filter(age > 7) |>
#  filter(old_year != 1973) |>
  group_by(old_year) |>
  summarise(mean_surplus = mean(surplus)) |>
  mutate(rolling = slide_mean(mean_surplus, before = 4, after = 4)) |>
  ggplot(aes(x = old_year, y = mean_surplus)) +
  geom_point(color = point_col)  +
  geom_line(
    aes(x = old_year, y = rolling),
    linewidth = 0.5,
    alpha = 0.5,
    color = point_col
  ) +
  xlab(element_blank()) +
  scale_x_continuous(breaks = (98:100)*20) +
  ylab("Mean annual citations above average") +
  scale_y_continuous(labels = scales::percent)

#print(year_by_year_average_plot)

effect_by_age_average <- function(early, late){
  age_effect_tibble |>
    filter(age >= early, age <= late) |>
    #    add_count(old_year, name = "data_points") |>
    #    filter(data_points >= min_data) |>
    group_by(old_year) |>
    summarise(mean_ratio = mean(cite_ratio)) |>
    ggplot(aes(x = old_year, y = mean_ratio)) +
    geom_point() +
    geom_smooth() +
    xlab(element_blank()) +
    ylab(element_blank()) +
    labs(title = case_when(
      early == late ~ paste0("Citation ratio at age ", early),
      TRUE ~ paste0("Mean citation ratio from ages ",early," to ",late)))
}

effect_by_age_facet <- function(early, late){age_effect_tibble |>
    filter(age>= early, age <= late) |>
    ggplot(aes(x = old_year, y = cite_ratio)) +
    geom_point() + geom_smooth() +
    facet_wrap(~age, ncol = 4)
}

year_to_mean_plot <- function(the_year){
  age_effect_tibble_adj |>
    filter(old_year == the_year) |>
    ggplot(aes(x = age, y = cite_ratio)) +
    geom_point(size = 2, alpha = 1, color = hcl(h = (the_year-1975)*(360/43)+15, l = 65, c = 100)) +
    # geom_jitter(aes(size=(old_year==2008 | old_year == 1985), shape = (old_year==2008)), alpha = 1) +
    #  geom_jitter(aes(size=(old_year %in% c(1978, 1980, 1985, 1987)), alpha = 1)) +
    # scale_size_manual(values=c(0.3,2)) +
    xlab("Age of cited articles") +
    ylab("Citation ratio") +
    geom_line(aes(x = age, y = mean_effect), color = point_col) +
    geom_point(aes(x = age, y = mean_effect), color = point_col, size = 0.4) +
    theme(legend.position = "none")
}

```

```{r}
#| label: calculate-variables

citations_1956 <- scales::label_comma()(
  filter(all_citations_per_year, new_year == 1956)$citations)
citations_2024 <- scales::label_comma()(
  filter(all_citations_per_year, new_year == 2024)$citations)

number_of_articles <- scales::label_comma()(nrow(active_philo_bib))
number_of_citations <- scales::label_comma()(nrow(active_philo_cite))

synthese_2021 <- scales::label_comma()(
  nrow(
    filter(
      active_philo_bib,
      year == 2021,
      journal == "Synthese"
    )
  )
)
synthese_2022 <- scales::label_comma()(
  nrow(
    filter(
      active_philo_bib,
      year == 2022,
      journal == "Synthese"
    )
  )
)
```

# Introduction {#sec-introduction}

This paper is about the patterns of citations of philosophy journal articles in philosophy journals. Obviously philosophy journals cite more things than philosophy journals, and just as obviously philosophy journal articles get cited in other places. But looking just at journal-to-journal citations allows us to get a citation set that is relatively complete, and hence make some systematic generalisations about the way articles are cited over time. It turns out some of these generalisations are surprising.

Before looking at the data, here are two things I believed about philosophy citations. First, philosophers tend to cite very old papers. We still regularly teach a number of papers over half a century old in introductory classes; e.g., @WOSA1969Y444700002, @WOSA1971Y116900003, @WOSA1972Z066400001, and @10.2307_2025310. These aren't taught as history papers, but as early entries into the contemporary philosophical debate. While most papers aren't cited as much as these papers are, I thought the pattern that old papers keep being cited extended to their less famous counterparts. Second, the technological changes of the last quarter century meant that this practice was being slowly reversed. A series of technological innovations made it easier to cite newer and newer works. These innovations included the spread of email, the rise of preprint archives (e.g., arXiv, SSRN, PhilPapers), and eventually official preprints in things like EarlyView. So, I thought, citations should be getting younger, because the delay between publishing and getting widely known was removed.

Both of these thoughts were wrong.

On the first point, the generalisation I made from those famous papers was just wrong. Normal papers differ from famous papers not just in how often they are cited, but in the shape of their citations. The main evidence I'll use for this is something I'll call the _citation ratio_. The citation ratio of year _o_ in year _n_ is the mean number of citations, in year _n_, of articles published in year _o_, divided by the mean number of citations, in year _n_, of articles published in years _n_-10 to _n_-3. (I'll say much more about why I'm using this measure in what follows.) @fig-master-citation-ratio shows the average citation ratio for different _ages_, of citations, i.e., the number of years between _o_ and _n_.^[The graph also includes some 'jitter' to make the different points more easily visible. I've put each decade of original publication in a different colour; I'll break those out in @fig-decades-cite-ratio. The graph starts in 1975 because the data is much noisier before then, for reasons we'll get to below.]

```{r}
#| label: fig-master-citation-ratio
#| fig-cap: "Age effects from 1975 onwards on a single graph, with the overall average shown."

age_effect_post_1975 <- year_in_year_out |>
  filter(old_year >= 1975,
         new_year >= old_year) |>
  left_join(
    select(
      articles_per_year, 
      old_year, 
      articles,
      high_articles,
      low_articles), 
    by = "old_year") |>
  left_join(
    select(
      articles_per_year, 
      old_year, 
      typical_articles), 
    by = c("new_year" = "old_year")) |>
  left_join(ct_all, by = "new_year") |> 
  mutate(
    age = new_year - old_year,
    cite_ratio = (citations/articles)/(typical_citations/typical_articles),
    high_cite_ratio = (high_citations/high_articles)/
      (typical_citations/typical_articles),
    low_cite_ratio = (low_citations/low_articles)/
      (typical_citations/typical_articles)) |>
  ungroup() |>
  group_by(new_year - old_year) |>
  mutate(
    mean_cite_ratio = mean(cite_ratio),
    mean_high_cite_ratio = mean(high_cite_ratio),
    mean_low_cite_ratio = mean(low_cite_ratio)
  ) |>
  ungroup() |>
  mutate(decade = paste0(
    floor((old_year-5)/10),
    "5-",
    floor((old_year-5)/10)+1,
    "4"
  ))

ggplot(age_effect_post_1975, aes(x = new_year - old_year, 
                                 y = cite_ratio,
                                 color = decade)) +
  geom_jitter(size = 0.5, alpha = 0.7) +
  xlab("Age of cited articles") +
  ylab("Citation ratio") +
  geom_line(aes(x = age, y = mean_cite_ratio), color = point_col) +
  labs(color = element_blank()) +
  theme(legend.position = "none")
```

Each dot on that graph is a citation ratio for a particular pair of years; the line shows the average citation ratio for all pairs with the same age. The shape is unmistakable; articles get cited much much more when they are relatively young than when they are older.

The 'evidence' I gave for the opposite view in the introductory paragraph wasn't entirely wrong. If we redo @fig-master-citation-ratio just looking at articles which have 15 or more citations in philosophy journals, we get @fig-ageeffecteverything-high. (Restricting to these articles means we look at a small percentage of a articles, but a decent percentage of the citations.)

```{r}
#| label: fig-ageeffecteverything-high
#| fig-cap: "A version of @fig-master-citation-ratio just looking at highly cited articles"

ggplot(age_effect_post_1975, aes(x = new_year - old_year, 
                                 y = high_cite_ratio,
                                 color = decade)) +
  geom_jitter(size = 0.5, alpha = 0.7) +
  xlab("Age of cited articles") +
  ylab("Citation ratio") +
  geom_line(aes(x = age, y = mean_high_cite_ratio), color = point_col) +
  labs(color = element_blank()) +
  theme(legend.position = "none")
```

The numbers on the y-axis in @fig-ageeffecteverything-high are higher than in @fig-master-citation-ratio. That's not surprising; it just means highly cited articles get cited more frequently. What is striking is the different shape of the graphs. Typical philosophy articles, if they get cited at all, get cited soon after publication and they fade into obscurity. Highly cited articles keep getting cited decades after their publication. 

These results aren't a priori obvious; things could have turned out otherwise. It could have been that there were a trove of articles which were ignored after publication and then accrued five to ten citations a couple of decades later. There are some articles that were very frequently cited soon after publication but which are now largely ignored. (This happens most frequently in philosophy of science and in philosophy of mind, I think for different reasons in the two cases.) But these cases are outliers. Most of the articles that were influential soon after publication stay that way.

For the second point, we can simply break up @fig-master-citation-ratio by ten year chunks. In @fig-decades-cite-ratio I've taken the points by from @fig-master-citation-ratio, and grouped them into 'decades'. Because I'm working here with 1975-2024 data, the decades are 1975-1984, 1985-1994 etc. To make it easier to compare decades, I've removed the last one, where there isn't enough data, and removed all points with an age over 20.

```{r}
#| label: fig-decades-cite-ratio
#| fig-cap: "Citation ratio for different decades"
#| fig-subcap:
#|    - "1975-1984"
#|    - "1985-1994"
#|    - "1995-2004"
#|    - "2005-2014"
#| layout-ncol: 2

ten_year_graph <- function(x){
  temp <- age_effect_post_1975 |>
    filter(old_year >= x, old_year <= x + 9, 
           new_year >= old_year, new_year <= old_year + 20) |>
    group_by(new_year - old_year) |>
    mutate(mean_effect = mean(cite_ratio))
  
  temp |>
    ggplot(aes(x = age, y = cite_ratio, color = as.factor(old_year))) +
    geom_jitter(size = 0.5, alpha = 0.7) +
    xlab("Age of cited articles") +
    ylab("Citation ratio") +
    geom_line(aes(x = age, y = mean_effect), color = point_col) +
    geom_point(aes(x = age, y = mean_effect), color = point_col, size = 0.4) +
    theme(legend.position = "none") +
    ylim(0, 1.6)
}

ten_year_graph(1975)
ten_year_graph(1985)
ten_year_graph(1995)
ten_year_graph(2005)
```

There are three general trends across these graphs, especially after the second graph.

1. The peaks are getting later. In the first two graphs, the line is clearly heading down by age 5; in the last one it is barely off the peak at that time.
2. The peaks are getting lower. In the last graph we barely see it cross 1.
3. The declines are much, much flatter. If you look around age 15 in the four graphs, you see the values rise steadily over time.

What all this means is that citations are getting older. While it's still true that articles from a year are (collectively) cited a more often from ages 2-5 than from ages 12-15, the difference between those two rates has fallen remarkably. The effect of technology on citations has been the complete opposite of what I expected.

The rest of this paper has two aims. 

First, I'm going to set out the methodology behind these graphs, go over the choices I made in building them, and argue that these were at least defensible choices. The intended conclusion is that these graphs really show what I say they do, that traditionally citations were mostly to very recent articles, but they are now more frequently to older articles. 

Second, I'm going to look citations from various years, after adjusting for these typical citation rates, and see which years have been more influential in the later literature. I suspect readers will not be surprised that the early 1970s stand out as being particularly influential. What might be more surprising is that the next most influential period, in terms of how often articles from then are cited compared to the overall trends, is the 2000s. There are a few possible reasons for this, but I suspect the main one is the rising importance at that time of epistemology. (This is something Eugenio @Petrovich2024 also found using a somewhat different data set.) More generally, looking at citations from different periods, and especially looking at which articles make up those citations, is a useful guide to the history of those periods. Most work on the history of analytic philosophy doesn't get beyond the early 1970s; this is an early attempt to quantify what happens in the years after the changes brought about by Kripke, Lewis, Rawls and others in those years.

# Age of Citations {#sec-age-of-citations}

## Methodology {#sec-methodology}

The data for this study comes from Web of Science (hereafter, WoS). In this section I'll go over which data I chose to use, and how I patched it together.

The bulk of the data comes from the XML files that WoS makes available to subscribing institutions. Until recently, that included my own, so that's where most of the data through 2021 comes from. That subscription has not been renewed, so the data since 2021 comes from the WoS API.^[This is also via a susbcription through my institution; the XML is more expensive.]

The XML file is rather large. After de-compression it's over a terabyte. To make it manageable, I filtered down to _articles_ (as opposed to discussion notes, book reviews, editorial matters, and so on), and whose category was either Philosophy or History & Philosophy of Science. I then selected by hand the hundred journals with the most inbound citations (among articles in these categories) which were (a) primarily English language, (b) not primarily history of science and (c) broadly 'analytic' rather than 'continental'. These were somewhat subjective choices, but the result was a reasonable collection of the journals which are most important for telling the story of a certain kind of philosophy over the last several decades.

The list of journals being used, as well as some basic statistical information about them, is in @sec-statistics. Once I had those journals, I included all articles (and notes/reviews over 15 pages) from them. I did not restrict the study to pieces that were labelled as Philosophy or History & Philosophy of Science. For interdisciplinary journals, especially _Mind and Language_, those labels seemed very unreliable on a paper-by-paper basis, and I preferred to have a full picture of each journal I was using.

The data from the XML was supplemented in two ways. First, WoS does not index _The Journal of Philosophy_ between 1971 and 1974. It is missing a few other journals in 1974 in particular, but this gap was the longest and most important, and I thought I needed to fix it. Between 1971 and 1974 the Journal published groundbreaking articles by Harry @Frankfurt1971, George @Boolos1971, Paul @Benacerraf1973, Jaegwon @Kim1973, Michael @Friedman1974, Isaac @Levi1974, and David Lewis [-@Lewis1971cen; -@Lewis1973ben]. Leaving all of those papers out seemed like it undermined the story. So I used JSTOR to find a full list of articles (as opposed to notes or book reviews) in _Journal of Philosophy_ in those years, and then looked through the citations in articles in @tbl-list-of-journals to see which citations were to one of those articles. This did mean I was using a different classification of publications into articles and non-articles, and there are some odd choices.^[Notably, the JSTOR list seemed to exclude the symposium centered around Kenneth Arrow's "Some Ordinalist-Utilitarian Notes on Rawls’s Theory of Justice"; I'm not sure why that was.] And it meant I had to do a fair bit of data cleaning just to track down references to those four years.^[A non-trivial chunk of the cleaning was sorting through the many and varied ways that philosophers have spelled Brian O'Shaughnessy's name over the years.] While I've strived to make the data as consistent as possible with the other years, it's possible that I haven't succeeded, and some discontinuities around the early 1970s are due to this discontinuity in how the data was acquired.

The tables in @sec-introduction start in 1975 in part because I'm concerned about the consistency of the data that had to be complied in two different ways, but largely because WoS only starts indexing _Analysis_ in 1975. Without _Analysis_, and especially without the papers on the analysis of knowledge and on inferentialism, you don't get a particularly complete picture of how citations work in those years. I've included 1956-1974 in some of the studies below, but the data presented there is much less complete, and hence they aren't as useful for figuring out larger trends.

The other way I supplemented the XML relates to the fact that the XML I have only goes through mid-2022. Using the WoS website, I downloaded all the articles in, and citations in, articles in these 100 journals from 2021-2024. I processed these using the bibliometrix package (@bibliometrix). I used the 2021 data to check that this method yielded roughly the same results as the XML. The differences were not great - well under 1% for the number of articles, and a little over 1% for the number of citations. So it's not a perfect match, but it's fairly close. The data used in this study for 2022-2024 comes from the WoS website via bibliometrix.

## Journal to Journal {#sec-journal-to-journal}

As noted earlier, this study is restricted to a particular kind of citation: when a philosophy journal article is cited in another philosophy journal article. That obviously leaves out a lot. The restriction to journal articles means we exclude edited volumes, theses, conference programs, and, above all, books. The restriction to philosophy means that we exclude citations in journals in adjacent fields.

The reason for these restrictions is threefold. 

First, the journal-to-journal data is so much cleaner than any other data. When WoS records a citation of an article it indexes by an article it indexes, the citation record includes the WoS ID number for the cited journal article. That means that we don't have to clean up cases where the citing author got any details of the cited article wrong. It's very common for authors to cite incorrect page numbers for an article. It's less common, but still sufficiently frequent that one has to check it, for authors to cite incorrect titles, author names (especially for hard to spell names) or even publication years. Cleaning this is a lot of work. In practice, restricting attention to cases where WoS includes an ID number for the cited article does not avoid this problem as much as delegate it to WoS. Otherwise, doing this is a huge amount of work. A similar study to this one was earlier done by Eugenio @Petrovich2024; he looked at all citations in five leading philosophy journals. It wasn't practical for him to look at more than five because of how much work it was to clean all those citations. I'm losing some comprehensiveness compared to his study, but covering twenty times more journals. This isn't to say that one of the ways of doing things is right and the other wrong; rather that by looking at slightly different things, the two studies should complement each other.

Second, looking at journals allows for a kind of comprehensiveness. To find out how often the average philosophy book from a particular year was cited, we'd need a database of all the books. Maybe that's possible via the Library of Congress, but it would be a challenge. To find out how often the average paper in an edited volume was published, we'd need a database of all the chapters in edited volumes. I don't know where one would start looking for such a thing. Journals have the advantage that they number their issues; you can typically confirm that you have everything.

Third, dealing with whole journals makes the challenge of demarcating philosophy from non-philosophy a little more manageable. At the very least, I can show you what I mean by a philosophy journal; I mean the journals listed in @tbl-list-of-journals. If I had to go through book-by-book, or chapter-by-chapter, making decisions on which were inside philosophy, it would be a massive task, and it would be nearly as massive a task for anyone to double check. The key thing here is that I'm not attempting to quantify philosophy articles published in journals, but articles published in philosophy journals. The demarcation problem is still incredibly challenging; for instance, should I have included _Cognition_ in this study? Invariably some arbitrary boundaries will be drawn. The upside of the way I'm doing things is that it involves fewer such boundaries, and they are more visible to you the reader.

In practice there are two major downsides to restricting attention to philosophy-journal-to-philosophy-journal citations.

With respect to inbound citations, a big difference is that different kinds of books and journal articles are cited, and these can give you a very different impression of the field. History of philosophy does not involve as much publishing in journals, and the articles that are published cite primary sources, and more recent books, more than other journal articles. This kind of work offers essentially zero insight into developments in the history of philosophy. Also, and this will become important later, often citations to books are to much older works than citations to articles. @Petrovich2024 notes that through the 1990s, Quine, Wittgenstein and Davidson are amongst the most cited authors. None of them show up as near the top if you just look at cited journal articles.

Davidson, in particular, raises another issue about citations to journal articles. A citation is only recorded as being to a journal article if the journal is identified in some way, ideally by name though a DOI reference would also work, in the citing article. In older works, citations to famous articles often just mention one or other collection in which they were reprinted. If someone cites "Actions, Reasons, and Causes", but the only bibliographic detail they give is that it's chapter one in _Essays on Actions and Events_, it won't necessarily show up as a journal-to-journal citation in WoS. Most articles aren't reprinted, and these days people cite originals as well as, or instead of, reprints. So overall this isn't a huge effect. But if one was trying to find the most cited articles, it's a huge source of error.^[I had been planning a study on which articles had the largest declines in citations, as a way of measuring changes in philosophical fashion. But most of the articles I found had been reprinted so often that this effect explained most of what I found. It isn't a big effect overall, but if you go looking for outliers, you'll mostly find cases where the data is unreliable.]

With respect to outbound citations, the study I'm doing doesn't show how often journals are cited outside philosophy. It doesn't show how often they are cited in books either, but that's less of a problem, I believe, because citations in books and citations in journals have similar patterns. But citations inside philosophy are a very poor guide to citations outside philosophy. If you look at @tbl-list-of-journals, you'll see that the articles in _Journal of Medical Ethics_ are, collectively, cited very rarely. This is almost entirely a consequence of my excluding medical journals where that journal is cited more often. The data in @tbl-list-of-journals tells you something. If you wanted confirmation that 'core' philosophy journals don't publish much bioethics, the citation numbers for _Journal of Medical Ethics_ are evidence for that. But they are not evidence for anything about the overall impact of the journal; we just aren't looking in the right place to see that.

## Age, Period, and Cohort {#sec-apc}

To help understand the citation patterns, I'll borrow some terminology that's common in both sociology and medicine. It's easiest to introduce this terminology with an example. Imagine that we see, in the historical record, some interesting patterns among teenagers in the late 1960s, and we're wondering what could explain the pattern. Two types of pattern spring immediately to mind, along with ways to test them.

First, the behaviour could be explained by the fact the people involved are teenagers. If so, it is an **age effect**. The natural way to test this is to see if similar patterns show up with teenagers at different times.

Second, the behaviour could be explained by the fact that it was the 1960s, and lots of striking things happened in the 1960s. If so, it is a **period effect**. The natural way to test this is to see if the same pattern shows up with non-teenagers in the 1960s.

There is an important third kind of explanation. The people involved are born in the early 1950s, so they are part of the post-war baby boom. Colloquially, they are boomers. Maybe that could explain the pattern we see. If so, it is a **cohort effect**. The natural way to test this is to see if the same pattern shows up if we look at the same people in other stages of their life.

It's easy to overlook the importance of cohort effects. Sometimes they simply look like age effects. @GhitzaEtAl2023 argue that many hypotheses about age effects on voting, e.g., that older people are more naturally conservative, are really just cohort effects. @Bump2023 argues that understanding the distinctive role the boomers in particular play is crucial for understanding many aspects of modern American life.

There are mathematical reasons that it is hard to tease these effects apart too. Many statistical techniques for separating out influences start to fall apart when there are linear correlations between combinations of variables. In this case there is as tight a correlation as is possible. By definition, cohort plus age equals period. There are some things you can do to get around this problem - see @KeyesEtAl2010 for a useful survey of some of the options, and see @Rohrer2025 for some recent scepticism about general solutions to it - but it remains a challenge.

Even conceptually, it is hard to separate out these three effects in cases where there is evidence that the strength of the effects changes over time. As I noted at the start, the natural way to test hypotheses about which effect is strongest involve looking at other times. That works well when the age effects are constant. When they are not (and they might not be here), it is harder.

For most of our story, however, it helps just to have these three effects in mind. Using them, we can summarise the data reasonably quickly.

- The age effect is that articles get cited most when they are two to five years old.
- The period effect is that there are many more citations in recent years than in earlier years. This is in part because the number of articles published in these journals has been growing, and in part because the number of citations per article grew substantially over the 2000s and 2010s, and exploded in the 2020s.
- The cohort effect is that articles from the 1970s and 2000s get cited more than you'd expect given these age and period effects, while articles from other times, most especially before 1965, but also around 1990, get cited less. The reasons for this are more complicated, and I'll return to them below.

The period effect is the largest, and in some ways the least interesting, so I'll start the analysis by quantifying it, and arguing for a particular way to screen it off.

# Period Effects {#sec-period}

The database contains `r number_of_citations` citations, but they are not distributed evenly over time. Instead, they grow rapidly. At the start, in 1956, there are only `r citations_1956` citations. That's not too surprising; without the ability to cite preprints, there aren't going to be many citations of articles that have come out that year. By 2024, there are `r citations_2024`. In @fig-citationsperyear, I show how these grew.

```{r}
#| label: fig-citationsperyear
#| fig-cap: "The number of citations in the dataset made each year."

all_citations_per_year_plot
```

As noted in @sec-methodology, I used a slightly different method to extract the citations from 2022 onwards. It's possible that the drop between 2021 and 2022 is a consequence of that change. However I don't think it is for two reasons. First, it's more likely that 2021 is just an outlier; it's a consequence largely of _Synthese_ publishing `r synthese_2021` articles in 2021, then a relatively few `r synthese_2022` articles in 2022. Second, I applied the method I'm using for 2022-2024 to 2020 and 2021, and got a fairly close agreement (within 1-2%) with each year.

What explains this dramatic growth, at least through 2021? Part of the explanation is that more articles are being published, and more articles are being indexed. @fig-articlesperyear shows how many articles are in the dataset each year.

```{r}
#| label: fig-articlesperyear
#| fig-cap: "The number of articles in the dataset published each year."

articles_per_year_plot
```

That explains some of the growth, but not all of it. The curve in @fig-articlesperyear is not nearly as steep as the curve in @fig-citationsperyear. The number of (indexed) citations per article is also rising. In @fig-outboundcitations I've plotted the average number of citations to other articles in the dataset each year.

```{r}
#| label: fig-outboundcitations
#| fig-cap: "The average number of citations to indexed articles each year."

outbound_citations_plot
```

There are a few possible explanations for the shape of this graph.

At the left-hand edge, there are obvious boundary effects. Since we're only counting citations to articles published since 1956, it isn't surprising that there aren't very many of them per article in the 1950s. Since articles rarely get unpublished, there are more articles available to cite every year.

That can't explain the massive jumps we see at the right hand edge of @fig-outboundcitations. The jump there looks like the convergence of two cultural trends. One is a trend simply to greater numbers of citations. The most casual perusal of journals will confirm that trend. The other is a trend to greater citations of journals themselves, as opposed to books or edited volumes.

A sharp jump like this is a warning sign that there is something wrong with the data, and so the data should be checked. It's impractical to cross-check every entry, but those I have checked look correct. The change seems led by the most prestigious journals. For each journal I calculated the average number of outbound citations (to these hundred journal) for both the 2010s, and the first two years of the 2020s. The ten journals with the largest increase between the decades are shown in @tbl-large-growth.

```{r}
#| label: tbl-large-growth
#| tbl-cap: "Mean outbound citations for some journals over the last two decades."

who_cites_more <- citation_tibble |>
  left_join(
    select(
      active_philo_bib,
      id,
      journal
    ), by = c("new" = "id")
  ) |>
  filter(new_year >= 2010, new_year <= 2024) |>
  mutate(period = case_when(
    new_year < 2020 ~ "2010-2019",
    TRUE ~ "2020-2024"
  )) |>
  group_by(journal, period) |>
  summarise(articles = n_distinct(new), citations = n(), .groups = "drop") |>
  mutate(name_len = str_length(journal)) |>
  mutate(mean_cites = citations/articles) |>
  pivot_wider(id_cols = c(journal, name_len), names_from = period, values_from = mean_cites) |>
  mutate(diff = `2020-2024` - `2010-2019`) |>
  mutate(Difference = round(diff, 1),
         `2010-2019` = round(`2010-2019`, 1),
         `2020-2024` = round(`2020-2024`, 1)) |>
  arrange(-diff) |>
  slice(1:10) |>
  select(Journal = journal,
         `2010-2019`,
         `2020-2024`,
         Difference)

kable(who_cites_more)

```

Since _Philosophical Review_ only publishes 10 to 12 articles per year, it is not surprising that it shows the most variation on this list. Still, the change in the 2010s isn't only small sample size variation. Of the 22 articles it published in 2020 and 2021, only one of them [@WOS000575210400003] had fewer than 14.8 outbound citations. With a sample of just 22 anything could happen, but it would be surprising to have all but one end up on the same side of the historical average by chance.

We could just ask what proportion of all citations accrue to an article in a given year. But that would be an overcorrection. In the 2020s there are more citations to be shared around, but also more articles to share them between. We need to adjust for both things. Here's the way I'll do it.

Say an article is from a year that is one of the _typically_ cited articles iff it is between 3 and 10 years before the citing year. As we saw in @fig-master-citation-ratio, that is when citations typically peak. Using this definition, @fig-articlecounts shows how many of these typically cited articles there are at any given time. (So for 2000, it shows the number of articles published 1990-1997.)

```{r}
#| label: fig-articlecounts
#| fig-cap: "Typically cited articles."

typical_plot
```

In @fig-citationcounts, I've shown how often, in each year, these 'typical' articles are cited, and in @fig-citationrate I've shown the mean number of citations to these typical articles there are in each year.

```{r}
#| label: fig-citationcounts
#| fig-cap: "Citations to typical articles."

typical_citations_per_year_plot
```

```{r}
#| label: fig-citationrate
#| fig-cap: "Mean annual citations to typical articles."

typical_citation_rate_per_year_plot 

```

Two things stand out about @fig-citationrate. The graph is fairly flat for a long time. Between the mid 1970s and early 2000s it bounces around without moving much. Then it takes off, and go through the roof in 2021, before returning to the long term trend. The other thing is that the numbers are never high. For most of this study, even articles from this peak citation age, three to ten years old, are cited in one of these hundred journals once a _decade_. Actually, since citation rates are extremely long-tailed, and mean rates are well above medians, that somewhat overstates how often the 'average article' was being cited. Frequent citation is very much not the norm.^[In the long run the average number of times an article is cited equals the average number of citations per article. So it shouldn't be too surprising that most article have just a handful of citations in philosophy journals.]

My first pass measure of an article's influence at a time is how often it is cited at that time, divided by how often the typical article is cited at that time. This is a little arbitrary; I could have picked other ranges than three to ten years, but I think it gets things roughly right. I tried several other measures, and they all either led to implausible trends in the data, or to comparative judgments about the influence of various papers that didn't seem remotely plausible. This measure had the nice consequence that how influential the leading 50 articles from a period were 10-20 years after that period was reasonably stable, suggesting that it does correct for period effects reasonably well.


# Age Effects {#sec-age}

The next task is to work out how the age of an article affects how often it is cited. The simplest thing to do here would be to look at a typical year, and see how often articles from that year have been cited over time. That would, unfortunately, be completely wrong. @fig-1990-outbound-citations shows how often articles from 1990 have been cited over time.

```{r}
#| label: fig-1990-outbound-citations
#| fig-cap: "Citations to articles published in 1990."

year_in_year_out |>
  filter(old_year == 1990, new_year >= 1990) |>
  ggplot(aes(x = new_year, y = citations)) +
  geom_point(color = point_col) +
  xlab(element_blank()) +
  ylab("Number of citations")
```

If we're using citations as a measure of influence, @fig-1990-outbound-citations suggests that, collectively, articles from 1990 were most influential in 2021. That's not really true; they were most influential two to four years after they were published, like most articles. It's just that there are so many articles published in the 2020s, and each of them cite so many pieces, that citations to three decade old articles get lifted by the rising tide.

A more intuitive way of measuring influence uses the notion of typical articles from the @sec-period. Let's adjust @fig-1990-outbound-citations by dividing each value by two things. First, by the citation rate of typical articles, as shown in @fig-citationrate, to adjust for period effects. Second, by the number of articles published in 1990, so we're getting a measure of influence per article. The result what I earlier called the citation ratio, and the graph for 1990's citation ratio is @fig-1990-outbound-citations-norm.

```{r}
#| label: fig-1990-outbound-citations-norm
#| fig-cap: "Normalised measure of citations to articles published in 1990."

year_in_year_out |>
  filter(old_year == 1990, new_year >= 1990) |>
  left_join(
    select(
      typical_citation_rate_per_year,
      new_year,
      mean_cites
    )
  ) |>
  mutate(norm_cites = citations/(mean_cites* 1428)) |>  
  ggplot(aes(x = new_year, y = norm_cites)) +
  geom_point(color = point_col) +
  xlab(element_blank()) +
  ylab("Normalised citations")
```

There are two reasons to think that the graph in @fig-1990-outbound-citations-norm is a more plausible measure of influence than the one in @fig-1990-outbound-citations. One is just an appeal to intuition. I know how often work from 1990 came up in discussions in the 1990s and in the 2020s, and it was a lot higher in the former than in the latter. While that kind of intuitive evidence should be given some weight, it's obviously unreliable on its own. The better reason is that we get a very similar shaped graph to @fig-1990-outbound-citations-norm no matter which initial year we pick. This was already visible in  @fig-decades-cite-ratio, but it's worth seeing how stable it is.

I've used the expression 'citation ratio' a few times; it's worth being more precise about what it is. Let *c*(*o*, *n*) be the number of citations of articles from year *o* (the old year) in year *n* (the new year). Let *a*(*o*) be the number of articles published in year *o*. Then the citation ratio *r*(*o*, *n*) is:

$$
r(o, n) = \left(\frac{c(o, n)}{a(o)}\right) / \left(\frac{\sum\limits_{i = n-10}^{n-3}c(i, n)}{\sum\limits_{i = n-10}^{n-3}a(i)}\right)
$$

In @fig-ageeffecttibble-early and @fig-ageeffecttibble-late each facet is a different value for *o*, the x-axis is *n*, and the *y* axis is *r*(*o*, *n*). The key thing to note is how steady these graphs are. I have cheated a little; if I'd shown earlier years the shapes would not have been the same. There are so few citations in the 1960s that the noise overwhelms the signal. Since then, we get a reasonably steady pattern.

```{r}
#| label: fig-ageeffecttibble-early
#| fig-cap: "Citation rates for articles published 1968-1992."
#| fig-height: 12
#| fig-width: 9

age_effect_tibble |>
  filter(old_year >= start_year + 1, old_year <= end_year - min_data, new_year >= start_year) |>
  filter(old_year >= 1968, old_year <= 1992) |>
  ggplot(aes(x = new_year, y = cite_ratio)) +
  geom_point(size = 0.4, color = point_col) +
  facet_wrap(~old_year, ncol = 5) +
  xlab(element_blank()) +
  ylab(element_blank()) +
  theme(axis.text = element_text(size = 10),
        strip.text = element_text(size = 12))
```

```{r}
#| label: fig-ageeffecttibble-late
#| fig-cap: "Citation rates for articles published 1993-2017."
#| fig-height: 12
#| fig-width: 9

age_effect_tibble |>
  filter(old_year >= start_year + 1, old_year <= end_year - min_data, new_year >= start_year) |>
  filter(old_year >= 1993, old_year <= 2017) |>
  ggplot(aes(x = new_year, y = cite_ratio)) +
  geom_point(size = 0.4, color = point_col) +
  facet_wrap(~old_year, ncol = 5) +
  xlab(element_blank()) +
  ylab(element_blank()) +
  theme(axis.text = element_text(size = 10),
        strip.text = element_text(size = 12))
```

# Appendix: Summary Statistics {#sec-statistics}

The paper uses the journals shown in @tbl-list-of-journals.

```{r}
#| label: tbl-list-of-journals
#| tbl-cap: "Journals used in this paper"

journal_summary <- active_philo_bib %>%
  group_by(journal) %>%
  summarise(
    earliest_year = min(year, na.rm = TRUE),
    latest_year = max(year, na.rm = TRUE),
    n_articles = n()
  )

# Outbound citations: join articles to their journal, count refs per journal
outbound_cites <- active_philo_cite %>%
  left_join(active_philo_bib %>% select(id, journal), by = "id") %>%
  group_by(journal) %>%
  summarise(outbound_citations = n())

# Inbound citations: join refs to their journal, count refs per journal
inbound_cites <- active_philo_cite %>%
  left_join(active_philo_bib %>% select(id, journal), by = c("refs" = "id")) %>%
  group_by(journal) %>%
  summarise(inbound_citations = n())

# Combine all summaries
journal_summary <- journal_summary %>%
  left_join(outbound_cites, by = "journal") %>%
  left_join(inbound_cites, by = "journal") %>%
  replace_na(list(outbound_citations = 0, inbound_citations = 0)) |>
  rename(
    Journal = journal,
    `First Year` = earliest_year,
    `Last Year` = latest_year,
    `Articles` = n_articles,
    `Inbound Citations` = inbound_citations,
    `Outbound Citations` = outbound_citations
  )

kable(journal_summary)
```

What I've called an _article_ here is anything that either (a) marked as an article or research-article by WoS, or (b) marked as a review, discussion, or note by WoS and is at least 15 pages long. I needed to include (b) because some very important works (e.g., @WOSA1963CEU0700001 and @WOS000272855000002) were not recorded as articles by WoS.

The years here are **not** the first and last years that the journals published, but the earliest and latest years that are in the WoS index (as of the time I pulled the data). As mentioned in the main text, this makes a big difference for some journals, especially _Analysis_.

The way WoS handles the 'supplements' to _Noûs_, i.e., _Philosophical Perspectives_ and _Philosophical Issues_, is a little uneven. Some years these are recorded as being their own thing, i.e., with a source name of _Philosophical Perspectives_ or _Philosophical Issues_; and some years they are recorded as special issues of _Noûs_. When they were listed as special issues, the citations were extremely unreliable. Some high profile articles are recorded as having no citations until several years after publication. The bibliographic information for the articles themselves was also spotty. So I've manually removed all records that were listed as special or supplementary issues of _Noûs_ (and similarly removed the citations to those articles that did get tracked). What you see here are just the standalone issues of _Philosophical Perspectives_.