# Moral Uncertaintism

## Naming The Topic

Moral uncertaintism, as I'll use the phrase here, is the view that we should treat moral uncertainty in the same way that we treat factual uncertainty, as far as that is possible. What do I mean by "how we treat factual uncertainty"? Well, consider this little vignette, only partially fictionalised.

Imagine that I am only interested in money, specifically in getting as much money as possible. And imagine that very near my office, there is a casino. Here's something that I could do that would result in me getting a lot more money than I currently have. I get my hands on as much cash as I can, walk down to that casino, and bet it all on the roulette wheel. In particular, I bet on the number that will actually win. In a sense, this is possible. After all, for any number, I can bet a lot of cash on that number, and one of them will win, so whichever one will win, is one I can bet a lot of cash on. I keep repeating this until they start looking suspicious, at which point I collect my winnings, and leave with a very large amount of money.

In reality, I don't do that, and not just because money is not my sole aim in life. Why not? The obvious, and mostly correct, answer is that I don't know which number will win. In situations of uncertainty, like my uncertainty about which number will win, I don't do the thing that produces the best return. Rather, I do the thing that produces the best _expected_ returns. At least, that's what I do when I can reasonably assign probabilities to the various possible outcomes. When not even that is possible, I have to rely on more qualitative approaches.

There are two features of this vignette that I want to draw attention to, because they'll become important in what follows. First, while I don't know what action will maximise my money, there is one action that I know will not maximise my money. That's the action of declining to bet. And yet that's what I do. Sometimes maximising expected returns guarantees not maximising actual returns. In fact, when there are casinos in the area, that is almost always the case. Second, we can imagine a very fictionalised version of the story where the casino offered more than fair bets on roulette. Imagine they paid out $50 for every $1 bet if you guess the correct one of the 37 (or 38) numbers that could win. Then I really should go to the casino and bet heavily. And that's true even though it is very improbable that I'll win.[^To clarify this example, imagine that this 50-1 offer is a one-time deal; you can only exercise it for one bet on one spin of the wheel. And imagine that the casinos, with their Benthamite levels of surveillence, know whether you are teaming up with other people to bet on all the numbers, and won't allow those kind of shenanigans.] Just how much I should bet turns out to be a tricky question, depending on unclear issues about the shape of the function from how much money I have to how much utility I get from that money. But very plausibly I should bet a lot, even though it is more than 97% likely that I'll lose.

The moral uncertaintist thinks something similar is true for moral uncertainty. They say that in circumstances where we don't know what the morally right thing to do is, we often shouldn't do that right thing. Doing the right thing, in situations of moral uncertainty, is like betting on the roulette number that actually wins. It's nice that your strategy worked on this occasion, but it was the wrong strategy to follow. Rather what you should do is, if possible, maximise the expected moral value of your action. In cases where even that is impossible, because the relevant probabilities are not defined, you should use some other qualitative heuristic that is sensitive to your moral uncertainty.

I am following Elizabeth @Harman2013 in calling this view moral uncertaintism. I'm also going to follow Harman in suggesting that it isn't a good view. Proponents of the view, and some opponents too, have started using a different name for the view. They call it 'moral hedging'.^[Note to self: Include some examples here.] I think this is a bad name, and it's a bad name for a philosophically interesting reason. So I'm going to start this short discussion of moral uncertaintism by saying a bit about why it is a bad name, and why I'm using Harman's terminology instead.

To see why the name 'moral hedging' has seemed appealing, consider the case of Louise. (This kind of case has been used frequently in the literature.) Louise is trying to decide whether to have meat or vegetables for dinner. She's thought a bit about the ethics of meat eating, and she's decided that it is 90% likely that meat eating is morally acceptable. But she also thinks that if meat eating is morally unacceptable, it is an abomination. So she faces the following decision problem.

--------------- ------------------ --------------------
                  Meat-eating is     Meat-eating is
                 acceptable (90%)   unacceptable (10%)
 Eat meat              1                 -100
 Eat vegetables        0                   0
--------------- ------------------ --------------------

The numbers in each cell refer to the moral value of her choices. Assume she prefers meat to vegetables, and has a weak duty to promote her own well being where permissible, so if meat is acceptable, it is slightly morally preferable to eat meat to vegetables. But as noted above, if meat-eating is unacceptable, it is really morally bad - that's what the -100 value represents. Then for Louise, the expected moral value of eating meat is $0.9 \times 1 + 0.1 \times -100 = -9.1$. And the expected (and certain) moral value of eating vegetables is 0. Since $0 > -9.1$, the best option is to eat vegetables. That's what she should do.

And note that we've concluded she should do this without either saying anything substantive about the ethics of meat-eating, or without saying that she regards ethical vegetarianism is particularly likely. But what we have said is that, from Louise's perspective, meat-eating is morally risky. Eating vegetables, on the other hand, is morally safe. And the strategy of maximising expected moral value recommends this safe option. This is why, I think, the label 'moral hedging' has become popular. Louise should hedge her moral bets, play it safe, and settle for the vegetables.

But it wasn't just the theory that one should maximise expected moral value that led to this dietary recommendation. It also required some substantive assumptions about what Louise's moral views were, and what options were taken to be available to her. If we change those assumptions, we can easily make the view that we should maximise expected moral return look considerably less safe.

So consider Antoine, who in many respects is like Louise. He is also thinking about what to have for dinner. And he thinks it is 90% likely that meat-eating is morally acceptable. But conditional on meat-eating being unacceptable, he has slightly different views to Louise. She thought that if meat-eating is unacceptable, then one should settle for vegetables for dinner. Antoine mostly thinks that too. Conditional on meat-eating being unacceptable, he thinks it is 90% likely that one should have a vegetarian diet, and maybe be a touch sanctimonious about it around carnivores, but otherwise live life unchanged. But the other 10% of his conditional credence goes to the view that if meat-eating is wrong, then it's kind of like murder, and one has affirmative duties to protect those at risk of murder. In particular, that last 1% of Antoine's moral worldview goes to the theory that killing humans to prevent them killing animals is morally good, and declining to do so is a bad form of moral cowardice. That's especially true in cases where killing a human would lead in the long run to fewer animals being killed. And around where Antoine lives, cattle ranching is a rather unpopular line of work. Every cattle rancher you kill probably means one fewer person employed to raise cattle for beef. 

So here is the decision table Antoine faces. The middle column is the possibility that ordinary ethical vegetarianism is the correct view; the last column is the view that ethical vegetarianism requires the morally righteous to butcher the butchers. And Antoine doesn't really think that's right - he only gives it 1% credence - but there really are a lot of cows you could save this way, probably about 200 for every rancher you kill. So here's how the table looks for him.

--------------- ------------------ ------------------- ------------------
                  Meat-eating is     Meat-eating is     Meat-eating must
                 acceptable (90%)   unacceptable (9%)    be stopped (1%)
 Eat meat              1                 -100                -200
 Eat vegetables        0                   0                 -100
 Kill ranchers       -100                -100                20000 
--------------- ------------------ ------------------- ------------------

Now we just have to run the numbers. The expected moral value of eating meat is $0.9 \times 1 + 0.09 \times -100 + 0.01 \times -200 - -10.1$. The expected moral value of eating vegetables is $0.9 \times 0 + 0.09 \times 0 + 0.01 \times -100 = -1$. And the expected moral value of killing ranchers is $0.9 \times -100 + 0.09 \times -100 + 0.01 \times 20000 = 101$. By far, the best option is to turn into a rancher killing vigilante.

I don't mean this example to be a knock-down refutation of moral uncertaintism. I think moral uncertainists have at least two good responses to this example. (Though as we'll see in a bit, neither of them is cost-free.) 

First, they could say that what matters for decision making under moral uncertainty is not what credences a thinker actually assigns to various moral theories, but how probable those theories really are, given that thinker's evidence. And while Antoine actually gives some credence to the view that ranchers much be killed, he shouldn't. 

Second, they could say that maximising expected moral value is only one moral desideratum among many. This kind of view has some important historical precedents. Peter Abelard argued that we have moral obligations both to do what is actually right, and do do what we think is right. You can think of the obligation to maximise expected moral value as a modern, formal version of Abelard's view that we should do what we think is right. Abelard thought that people with false moral beliefs faced moral dilemmas; whatever they do will violate one of their moral obligations. And it isn't absurd to say that Antoine faces a dilemma of sorts as well. 

So I don't think Antoine provides an immediately compelling argument that moral uncertaintism is false. But he does provide an immediately compelling argument that it's a very bad idea to call this view moral hedging. It is really misleading to say that the hedging, careful, play it safe view is to murder ranchers because there is a 1% chance that this is a really compelling moral obligation. Sometimes maximising expected moral value will lead one to play it safe, and sometimes it will lead to acting with reckless abandon. Of course, maximising expected financial value, which really is constitutive of a certain kind of financial prudence, has the same characteristics. So better to stick with the idea that what's definitive of the theory is that moral and factual uncertainty are treated the same way. That's an interesting enough view to discuss. And it's especially interesting for the purposes of this book because it looks like it will be committed to the kind of thesis that Lewis argued against.

## Against Moral Uncertaintism

In my _Normative Externalism_ [-@Weatherson2019] I argued against moral uncertaintism at some length. I won't repeat all those arguments here, but just to have some details about the view on the table, I will run over two quick arguments. I argued that moral uncertaintism is under-specified, and under-motivated. I'll take those in order first.

To see the under-specification problem, consider Camille, who is mostly like Antoine. That is, he has the same moral credences as Antoine, and so by his lights it maximises expected moral value to kill ranchers. But Camille also has a strong desire to not kill people unless he is pretty confident that they are wrong-doers. This is over and above his desire to do the right thing. And so relative to all the things Camille values (i.e., both doing the right thing and not killing the probably innocent), it maximises expected value to eat vegetables rather than kill ranchers. Let's assume that Camille does that, and that it would have been morally wrong to kill the ranchers. 

The moral uncertaintist says Camille shouldn't have done that; he should have killed the ranchers. But what's the sense of 'should' here? It can't be that he morally should kill the ranchers. By hypothesis, morally he should not do this. It can't be that be rationally should kill the ranchers, at least if rationality is concerned with doing well by one's own lights. By hypothesis, it would be irrational by his lights to kill the ranchers. The most you can say is that if he had the same beliefs (i.e., the credal distribution from Antoine's table), but only valued doing the right thing as such, then it would produce the most (expected) value to kill the ranchers. But why should we even care about that conditional? Why think it corresponds to any kind of norm we antecedently care about? It would be actually immoral to kill the ranchers, and bad, i.e., irrational, by Camille's own lights. The moral uncertaintist is aiming for some odd hybrid norm, and it isn't one I think we should give any weight to.

To see the under-motivation problem, it helps to work through the various motivations for moral uncertaintism. I won't go through these in great detail, but I've already said enough here to see the outline of the problem.

One argument for moral uncertaintism is that it guards against an unhelpful form of moral recklessness. Just doing the right thing, whatever it is, might be excessively morally risky when one doesn't know what the right thing to do is. But if the aim is to reduce moral risk, a theory that recommends Antoine kills ranchers doesn't have a lot going for it.

Another pair of motivations are that it is unfair to ask people to do the right thing when they don't know what it is, and that it is useless advice to tell people to just do the right thing when they don't know what it is. But it's not so easy to maximise expected moral value either. There are two technical challenges here. One is figuring out how to translate different moral theories onto a common scale. This has become known as the problem of inter-theoretic value comparisons, and it seems just as hard as any other problem in first-order ethics [@Hedden2016]. A second is figuring out the probability of different moral theories. Just how probable is it that Kant was right about the murderer at the door? If it's unfair to expect me to figure out whether Kant was right before acting, and it's bad advice to tell me in that situation to do the right thing, then it's also unfair to expect me to figure out the probability that Kant was right, and it's bad advice to tell me to react to the actual probability that he was right.^[Current philosophers are sometimes perplexed about why we should worry about a hypothetical case where someone turns up at the door planning to kill someone you know, and yet you nevertheless have non-trivial moral obligations towards that unwelcome guest. It helps to recall just when it was that Kant and Constant had the exchange this example originated in, and which killers were turning up at which doors in those days. For much more on this, see @Rousselire2018.] When we use probabilistic reasoning in everyday life, whether it be buying insurance or not buying casino chips, we can usually offer compelling reasons for using one probability function rather than a radically different one. In the moral case, Keynesian uncertainty is the normal case, not an aberration. Hypothetical examples where we just know that this theory has probability 0.9 and that theory has probability 0.1 are even more absurd than cases where we know the moral truth. In realistic cases, _do thing right thing_ is no more unfair a standard, and no worse advice, than _maximise expected moral value_.

Finally, one might try to motivate moral uncertaintism by appeal to a quite general intuition that all uncertainty should be treated alike. This is a much better argument than the previous ones, I think, and in _Normative Externalism_ I spend two long chapters replying to it. Here I'll just note that if we want to treat all uncertainty alike, then we have to treat _all_ uncertainty alike. We have to treat epistemic, logical and decision-theoretic uncertainty the same way we treat factual and moral uncertainty. And, I argue in that book, if we do that then we can't accept any of the existing theories of how to handle moral uncertainty, and we can't avail ourselves of either of the plausible responses to Antoine's case.

## Guise of the Good

