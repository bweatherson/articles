Most people think that factual uncertainty matters to the morality of actions. If there might be someone inside a building, then it is seriously morally wrong to demolish the building, even if it turns out that building is empty. In recent years, a number of philosophers have argued that moral uncertainty is relevant, perhaps in a very similar way, to the normative status of actions. If something might be seriously morally wrong, that is a reason to not perform the action. If someone performs the action anyway, they are criticisable even if it turns out the action was not in fact morally wrong. 

Borrowing a term from Elizabeth [#Harman201x;], we'll call this kind of position _moral uncertaintism_. And we'll say that _strong moral uncertaintism_ is the view that moral uncertainty matters in a very similar way to how factual uncertainty matters. The purpose of this paper is to consider how some technical results about the role of belief in motivation, results that trace back to David Lewis's Desire as Belief papers [#Lewis1988x, Lewis1996a], effect the plausibility of moral uncertaintism. I'm going to argue that Lewis's arguments, or at least the application of Lewis's arguments to debates in moral psychology, rest on premises that are collectively implausible. The short version is that one step of the argument is implausible if one is an evidential decision theorist, and another step is implausible if one is a causal decision theorist, so there are going to be very few people who find the whole argument plausible. But I'm going to argue that a recent version of the argument due to Jeffrey Russell and John Hawthorne is more relevant [#RussellHawthorne2016]. In particular, the Russell and Hawthorne argument does show that there is a problem for strong moral uncertaintism, though not I'll argue for the weaker version. 

Before all that, I'm going to discuss why we might have thought the Lewis argument posed a problem for moral uncertaintism. And do that, it helps to place moral uncertaintism in the context of some recent debates in meta-ethics and moral philosophy.

# Introduction

Alongside the rise of interest in moral uncertaintism, there have been a number of criticisms of the view. There are difficult technical issues in getting the details of uncertaintism right, and Brian [#Hedden201x;] has argued that these pose an insuperable difficulty to the view. Brian [#Weatherson201x;] has argued that moral uncertaintism is committed to being on the wrong side of debates about moral motivation. And Elizabeth [#Harman201x;] has argued that moral uncertaintism is committed to being on the wrong side of debates about blame and moral ignorance. All of these are controversial; it isn't obvious either that moral uncertaintism has these commitments, not that the side in question is the wrong side. Rather than dig into those debates, I want to take a step back, and look at how uncertaintism connects to some broader debates.

One issue for moral uncertaintism concerns the kind of criticisability it posits. On the one hand, it can't really be moral criticism, on pain of contradiction. After all, it is conceded that the people who do the 'wrong' thing may not actually be violating any moral rules. The most popular move at this point is to say that moral uncertaintism concerns rational norms; it would be irrational to do the thing that may be seriously morally wrong. It's not entirely clear how this is consistent with strong moral uncertaintism; after all, it is morally wrong to demolish the building without checking who is inside it. Just what kind of criticism is involved in moral uncertaintism is something that different moral uncertainists have rather different views on, and we'll come back at points below to how different answers to this question interact with the primary question we're looking at.

Another issue concerns what to say about people who are not just morally uncertain, but straightforwardly morally mistaken. Moral uncertaintists have a hard time accommodating the points Nomy [#Arpaly200x;] makes about cases of 'inadvertent virtue', i.e., right-doing by people who think they are acting wrongly, and 'misguided conscience', i.e., wrong-doing by people who think they are acting rightly. This issue will mostly stay under the surface here, but is important to keep in mind.

The big issue I'm going to discuss is a purely structural feature of moral uncertaintism. Moral uncertaintism implies that moral attitudes are, to borrow terminology from the 1980s and 1990s, _besires_. That is, they are beliefs - they have the logic and dynamics of beliefs - but they play the characteristic role of desires, to motivate. According to moral uncertaintism, there is a certain kind of norm of action such that compliance with that norm is entirely a function of what one believes, and not at all a function of what one desires. If one believes that A is right and that B is wrong, then compliance with the norm requires doing A. And that's true, at least as I read moral uncertaintists, no matter what one desires. At least, one doesn't typically see moral uncertainists saying that their view only applies to people who have a certain desire, namely the desire to do what is right. Rather, they tend to put forward their view as a universal imperative, and the inputs to that imperative are entirely doxastic.

The main aim of this paper is to connect debates about moral uncertaintism to debates from the 1980s and 1990s about whether there can be such universal imperatives that are only sensitive to doxastic states. There is a lot of material one could draw on here, but my focus will be on David Lewis's two Desire as Belief papers. Here is the plan for the next four sections.

* In section 2, I'll set out Lewis's argument, and show why it looks to raise a problem for moral uncertaintism.
* In section 3, I'll look at the assumptions behind Lewis's argument, and argue that few (if any) philosophers should feel compelled to accept all of its premises.
* In section 4, I'll compare my response to Lewis with some existing responses. This is both to give credit to those whose work I'm building on, and to argue that this is a genuinely new response to Lewis.
* And in section 5, I'll look at ways of trying to model moral uncertaintism so as to show that no Lewisian argument against it could succeed. This is in part to show how moral uncertaintism connects to the broader debates Lewis was contributing to, and in part to set up the discussion of Russell and Hawthorne in the second half of the paper.

# The Lewisian Objection

In order to avoid technical complications about the formulation of moral uncertaintism, I'm going to focus on an exceedingly simple case. The case has the downside of being completely unrealistic, and the upside of sidestepping those technical challenges that [#Hedden201x;] argues are fatal for moral uncertaintism. So we'll assume that X is facing a choice between some options, and X doesn't know a bunch of moral facts. But X does know that their choice, whatever it is, will be either Good or Bad. And X also knows that all Good choices are equally right and equally good, and all Bad choices are equally wrong and equally bad. In other words, we can usefully model the moral value of X's choice as being either 1 or 0, depending on what X chooses, and on what the moral facts are. Given this extremely idealised set of assumptions, every extant approach to moral uncertaintism says that X should choose the option that has the highest probability of being Good. That will option will maximise expected moral value, and will be the one that is most probably morally acceptable, so lots of moral uncertainists should like it.

In fact, it is plausible that moral uncertaintists should like the following equation, where A is an arbitrary option, the 'halo' maps a proposition p onto the proposition that p is good, V is the value (or desirability) of a proposition being true, and Pr is the relevant probability function.[^Note that I'm not taking a stance here on whether Pr measure something epistemic, like an evidential probability function, or something doxastic, like the agent's credence. The arguments to follow go through either way.]

> V(A) = Pr(Ã…)

This is what Lewis calls the desire as belief thesis. And, he argues, it must be false. To see why, we just need one bit of theory. 

Say that a _world_ is a specification of all the things that matter to fixing a value for an outcome. This is more or less what Savage calls a 'small world'. A world in this sense is more detailed in some ways that a Lewisian concreta, and much less detailed in others. Imagine that we are tasked with deciding what will happen to the captured enemies of the revolution. A small world will fix what we do, and will fix the moral value of that, but will not fix much more. It won't determine, for instance, who won last night's baseball game. But it might fix things in a way that is both metaphysically impossible, and a priori incoherent. Assume (plausibly enough) that it is wrong to execute these enemies. Then there may be no possible world where we execute them, and this is Good. And, if some form of Kantianism is correct, it may even be a priori knowable that there is no such world. But we'll include in our model a world where the executions are carried out, and this is Good. And this is because even if one could in principle know that no such world obtains, a particular actor at a particular time in history may not know this, and may be unsure what morality demands of them in the middle of a revolution. And we want to model how things go for such an ignorant agent.

In many cases, especially when we know the result will be Good or Bad and not anything in between, there will be finitely many worlds to consider. Each of these worlds could be filled out in more detail, but we're assuming the filling out won't matter, at least to the relative value of the worlds being considerd. Given this, we'll assume the following equation holds for values.

> V(A) = $\sum_{w \in A}$ V(w) Pr(w | A)

That is, the value of a proposition is the weighted average of the value of the worlds where the proposition is true, where the weighting is given by the probability of each world being the one that makes the proposition true. 

Lewis attributes this view about value to Richard [#Jeffrey1965;], who in turn credits XXX with describing it as the 'news-value' of a proposition. This last description, while widely repeated, turns out to be misleading in an important way; it's really more like one's contentedness with the news that A. This will matter a bit in what follows, as will the fact that this really looks like the kind of value an evidential, as opposed to a causal, decision theorist will care about.

Now here is the big theoretical claim that we need. We will use subscripts to denote the value of a function after updating on a piece of evidence. So V_B(A) is the value of A after updating on B, and Pr_B(A) is the probability of A after updating on B.

> V_B(w) = V(w)

That is, the value of a world does not change when one learns something new. And that's for the simple reason that the worlds, as defined, fix all the information relevant to determining value. If that's so, then learning something new can't change the value of the world. (We'll come back to this argument in section 4.) Given that principle, we can infer the following equation:

> V(AB) = V_B(A)

Proof: V(AB) = $\sum_{w \in A}$ V(w) Pr(w | AB) = $\sum_{w \in A}$ V_B(w) Pr(w | AB) = $\sum_{w \in A}$ V_B(w) Pr_B(w | A) = V_B(A). Substituting A for B, and using the fact that AA = A, we get

>  V(A) = V_A(A)

Now the desire as belief equation V(A) = Pr(Ã…) is meant to hold for all (rational) subjects at all times. So it should hold before and after conditionalising on A. So we have Pr(Ã…) = V(A) = V_A(A) = Pr_A(Ã…). That is, Pr(Ã…) = Pr_A(Ã…). That is, A and Ã… are probabilistically independent. And we have proven this on the basis of just considerations of rationality, so it must hold for all agents at all times. But this is absurd. 

Imagine, says Lewis, that an agent has just learned A $\vee$ Ã…. (It seems like anyone could learn this via testimony, at least given the permissive view we are taking towards moral epistemology that allows that rational people can have moral beliefs that are a priori false.) It is incoherent to be sure that A $\vee$ Ã…, to have a probability for each of A and for Ã… that is strictly between 0 and 1, and have A and Ã… be probabilistically independent. Since the agent is sure of A $\vee$ Ã…, they  just learned it, and the independence claim has been proven to be a constraint on all agents. So this implies that on learning A $\vee$ Ã…, the agent must be sure of one of A, $\neg$ A, Ã… or $\neg$ Ã…. That's already absurd (assuming moral uncertainty is rational), but we can arguably make it worse. Assuming that learning goes by conditionalisation, the only way to guarantee that one of these four will be certain after conditionalising on A $\vee$ Ã… is that prior to conditionalisation, the agent must have been certain of one of the four of them. (At least, assuming A and Ã… were independent prior to learning, which we've also proven is a constraint.) 

This is all absurd, and Lewis concludes that the villain of the piece is the initial equation,  V(A) = Pr(Ã…). And if that equation falls, then it seems moral uncertaintism must fall with it, since the moral uncertaintist endorsed the idea that the value of an action is tied to the probability that it is Good. So we have a new, and seemingly systematic, objection to uncertaintism. The problem is, it doesn't work. The next section will outline why.

# Replying to Lewis

In this section I'll look at a number of ways of interpreting the formalism in Lewis's argument. First, I'll consider the interpretation that Lewis himself put forward, that V measures something like desirability. Second, I'll consider what happens if V measures not desirability, but choice-worthiness, and our theory of choice is evidential decision theory. Third, I'll consider what happens if V measures not desirability, but choice-worthiness, and our theory of choice is causal decision theory. On each of the first two interpretations, I'll argue that the uncertaintist is not committed to the desire as belief equation. Rather, they are committed to the slightly more complicated equation V(A) = Pr(Ã… | A). This is what Lewis calls Desire as Conditional Belief, and he notes (correctly) that he has no distinctive argument against it. On the third interpretation, I'll note the now familiar arguments that V(A) = $\sum_{w \in A}$ V(w) Pr(w | A) is false, so the argument doesn't get off the ground.

So let's start with the first interpretation, which is the one that Lewis focussed on. This says V measures the desirability of something being true. To make sure that we're really focussed on desirability, and not choice, we'll consider a case where a third party, call them X, is making a choice. And X has to choose between three options, A, B and C. We don't know which of these is morally best, because each of them has a flaw. A is arguably wrong because insufficiently benevolent, B is arguably wrong because it would be an unjustified promise-breaking, and C is arguably wrong because it would violate a duty of friendship. What should we hope X will do, if we are moral uncertaintists? (Remember we are still assuming we know that whatever X does, it will be Good or Bad, and not anything better, worse, or in between.)

The simple answer is that we should hope X does the thing that is most probably Good. But the simple answer is too simple; it gets things wrong in some cases. Imagine we know a little about X. We know that X is not a terrible person, and the fact that they choose something is some evidence that it is Good and not Bad. But we know a bit more than that. We know that X sometimes messes up when it comes to benevolence and friendship, but X does not mess up when it comes to promises. We are sure that X would never wrongly break a promise. They might break a promise when this is justified, e.g., when the only way to save a drowning child involves breaking a promise to meet a friend for coffee. But they never wrongly break a promise.

Given that, we should hope that X takes option B. If X does take option B, we will be sure they have done something Good. And that's the best case scenario. If they do A or C, we will still think think there is some chance they are doing what is Bad. So if we care about getting to the Good, we should be hoping that its option B that gets chosen. That's not to say that option B is best to choose - it may be terrible and not be chosen because it is terrible. But it is the one we should hope is chosen.

The general lesson of this case is that we should hope that X chooses something that, given it is chosen, is probably good. In symbols, we have V(A) = Pr(Ã… | A). And that's the thesis that Lewis calls Desire as Conditional Belief, and which he doesn't have a distinctive argument against. That's what the moral uncertaintist should adopt if they want to identify V with something like desirability or newsworthiness.

What if we identify V with something more like choice-worthiness? This is, after all, a more common focus for moral uncertaintists; they want to develop principles for morally uncertain agents to make choices. Here we need to make a choice between something like evidential decision theory, and something like causal decision theory. To see the difference, consider this rather improbable situation. Our hero has to choose between two options, A and U. And she knows the following things:

* Ã… is 1% more probable than Å®.
* If she chooses A, then Pr(Ã…) = 0.1, and Pr(Å®) = 0.09.
* If she chooses U, then Pr(Ã…) = 0.91, and Pr(Å®) = 0.9.

As far as I can tell, extant versions of moral uncertaintism do not take a stand on what to do in such a situation. And, as far as I can tell, this is a perfectly reasonable state of affairs, because Newcomb-like problems for moral uncertainty are not exactly thick on the ground. But still, for completeness, we might wonder how those theories would extend to such a case. And it isn't, I think, entirely obvious what to do. Let's say that evidential moral uncertaintism says that in this case, the hero should do U, because doing U makes it 90% likely that she'll do the right thing, while doing A makes this only 10% likely. And let's say that causal moral uncertaintism says that in this case, the hero should do A, because no matter what, Ã… is 1% more probable than Å®, and she wants to do what is probably right. I really don't have views on which of these is correct. What I do want to say is that neither view is vulnerable to Lewis's argument, but the views differ on just where they think Lewis's argument goes wrong.

The evidential theory says that Desire as Belief does not represent their view; rather, their view is more like Desire as Conditional Belief. So they are perfectly content to see Lewis refute V(A) = Pr(Ã…), since they think value goes with probability of goodness conditional on the act being performed. After all, V(A) = Pr(Ã…) would imply that in this case it is better to do A than U, and the evidential moral uncertaintist denies this.

The causal theory says that Desire is Belief is right. But they deny what Lewis calls the addition rule for value, namely V(A) = $\sum_{w \in A}$ V(w) Pr(w | A). Rather, they endorse something more like the theory of value put forward by that great causal decision theorist, David [#Lewis1981x]. Here's how they might do it, where the variable H ranges over hypotheses about the way things might be morally.

> V(A) = $\sum_{H}$ V(AH) Pr(H)

And given that account of value, we can't infer V_A(A) = V(A), since we don't have Pr_A(H) = Pr(H). And so a crucial step in the derivation doesn't go through.

Now it is possible that there are some antecedently plausible ways of developing moral uncertaintism so that Lewis's argument raises a problem. But I think most forms of moral uncertaintism will be untouched by it. Forms that look more or less like evidential decision theory will be, for independent reasons, committed to Desire as Conditional Belief, not Desire as Belief. And forms that look more or less like causal decision theory will reject the picture of value that lets us derive V_A(A) = V(A), which is essential to Lewis's argument. Causal decision theory is not the negation of evidential decision theory, so it is possible there are other options which are threatened. But the most obvious ways to develop moral uncertaintism are not.

# Comparison to Other Responses


