# Moral Uncertaintism

## Naming The Topic

Moral uncertaintism, as I'll use the phrase here, is the view that we should treat moral uncertainty in the same way that we treat factual uncertainty, as far as that is possible. What do I mean by "how we treat factual uncertainty"? Well, consider this little vignette, only partially fictionalised.

Imagine that I am only interested in money, specifically in getting as much money as possible. And imagine that very near my office, there is a casino. Here's something that I could do that would result in me getting a lot more money than I currently have. I get my hands on as much cash as I can, walk down to that casino, and bet it all on the roulette wheel. In particular, I bet on the number that will actually win. In a sense, this is possible. After all, for any number, I can bet a lot of cash on that number, and one of them will win, so whichever one will win, is one I can bet a lot of cash on. I keep repeating this until they start looking suspicious, at which point I collect my winnings, and leave with a very large amount of money.

In reality, I don't do that, and not just because money is not my sole aim in life. Why not? The obvious, and mostly correct, answer is that I don't know which number will win. In situations of uncertainty, like my uncertainty about which number will win, I don't do the thing that produces the best return. Rather, I do the thing that produces the best _expected_ returns. At least, that's what I do when I can reasonably assign probabilities to the various possible outcomes. When not even that is possible, I have to rely on more qualitative approaches.

There are two features of this vignette that I want to draw attention to, because they'll become important in what follows. First, while I don't know what action will maximise my money, there is one action that I know will not maximise my money. That's the action of declining to bet. And yet that's what I do. Sometimes maximising expected returns guarantees not maximising actual returns. In fact, when there are casinos in the area, that is almost always the case. Second, we can imagine a very fictionalised version of the story where the casino offered more than fair bets on roulette. Imagine they paid out $50 for every $1 bet if you guess the correct one of the 37 (or 38) numbers that could win. Then I really should go to the casino and bet heavily. And that's true even though it is very improbable that I'll win.[^To clarify this example, imagine that this 50-1 offer is a one-time deal; you can only exercise it for one bet on one spin of the wheel. And imagine that the casinos, with their Benthamite levels of surveillence, know whether you are teaming up with other people to bet on all the numbers, and won't allow those kind of shenanigans.] Just how much I should bet turns out to be a tricky question, depending on unclear issues about the shape of the function from how much money I have to how much utility I get from that money. But very plausibly I should bet a lot, even though it is more than 97% likely that I'll lose.

The moral uncertaintist thinks something similar is true for moral uncertainty. They say that in circumstances where we don't know what the morally right thing to do is, we often shouldn't do that right thing. Doing the right thing, in situations of moral uncertainty, is like betting on the roulette number that actually wins. It's nice that your strategy worked on this occasion, but it was the wrong strategy to follow. Rather what you should do is, if possible, maximise the expected moral value of your action. In cases where even that is impossible, because the relevant probabilities are not defined, you should use some other qualitative heuristic that is sensitive to your moral uncertainty.

I am following Elizabeth [@Harman2011;] in calling this view moral uncertaintism. I'm also going to follow Harman in suggesting that it isn't a good view. Proponents of the view, and some opponents too, have started using a different name for the view. They call it 'moral hedging'.[^Note to self: Include some examples here.] I think this is a bad name, and it's a bad name for a philosophically interesting reason. So I'm going to start this short discussion of moral uncertaintism by saying a bit about why it is a bad name, and why I'm using Harman's terminology instead.

To see why the name 'moral hedging' has seemed appealing, consider the case of Louise. (This kind of case has been used frequently in the literature.) Louise is trying to decide whether to have meat or vegetables for dinner. She's thought a bit about the ethics of meat eating, and she's decided that it is 90% likely that meat eating is morally acceptable. But she also thinks that if meat eating is morally unacceptable, it is an abomination. So she faces the following decision problem.

--------------- ------------------ --------------------
                  Meat-eating is     Meat-eating is
                 acceptable (90%)   unacceptable (10%)
 Eat meat              1                 -100
 Eat vegetables        0                   0
--------------- ------------------ --------------------

The numbers in each cell refer to the moral value of her choices. Assume she prefers meat to vegetables, and has a weak duty to promote her own well being where permissible, so if meat is acceptable, it is slightly morally preferable to eat meat to vegetables. But as noted above, if meat-eating is unacceptable, it is really morally bad - that's what the -100 value represents. Then for Louise, the expected moral value of eating meat is $0.9 \times 1 + 0.1 \times -100 = -9.1$. And the expected (and certain) moral value of eating vegetables is 0. Since $0 > -9.1$, the best option is to eat vegetables. That's what she should do.

And note that we've concluded she should do this without either saying anything substantive about the ethics of meat-eating, or without saying that she regards ethical vegetarianism is particularly likely. But what we have said is that, from Louise's perspective, meat-eating is morally risky. Eating vegetables, on the other hand, is morally safe. And the strategy of maximising expected moral value recommends this safe option. This is why, I think, the label 'moral hedging' has become popular. Louise should hedge her moral bets, play it safe, and settle for the vegetables.

But it wasn't just the theory that one should maximise expected moral value that led to this dietary recommendation. It also required some substantive assumptions about what Louise's moral views were, and what options were taken to be available to her. If we change those assumptions, we can easily make the view that we should maximise expected moral return look considerably less safe.

So consider Antoine, who in many respects is like Louise. He is also thinking about what to have for dinner. And he thinks it is 90% likely that meat-eating is morally acceptable. But conditional on meat-eating being unacceptable, he has slightly different views to Louise. She thought that if meat-eating is unacceptable, then one should settle for vegetables for dinner. Antoine mostly thinks that too. Conditional on meat-eating being unacceptable, he thinks it is 90% likely that one should have a vegetarian diet, and maybe be a touch sanctimonious about it around carnivores, but otherwise live life unchanged. But the other 10% of his conditional credence goes to the view that if meat-eating is wrong, then it's kind of like murder, and one has affirmative duties to protect those at risk of murder. In particular, that last 1% of Antoine's moral worldview goes to the theory that killing humans to prevent them killing animals is morally good, and declining to do so is a bad form of moral cowardice. That's especially true in cases where killing a human would lead in the long run to fewer animals being killed. And around where Antoine lives, cattle ranching is a rather unpopular line of work. Every cattle rancher you kill probably means one fewer person employed to raise cattle for beef. 

So here is the decision table Antoine faces. The middle column is the possibility that ordinary ethical vegetarianism is the correct view; the last column is the view that ethical vegetarianism requires the morally righteous to butcher the butchers. And Antoine doesn't really think that's right - he only gives it 1% credence - but there really are a lot of cows you could save this way, probably about 200 for every rancher you kill. So here's how the table looks for him.

--------------- ------------------ ------------------- ------------------
                  Meat-eating is     Meat-eating is     Meat-eating must
                 acceptable (90%)   unacceptable (9%)    be stopped (1%)
 Eat meat              1                 -100                -200
 Eat vegetables        0                   0                 -100
 Kill ranchers       -100                -100                20000 
--------------- ------------------ ------------------- ------------------

Now we just have to run the numbers. The expected moral value of eating meat is $0.9 \times 1 + 0.09 \times -100 + 0.01 \times -200 - -10.1$. The expected moral value of eating vegetables is $0.9 \times 0 + 0.09 \times 0 + 0.01 \times -100 = -1$. And the expected moral value of killing ranchers is $0.9 \times -100 + 0.09 \times -100 + 0.01 \times 20000 = 101$. By far, the best option is to turn into a rancher killing vigilante.

I don't mean this example to be a knock-down refutation of moral uncertaintism. I think moral uncertainists have at least two good responses to this example. (Though as we'll see in a bit, neither of them is cost-free.) First, they could say that what matters for decision making under moral uncertainty is not what credences a thinker actually assigns to various moral theories, but how probable those theories really are, given that thinker's evidence. And while Antoine actually gives some credence to the view that ranchers much be killed, he shouldn't. Second, they could say that maximising expected moral value is only one moral desideratum 
